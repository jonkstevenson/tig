{
  "version": 4,
  "terraform_version": "1.3.7",
  "serial": 33,
  "lineage": "043819a7-078b-da17-d78a-87a68b55dc07",
  "outputs": {},
  "resources": [
    {
      "module": "module.docker_composer",
      "mode": "data",
      "type": "template_file",
      "name": "init_compose_env",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "23fe2c2f83a63594eec8e193c4eb48de27e1f4726fef05623d5ca66907c486b0",
            "rendered": "DOCKER_INFLUXDB_INIT_MODE=setup\n\n## Environment variables used during the setup and operation of the stack\n#\n\n# Primary InfluxDB admin/superuser credentials\n#\nDOCKER_INFLUXDB_INIT_USERNAME=jon\nDOCKER_INFLUXDB_INIT_PASSWORD=adminpwd\nDOCKER_INFLUXDB_INIT_ADMIN_TOKEN=39477970d8cb52692dcfc689f1a2614ac074ca94e5e231489fbcaa00ccac7763\n\n# Primary InfluxDB organization \u0026 bucket definitions\n# \nDOCKER_INFLUXDB_INIT_ORG=jon\nDOCKER_INFLUXDB_INIT_BUCKET=telegraf\n\n# Primary InfluxDB bucket retention period\n#\n# NOTE: Valid units are nanoseconds (ns), microseconds(us), milliseconds (ms)\n# seconds (s), minutes (m), hours (h), days (d), and weeks (w).\nDOCKER_INFLUXDB_INIT_RETENTION=4d\n\n\n# InfluxDB port \u0026 hostname definitions\n#\nDOCKER_INFLUXDB_INIT_PORT=8086\nDOCKER_INFLUXDB_INIT_HOST=influxdb\n\n# Telegraf configuration file\n# \n# Will be mounted to container and used as telegraf configuration\nTELEGRAF_CFG_PATH=./telegraf/telegraf.conf\n\n# Grafana port definition\nGRAFANA_PORT=3000\n",
            "template": "DOCKER_INFLUXDB_INIT_MODE=${DOCKER_INFLUXDB_INIT_MODE}\n\n## Environment variables used during the setup and operation of the stack\n#\n\n# Primary InfluxDB admin/superuser credentials\n#\nDOCKER_INFLUXDB_INIT_USERNAME=${DOCKER_INFLUXDB_INIT_USERNAME}\nDOCKER_INFLUXDB_INIT_PASSWORD=${DOCKER_INFLUXDB_INIT_PASSWORD}\nDOCKER_INFLUXDB_INIT_ADMIN_TOKEN=${DOCKER_INFLUXDB_INIT_ADMIN_TOKEN}\n\n# Primary InfluxDB organization \u0026 bucket definitions\n# \nDOCKER_INFLUXDB_INIT_ORG=${DOCKER_INFLUXDB_INIT_ORG}\nDOCKER_INFLUXDB_INIT_BUCKET=${DOCKER_INFLUXDB_INIT_BUCKET}\n\n# Primary InfluxDB bucket retention period\n#\n# NOTE: Valid units are nanoseconds (ns), microseconds(us), milliseconds (ms)\n# seconds (s), minutes (m), hours (h), days (d), and weeks (w).\nDOCKER_INFLUXDB_INIT_RETENTION=${DOCKER_INFLUXDB_INIT_RETENTION}\n\n\n# InfluxDB port \u0026 hostname definitions\n#\nDOCKER_INFLUXDB_INIT_PORT=${DOCKER_INFLUXDB_INIT_PORT}\nDOCKER_INFLUXDB_INIT_HOST=${DOCKER_INFLUXDB_INIT_HOST}\n\n# Telegraf configuration file\n# \n# Will be mounted to container and used as telegraf configuration\nTELEGRAF_CFG_PATH=${TELEGRAF_CFG_PATH}\n\n# Grafana port definition\nGRAFANA_PORT=${GRAFANA_PORT}\n",
            "vars": {
              "DOCKER_INFLUXDB_INIT_ADMIN_TOKEN": "39477970d8cb52692dcfc689f1a2614ac074ca94e5e231489fbcaa00ccac7763",
              "DOCKER_INFLUXDB_INIT_BUCKET": "telegraf",
              "DOCKER_INFLUXDB_INIT_HOST": "influxdb",
              "DOCKER_INFLUXDB_INIT_MODE": "setup",
              "DOCKER_INFLUXDB_INIT_ORG": "jon",
              "DOCKER_INFLUXDB_INIT_PASSWORD": "adminpwd",
              "DOCKER_INFLUXDB_INIT_PORT": "8086",
              "DOCKER_INFLUXDB_INIT_RETENTION": "4d",
              "DOCKER_INFLUXDB_INIT_USERNAME": "jon",
              "GRAFANA_PORT": "3000",
              "TELEGRAF_CFG_PATH": "./telegraf/telegraf.conf"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.docker_composer",
      "mode": "data",
      "type": "template_file",
      "name": "init_telegraf_config",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "b477164efda3652c4525eb6a744e1b7a1f36bf797b895f2e6bf819caaaa050df",
            "rendered": "\n[global_tags]\n  # dc = \"us-east-1\" # will tag all metrics with dc=us-east-1\n  # rack = \"1a\"\n  ## Environment variables can be used as tags, and throughout the config file\n  # user = \"$USER\"\n\n\n# Configuration for telegraf agent\n[agent]\n  ## Default data collection interval for all inputs\n  interval = \"10s\"\n  ## Rounds collection interval to 'interval'\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\n  round_interval = true\n\n  ## Telegraf will send metrics to outputs in batches of at most\n  ## metric_batch_size metrics.\n  ## This controls the size of writes that Telegraf sends to output plugins.\n  metric_batch_size = 1000\n\n  ## Maximum number of unwritten metrics per output.  Increasing this value\n  ## allows for longer periods of output downtime without dropping metrics at the\n  ## cost of higher maximum memory usage.\n  metric_buffer_limit = 10000\n\n  ## Collection jitter is used to jitter the collection by a random amount.\n  ## Each plugin will sleep for a random time within jitter before collecting.\n  ## This can be used to avoid many plugins querying things like sysfs at the\n  ## same time, which can have a measurable effect on the system.\n  collection_jitter = \"0s\"\n\n  ## Collection offset is used to shift the collection by the given amount.\n  ## This can be be used to avoid many plugins querying constraint devices\n  ## at the same time by manually scheduling them in time.\n  # collection_offset = \"0s\"\n\n  ## Default flushing interval for all outputs. Maximum flush_interval will be\n  ## flush_interval + flush_jitter\n  flush_interval = \"10s\"\n  ## Jitter the flush interval by a random amount. This is primarily to avoid\n  ## large write spikes for users running a large number of telegraf instances.\n  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\n  flush_jitter = \"0s\"\n\n  ## Collected metrics are rounded to the precision specified. Precision is\n  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).\n  ## Valid time units are \"ns\", \"us\" (or \"Âµs\"), \"ms\", \"s\".\n  ##\n  ## By default or when set to \"0s\", precision will be set to the same\n  ## timestamp order as the collection interval, with the maximum being 1s:\n  ##   ie, when interval = \"10s\", precision will be \"1s\"\n  ##       when interval = \"250ms\", precision will be \"1ms\"\n  ##\n  ## Precision will NOT be used for service inputs. It is up to each individual\n  ## service input to set the timestamp at the appropriate precision.\n  precision = \"\"\n\n  ## Log at debug level.\n  # debug = false\n  ## Log only error level messages.\n  # quiet = false\n\n  ## Log target controls the destination for logs and can be one of \"file\",\n  ## \"stderr\" or, on Windows, \"eventlog\".  When set to \"file\", the output file\n  ## is determined by the \"logfile\" setting.\n  # logtarget = \"file\"\n\n  ## Name of the file to be logged to when using the \"file\" logtarget.  If set to\n  ## the empty string then logs are written to stderr.\n  # logfile = \"\"\n\n  ## The logfile will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.  Logs are rotated only when\n  ## written to, if there is no log activity rotation may be delayed.\n  # logfile_rotation_interval = \"0d\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # logfile_rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # logfile_rotation_max_archives = 5\n\n  ## Pick a timezone to use when logging or type 'local' for local time.\n  ## Example: America/Chicago\n  # log_with_timezone = \"\"\n\n  ## Override default hostname, if empty use os.Hostname()\n  hostname = \"\"\n  ## If set to true, do no set the \"host\" tag in the telegraf agent.\n  omit_hostname = false\n\n###############################################################################\n#                            OUTPUT PLUGINS                                   #\n###############################################################################\n\n\n# Configuration for sending metrics to InfluxDB\n# [[outputs.influxdb]]\n  ## The full HTTP or UDP URL for your InfluxDB instance.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  # urls = [\"unix:///var/run/influxdb.sock\"]\n  # urls = [\"udp://127.0.0.1:8089\"]\n  # urls = [\"http://127.0.0.1:8086\"]\n\n  ## The target database for metrics; will be created as needed.\n  ## For UDP url endpoint database needs to be configured on server side.\n  # database = \"telegraf\"\n\n  ## The value of this tag will be used to determine the database.  If this\n  ## tag is not set the 'database' option is used as the default.\n  # database_tag = \"\"\n\n  ## If true, the 'database_tag' will not be included in the written metric.\n  # exclude_database_tag = false\n\n  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using\n  ## Telegraf with a user without permissions to create databases or when the\n  ## database already exists.\n  # skip_database_creation = false\n\n  ## Name of existing retention policy to write to.  Empty string writes to\n  ## the default retention policy.  Only takes effect when using HTTP.\n  # retention_policy = \"\"\n\n  ## The value of this tag will be used to determine the retention policy.  If this\n  ## tag is not set the 'retention_policy' option is used as the default.\n  # retention_policy_tag = \"\"\n\n  ## If true, the 'retention_policy_tag' will not be included in the written metric.\n  # exclude_retention_policy_tag = false\n\n  ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\".\n  ## Only takes effect when using HTTP.\n  # write_consistency = \"any\"\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## HTTP Basic Auth\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## UDP payload size is the maximum packet size to send.\n  # udp_payload = \"512B\"\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: \"42u\".  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  # influx_uint_support = false\n\n\n# # Configuration for Amon Server to send metrics to.\n# [[outputs.amon]]\n#   ## Amon Server Key\n#   server_key = \"my-server-key\" # required.\n#\n#   ## Amon Instance URL\n#   amon_instance = \"https://youramoninstance\" # required\n#\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n\n\n# # Publishes metrics to an AMQP broker\n# [[outputs.amqp]]\n#   ## Brokers to publish to.  If multiple brokers are specified a random broker\n#   ## will be selected anytime a connection is established.  This can be\n#   ## helpful for load balancing when not using a dedicated load balancer.\n#   brokers = [\"amqp://localhost:5672/influxdb\"]\n#\n#   ## Maximum messages to send over a connection.  Once this is reached, the\n#   ## connection is closed and a new connection is made.  This can be helpful for\n#   ## load balancing when not using a dedicated load balancer.\n#   # max_messages = 0\n#\n#   ## Exchange to declare and publish to.\n#   exchange = \"telegraf\"\n#\n#   ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n#   # exchange_type = \"topic\"\n#\n#   ## If true, exchange will be passively declared.\n#   # exchange_passive = false\n#\n#   ## Exchange durability can be either \"transient\" or \"durable\".\n#   # exchange_durability = \"durable\"\n#\n#   ## Additional exchange arguments.\n#   # exchange_arguments = { }\n#   # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n#\n#   ## Authentication credentials for the PLAIN auth_method.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Auth method. PLAIN and EXTERNAL are supported\n#   ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n#   ## described here: https://www.rabbitmq.com/plugins.html\n#   # auth_method = \"PLAIN\"\n#\n#   ## Metric tag to use as a routing key.\n#   ##   ie, if this tag exists, its value will be used as the routing key\n#   # routing_tag = \"host\"\n#\n#   ## Static routing key.  Used when no routing_tag is set or as a fallback\n#   ## when the tag specified in routing tag is not found.\n#   # routing_key = \"\"\n#   # routing_key = \"telegraf\"\n#\n#   ## Delivery Mode controls if a published message is persistent.\n#   ##   One of \"transient\" or \"persistent\".\n#   # delivery_mode = \"transient\"\n#\n#   ## Static headers added to each published message.\n#   # headers = { }\n#   # headers = {\"database\" = \"telegraf\", \"retention_policy\" = \"default\"}\n#\n#   ## Connection timeout.  If not provided, will default to 5s.  0s means no\n#   ## timeout (not recommended).\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## If true use batch serialization format instead of line based delimiting.\n#   ## Only applies to data formats which are not line based such as JSON.\n#   ## Recommended to set to true.\n#   # use_batch_format = false\n#\n#   ## Content encoding for message payloads, can be set to \"gzip\" to or\n#   ## \"identity\" to apply no encoding.\n#   ##\n#   ## Please note that when use_batch_format = false each amqp message contains only\n#   ## a single metric, it is recommended to use compression with batch format\n#   ## for best results.\n#   # content_encoding = \"identity\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n\n\n# # Send metrics to Azure Application Insights\n# [[outputs.application_insights]]\n#   ## Instrumentation key of the Application Insights resource.\n#   instrumentation_key = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"\n#\n#   ## Regions that require endpoint modification https://docs.microsoft.com/en-us/azure/azure-monitor/app/custom-endpoints\n#   # endpoint_url = \"https://dc.services.visualstudio.com/v2/track\"\n#\n#   ## Timeout for closing (default: 5s).\n#   # timeout = \"5s\"\n#\n#   ## Enable additional diagnostic logging.\n#   # enable_diagnostic_logging = false\n#\n#   ## Context Tag Sources add Application Insights context tags to a tag value.\n#   ##\n#   ## For list of allowed context tag keys see:\n#   ## https://github.com/microsoft/ApplicationInsights-Go/blob/master/appinsights/contracts/contexttagkeys.go\n#   # [outputs.application_insights.context_tag_sources]\n#   #   \"ai.cloud.role\" = \"kubernetes_container_name\"\n#   #   \"ai.cloud.roleInstance\" = \"kubernetes_pod_name\"\n\n\n# # Sends metrics to Azure Data Explorer\n# [[outputs.azure_data_explorer]]\n#   ## Azure Data Explorer cluster endpoint\n#   ## ex: endpoint_url = \"https://clustername.australiasoutheast.kusto.windows.net\"\n#   endpoint_url = \"\"\n#\n#   ## The Azure Data Explorer database that the metrics will be ingested into.\n#   ## The plugin will NOT generate this database automatically, it's expected that this database already exists before ingestion.\n#   ## ex: \"exampledatabase\"\n#   database = \"\"\n#\n#   ## Timeout for Azure Data Explorer operations\n#   # timeout = \"20s\"\n#\n#   ## Type of metrics grouping used when pushing to Azure Data Explorer.\n#   ## Default is \"TablePerMetric\" for one table per different metric.\n#   ## For more information, please check the plugin README.\n#   # metrics_grouping_type = \"TablePerMetric\"\n#\n#   ## Name of the single table to store all the metrics (Only needed if metrics_grouping_type is \"SingleTable\").\n#   # table_name = \"\"\n#\n#   ## Creates tables and relevant mapping if set to true(default).\n#   ## Skips table and mapping creation if set to false, this is useful for running Telegraf with the lowest possible permissions i.e. table ingestor role.\n#   # create_tables = true\n\n\n# # Send aggregate metrics to Azure Monitor\n# [[outputs.azure_monitor]]\n#   ## Timeout for HTTP writes.\n#   # timeout = \"20s\"\n#\n#   ## Set the namespace prefix, defaults to \"Telegraf/\u003cinput-name\u003e\".\n#   # namespace_prefix = \"Telegraf/\"\n#\n#   ## Azure Monitor doesn't have a string value type, so convert string\n#   ## fields to dimensions (a.k.a. tags) if enabled. Azure Monitor allows\n#   ## a maximum of 10 dimensions so Telegraf will only send the first 10\n#   ## alphanumeric dimensions.\n#   # strings_as_dimensions = false\n#\n#   ## Both region and resource_id must be set or be available via the\n#   ## Instance Metadata service on Azure Virtual Machines.\n#   #\n#   ## Azure Region to publish metrics against.\n#   ##   ex: region = \"southcentralus\"\n#   # region = \"\"\n#   #\n#   ## The Azure Resource ID against which metric will be logged, e.g.\n#   ##   ex: resource_id = \"/subscriptions/\u003csubscription_id\u003e/resourceGroups/\u003cresource_group\u003e/providers/Microsoft.Compute/virtualMachines/\u003cvm_name\u003e\"\n#   # resource_id = \"\"\n#\n#   ## Optionally, if in Azure US Government, China or other sovereign\n#   ## cloud environment, set appropriate REST endpoint for receiving\n#   ## metrics. (Note: region may be unused in this context)\n#   # endpoint_url = \"https://monitoring.core.usgovcloudapi.net\"\n\n\n# # Configuration for Google Cloud BigQuery to send entries\n# [[outputs.bigquery]]\n#   ## Credentials File\n#   credentials_file = \"/path/to/service/account/key.json\"\n#\n#   ## Google Cloud Platform Project\n#   project = \"my-gcp-project\"\n#\n#   ## The namespace for the metric descriptor\n#   dataset = \"telegraf\"\n#\n#   ## Timeout for BigQuery operations.\n#   # timeout = \"5s\"\n#\n#   ## Character to replace hyphens on Metric name\n#   # replace_hyphen_to = \"_\"\n\n\n# # Publish Telegraf metrics to a Google Cloud PubSub topic\n# [[outputs.cloud_pubsub]]\n#   ## Required. Name of Google Cloud Platform (GCP) Project that owns\n#   ## the given PubSub topic.\n#   project = \"my-project\"\n#\n#   ## Required. Name of PubSub topic to publish metrics to.\n#   topic = \"my-topic\"\n#\n#   ## Required. Data format to consume.\n#   ## Each data format has its own unique set of configuration options.\n#   ## Read more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n#   ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n#   ## Application Default Credentials, which is preferred.\n#   # credentials_file = \"path/to/my/creds.json\"\n#\n#   ## Optional. If true, will send all metrics per write in one PubSub message.\n#   # send_batched = true\n#\n#   ## The following publish_* parameters specifically configures batching\n#   ## requests made to the GCP Cloud PubSub API via the PubSub Golang library. Read\n#   ## more here: https://godoc.org/cloud.google.com/go/pubsub#PublishSettings\n#\n#   ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n#   ## when it has this many PubSub messages. If send_batched is true,\n#   ## this is ignored and treated as if it were 1.\n#   # publish_count_threshold = 1000\n#\n#   ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n#   ## when it has this many PubSub messages. If send_batched is true,\n#   ## this is ignored and treated as if it were 1\n#   # publish_byte_threshold = 1000000\n#\n#   ## Optional. Specifically configures requests made to the PubSub API.\n#   # publish_num_go_routines = 2\n#\n#   ## Optional. Specifies a timeout for requests to the PubSub API.\n#   # publish_timeout = \"30s\"\n#\n#   ## Optional. If true, published PubSub message data will be base64-encoded.\n#   # base64_data = false\n#\n#   ## Optional. PubSub attributes to add to metrics.\n#   # [outputs.cloud_pubsub.attributes]\n#   #   my_attr = \"tag_value\"\n\n\n# # Configuration for AWS CloudWatch output.\n# [[outputs.cloudwatch]]\n#   ## Amazon REGION\n#   region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Namespace for the CloudWatch MetricDatums\n#   namespace = \"InfluxData/Telegraf\"\n#\n#   ## If you have a large amount of metrics, you should consider to send statistic\n#   ## values instead of raw metrics which could not only improve performance but\n#   ## also save AWS API cost. If enable this flag, this plugin would parse the required\n#   ## CloudWatch statistic fields (count, min, max, and sum) and send them to CloudWatch.\n#   ## You could use basicstats aggregator to calculate those fields. If not all statistic\n#   ## fields are available, all fields would still be sent as raw metrics.\n#   # write_statistics = false\n#\n#   ## Enable high resolution metrics of 1 second (if not enabled, standard resolution are of 60 seconds precision)\n#   # high_resolution_metrics = false\n\n\n# # Configuration for AWS CloudWatchLogs output.\n# [[outputs.cloudwatch_logs]]\n# ## The region is the Amazon region that you wish to connect to.\n# ## Examples include but are not limited to:\n# ## - us-west-1\n# ## - us-west-2\n# ## - us-east-1\n# ## - ap-southeast-1\n# ## - ap-southeast-2\n# ## ...\n# region = \"us-east-1\"\n#\n# ## Amazon Credentials\n# ## Credentials are loaded in the following order\n# ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n# ## 2) Assumed credentials via STS if role_arn is specified\n# ## 3) explicit credentials from 'access_key' and 'secret_key'\n# ## 4) shared profile from 'profile'\n# ## 5) environment variables\n# ## 6) shared credentials file\n# ## 7) EC2 Instance Profile\n# #access_key = \"\"\n# #secret_key = \"\"\n# #token = \"\"\n# #role_arn = \"\"\n# #web_identity_token_file = \"\"\n# #role_session_name = \"\"\n# #profile = \"\"\n# #shared_credential_file = \"\"\n#\n# ## Endpoint to make request against, the correct endpoint is automatically\n# ## determined and this option should only be set if you wish to override the\n# ## default.\n# ##   ex: endpoint_url = \"http://localhost:8000\"\n# # endpoint_url = \"\"\n#\n# ## Cloud watch log group. Must be created in AWS cloudwatch logs upfront!\n# ## For example, you can specify the name of the k8s cluster here to group logs from all cluster in oine place\n# log_group = \"my-group-name\"\n#\n# ## Log stream in log group\n# ## Either log group name or reference to metric attribute, from which it can be parsed:\n# ## tag:\u003cTAG_NAME\u003e or field:\u003cFIELD_NAME\u003e. If log stream is not exist, it will be created.\n# ## Since AWS is not automatically delete logs streams with expired logs entries (i.e. empty log stream)\n# ## you need to put in place appropriate house-keeping (https://forums.aws.amazon.com/thread.jspa?threadID=178855)\n# log_stream = \"tag:location\"\n#\n# ## Source of log data - metric name\n# ## specify the name of the metric, from which the log data should be retrieved.\n# ## I.e., if you  are using docker_log plugin to stream logs from container, then\n# ## specify log_data_metric_name  = \"docker_log\"\n# log_data_metric_name  = \"docker_log\"\n#\n# ## Specify from which metric attribute the log data should be retrieved:\n# ## tag:\u003cTAG_NAME\u003e or field:\u003cFIELD_NAME\u003e.\n# ## I.e., if you  are using docker_log plugin to stream logs from container, then\n# ## specify log_data_source  = \"field:message\"\n# log_data_source  = \"field:message\"\n\n\n# # Configuration for CrateDB to send metrics to.\n# [[outputs.cratedb]]\n#   # A github.com/jackc/pgx/v4 connection string.\n#   # See https://pkg.go.dev/github.com/jackc/pgx/v4#ParseConfig\n#   url = \"postgres://user:password@localhost/schema?sslmode=disable\"\n#   # Timeout for all CrateDB queries.\n#   timeout = \"5s\"\n#   # Name of the table to store metrics in.\n#   table = \"metrics\"\n#   # If true, and the metrics table does not exist, create it automatically.\n#   table_create = true\n#   # The character(s) to replace any '.' in an object key with\n#   key_separator = \"_\"\n\n\n# # Configuration for DataDog API to send metrics to.\n# [[outputs.datadog]]\n#   ## Datadog API key\n#   apikey = \"my-secret-key\"\n#\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n#\n#   ## Write URL override; useful for debugging.\n#   # url = \"https://app.datadoghq.com/api/v1/series\"\n#\n#   ## Set http_proxy (telegraf uses the system wide proxy settings if it isn't set)\n#   # http_proxy_url = \"http://localhost:8888\"\n#\n#   ## Override the default (none) compression used to send data.\n#   ## Supports: \"zlib\", \"none\"\n#   # compression = \"none\"\n\n\n# # Send metrics to nowhere at all\n# [[outputs.discard]]\n#   # no configuration\n\n\n# # Send telegraf metrics to a Dynatrace environment\n# [[outputs.dynatrace]]\n#   ## For usage with the Dynatrace OneAgent you can omit any configuration,\n#   ## the only requirement is that the OneAgent is running on the same host.\n#   ## Only setup environment url and token if you want to monitor a Host without the OneAgent present.\n#   ##\n#   ## Your Dynatrace environment URL.\n#   ## For Dynatrace OneAgent you can leave this empty or set it to \"http://127.0.0.1:14499/metrics/ingest\" (default)\n#   ## For Dynatrace SaaS environments the URL scheme is \"https://{your-environment-id}.live.dynatrace.com/api/v2/metrics/ingest\"\n#   ## For Dynatrace Managed environments the URL scheme is \"https://{your-domain}/e/{your-environment-id}/api/v2/metrics/ingest\"\n#   url = \"\"\n#\n#   ## Your Dynatrace API token.\n#   ## Create an API token within your Dynatrace environment, by navigating to Settings \u003e Integration \u003e Dynatrace API\n#   ## The API token needs data ingest scope permission. When using OneAgent, no API token is required.\n#   api_token = \"\"\n#\n#   ## Optional prefix for metric names (e.g.: \"telegraf\")\n#   prefix = \"telegraf\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional flag for ignoring tls certificate check\n#   # insecure_skip_verify = false\n#\n#\n#   ## Connection timeout, defaults to \"5s\" if not set.\n#   timeout = \"5s\"\n#\n#   ## If you want metrics to be treated and reported as delta counters, add the metric names here\n#   additional_counters = [ ]\n#\n#   ## Optional dimensions to be added to every metric\n#   # [outputs.dynatrace.default_dimensions]\n#   # default_key = \"default value\"\n\n\n# # Configuration for Elasticsearch to send metrics to.\n# [[outputs.elasticsearch]]\n#   ## The full HTTP endpoint URL for your Elasticsearch instance\n#   ## Multiple urls can be specified as part of the same cluster,\n#   ## this means that only ONE of the urls will be written to each interval.\n#   urls = [ \"http://node1.es.example.com:9200\" ] # required.\n#   ## Elasticsearch client timeout, defaults to \"5s\" if not set.\n#   timeout = \"5s\"\n#   ## Set to true to ask Elasticsearch a list of all cluster nodes,\n#   ## thus it is not necessary to list all nodes in the urls config option.\n#   enable_sniffer = false\n#   ## Set to true to enable gzip compression\n#   enable_gzip = false\n#   ## Set the interval to check if the Elasticsearch nodes are available\n#   ## Setting to \"0s\" will disable the health check (not recommended in production)\n#   health_check_interval = \"10s\"\n#   ## HTTP basic authentication details\n#   # username = \"telegraf\"\n#   # password = \"mypassword\"\n#   ## HTTP bearer token authentication details\n#   # auth_bearer_token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9\"\n#\n#   ## Index Config\n#   ## The target index for metrics (Elasticsearch will create if it not exists).\n#   ## You can use the date specifiers below to create indexes per time frame.\n#   ## The metric timestamp will be used to decide the destination index name\n#   # %Y - year (2016)\n#   # %y - last two digits of year (00..99)\n#   # %m - month (01..12)\n#   # %d - day of month (e.g., 01)\n#   # %H - hour (00..23)\n#   # %V - week of the year (ISO week) (01..53)\n#   ## Additionally, you can specify a tag name using the notation {{tag_name}}\n#   ## which will be used as part of the index name. If the tag does not exist,\n#   ## the default tag value will be used.\n#   # index_name = \"telegraf-{{host}}-%Y.%m.%d\"\n#   # default_tag_value = \"none\"\n#   index_name = \"telegraf-%Y.%m.%d\" # required.\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Template Config\n#   ## Set to true if you want telegraf to manage its index template.\n#   ## If enabled it will create a recommended index template for telegraf indexes\n#   manage_template = true\n#   ## The template name used for telegraf indexes\n#   template_name = \"telegraf\"\n#   ## Set to true if you want telegraf to overwrite an existing template\n#   overwrite_template = false\n#   ## If set to true a unique ID hash will be sent as sha256(concat(timestamp,measurement,series-hash)) string\n#   ## it will enable data resend and update metric points avoiding duplicated metrics with diferent id's\n#   force_document_id = false\n#\n#   ## Specifies the handling of NaN and Inf values.\n#   ## This option can have the following values:\n#   ##    none    -- do not modify field-values (default); will produce an error if NaNs or infs are encountered\n#   ##    drop    -- drop fields containing NaNs or infs\n#   ##    replace -- replace with the value in \"float_replacement_value\" (default: 0.0)\n#   ##               NaNs and inf will be replaced with the given number, -inf with the negative of that number\n#   # float_handling = \"none\"\n#   # float_replacement_value = 0.0\n#\n#   ## Pipeline Config\n#   ## To use a ingest pipeline, set this to the name of the pipeline you want to use.\n#   # use_pipeline = \"my_pipeline\"\n#   ## Additionally, you can specify a tag name using the notation {{tag_name}}\n#   ## which will be used as part of the pipeline name. If the tag does not exist,\n#   ## the default pipeline will be used as the pipeline. If no default pipeline is set,\n#   ## no pipeline is used for the metric.\n#   # use_pipeline = \"{{es_pipeline}}\"\n#   # default_pipeline = \"my_pipeline\"\n\n\n# # Configuration for Event Hubs output plugin\n# [[outputs.event_hubs]]\n#   ## The full connection string to the Event Hub (required)\n#   ## The shared access key must have \"Send\" permissions on the target Event Hub.\n#   connection_string = \"Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName\"\n#\n#   ## Client timeout (defaults to 30s)\n#   # timeout = \"30s\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"json\"\n\n\n# # Send metrics to command as input over stdin\n# [[outputs.exec]]\n#   ## Command to ingest metrics via stdin.\n#   command = [\"tee\", \"-a\", \"/dev/null\"]\n#\n#   ## Timeout for command to complete.\n#   # timeout = \"5s\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n\n\n# # Run executable as long-running output plugin\n# [[outputs.execd]]\n#   ## Program to run as daemon\n#   command = [\"my-telegraf-output\", \"--some-flag\", \"value\"]\n#\n#   ## Delay before the process is restarted after an unexpected termination\n#   restart_delay = \"10s\"\n#\n#   ## Data format to export.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send telegraf metrics to file(s)\n# [[outputs.file]]\n#   ## Files to write to, \"stdout\" is a specially handled file.\n#   files = [\"stdout\", \"/tmp/metrics.out\"]\n#\n#   ## Use batch serialization format instead of line based delimiting.  The\n#   ## batch format allows for the production of non line based output formats and\n#   ## may more efficiently encode metric groups.\n#   # use_batch_format = false\n#\n#   ## The file will be rotated after the time interval specified.  When set\n#   ## to 0 no time based rotation is performed.\n#   # rotation_interval = \"0d\"\n#\n#   ## The logfile will be rotated when it becomes larger than the specified\n#   ## size.  When set to 0 no size based rotation is performed.\n#   # rotation_max_size = \"0MB\"\n#\n#   ## Maximum number of rotated archives to keep, any older logs are deleted.\n#   ## If set to -1, no archives are removed.\n#   # rotation_max_archives = 5\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Configuration for Graphite server to send metrics to\n# [[outputs.graphite]]\n#   ## TCP endpoint for your graphite instance.\n#   ## If multiple endpoints are configured, output will be load balanced.\n#   ## Only one of the endpoints will be written to with each iteration.\n#   servers = [\"localhost:2003\"]\n#   ## Prefix metrics name\n#   prefix = \"\"\n#   ## Graphite output template\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   template = \"host.tags.measurement.field\"\n#\n#   ## Enable Graphite tags support\n#   # graphite_tag_support = false\n#\n#   ## Define how metric names and tags are sanitized; options are \"strict\", or \"compatible\"\n#   ## strict - Default method, and backwards compatible with previous versionf of Telegraf\n#   ## compatible - More relaxed sanitizing when using tags, and compatible with the graphite spec\n#   # graphite_tag_sanitize_mode = \"strict\"\n#\n#   ## Character for separating metric name and field for Graphite tags\n#   # graphite_separator = \".\"\n#\n#   ## Graphite templates patterns\n#   ## 1. Template for cpu\n#   ## 2. Template for disk*\n#   ## 3. Default template\n#   # templates = [\n#   #  \"cpu tags.measurement.host.field\",\n#   #  \"disk* measurement.field\",\n#   #  \"host.measurement.tags.field\"\n#   #]\n#\n#   ## timeout in seconds for the write connection to graphite\n#   timeout = 2\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Send telegraf metrics to graylog\n# [[outputs.graylog]]\n#   ## Endpoints for your graylog instances.\n#   servers = [\"udp://127.0.0.1:12201\"]\n#\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n#\n#   ## The field to use as the GELF short_message, if unset the static string\n#   ## \"telegraf\" will be used.\n#   ##   example: short_message_field = \"message\"\n#   # short_message_field = \"\"\n#\n#   ## According to GELF payload specification, additional fields names must be prefixed\n#   ## with an underscore. Previous versions did not prefix custom field 'name' with underscore.\n#   ## Set to true for backward compatibility.\n#   # name_field_no_prefix = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Send telegraf metrics to GroundWork Monitor\n# [[outputs.groundwork]]\n#   ## URL of your groundwork instance.\n#   url = \"https://groundwork.example.com\"\n#\n#   ## Agent uuid for GroundWork API Server.\n#   agent_id = \"\"\n#\n#   ## Username and password to access GroundWork API.\n#   username = \"\"\n#   password = \"\"\n#\n#   ## Default display name for the host with services(metrics).\n#   # default_host = \"telegraf\"\n#\n#   ## Default service state.\n#   # default_service_state = \"SERVICE_OK\"\n#\n#   ## The name of the tag that contains the hostname.\n#   # resource_tag = \"host\"\n#\n#   ## The name of the tag that contains the host group name.\n#   # group_tag = \"group\"\n\n\n# # Configurable HTTP health check resource based on metrics\n# [[outputs.health]]\n#   ## Address and port to listen on.\n#   ##   ex: service_address = \"http://localhost:8080\"\n#   ##       service_address = \"unix:///var/run/telegraf-health.sock\"\n#   # service_address = \"http://:8080\"\n#\n#   ## The maximum duration for reading the entire request.\n#   # read_timeout = \"5s\"\n#   ## The maximum duration for writing the entire response.\n#   # write_timeout = \"5s\"\n#\n#   ## Username and password to accept for HTTP basic authentication.\n#   # basic_username = \"user1\"\n#   # basic_password = \"secret\"\n#\n#   ## Allowed CA certificates for client certificates.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## TLS server certificate and private key.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## One or more check sub-tables should be defined, it is also recommended to\n#   ## use metric filtering to limit the metrics that flow into this output.\n#   ##\n#   ## When using the default buffer sizes, this example will fail when the\n#   ## metric buffer is half full.\n#   ##\n#   ## namepass = [\"internal_write\"]\n#   ## tagpass = { output = [\"influxdb\"] }\n#   ##\n#   ## [[outputs.health.compares]]\n#   ##   field = \"buffer_size\"\n#   ##   lt = 5000.0\n#   ##\n#   ## [[outputs.health.contains]]\n#   ##   field = \"buffer_size\"\n\n\n# # A plugin that can transmit metrics over HTTP\n# [[outputs.http]]\n#   ## URL is the address to send metrics to\n#   url = \"http://127.0.0.1:8080/telegraf\"\n#\n#   ## Timeout for HTTP message\n#   # timeout = \"5s\"\n#\n#   ## HTTP method, one of: \"POST\" or \"PUT\"\n#   # method = \"POST\"\n#\n#   ## HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## OAuth2 Client Credentials Grant\n#   # client_id = \"clientid\"\n#   # client_secret = \"secret\"\n#   # token_url = \"https://indentityprovider/oauth2/v1/token\"\n#   # scopes = [\"urn:opc:idm:__myscopes__\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional Cookie authentication\n#   # cookie_auth_url = \"https://localhost/authMe\"\n#   # cookie_auth_method = \"POST\"\n#   # cookie_auth_username = \"username\"\n#   # cookie_auth_password = \"pa$$word\"\n#   # cookie_auth_headers = '{\"Content-Type\": \"application/json\", \"X-MY-HEADER\":\"hello\"}'\n#   # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n#   ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n#   # cookie_auth_renewal = \"5m\"\n#\n#   ## Data format to output.\n#   ## Each data format has it's own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n#\n#   ## Use batch serialization format (default) instead of line based format.\n#   ## Batch format is more efficient and should be used unless line based\n#   ## format is really needed.\n#   # use_batch_format = true\n#\n#   ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## Additional HTTP headers\n#   # [outputs.http.headers]\n#   #   # Should be set manually to \"application/json\" for json data_format\n#   #   Content-Type = \"text/plain; charset=utf-8\"\n#\n#   ## Idle (keep-alive) connection timeout.\n#   ## Maximum amount of time before idle connection is closed.\n#   ## Zero means no limit.\n#   # idle_conn_timeout = 0\n#\n#   ## Amazon Region\n#   #region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n\n\n# # Configuration for sending metrics to InfluxDB\n[[outputs.influxdb_v2]]\n  ## The URLs of the InfluxDB cluster nodes.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  urls = [\"http://ec2-35-80-237-168.us-west-2.compute.amazonaws.com:8086\"]\n\n  ## Token for authentication.\n  token = \"39477970d8cb52692dcfc689f1a2614ac074ca94e5e231489fbcaa00ccac7763\"\n\n  ## Organization is the name of the organization you wish to write to; must exist.\n  organization = \"jon\"\n\n  ## Destination bucket to write into.\n  bucket = \"telegraf\"\n\n#   ## The value of this tag will be used to determine the bucket.  If this\n#   ## tag is not set the 'bucket' option is used as the default.\n#   # bucket_tag = \"\"\n#\n#   ## If true, the bucket tag will not be added to the metric.\n#   # exclude_bucket_tag = false\n#\n#   ## Timeout for HTTP messages.\n#   # timeout = \"5s\"\n#\n#   ## Additional HTTP headers\n#   # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## HTTP Proxy override, if unset values the standard proxy environment\n#   ## variables are consulted to determine which proxy, if any, should be used.\n#   # http_proxy = \"http://corporate.proxy:3128\"\n#\n#   ## HTTP User-Agent\n#   # user_agent = \"telegraf\"\n#\n#   ## Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"gzip\"\n#\n#   ## Enable or disable uint support for writing uints influxdb 2.0.\n#   # influx_uint_support = false\n#\n#   ## Optional TLS Config for use on HTTP connections.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  insecure_skip_verify = false\n\n\n# # Configuration for sending metrics to an Instrumental project\n# [[outputs.instrumental]]\n#   ## Project API Token (required)\n#   api_token = \"API Token\" # required\n#   ## Prefix the metrics with a given name\n#   prefix = \"\"\n#   ## Stats output template (Graphite formatting)\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n#   template = \"host.tags.measurement.field\"\n#   ## Timeout in seconds to connect\n#   timeout = \"2s\"\n#   ## Display Communication to Instrumental\n#   debug = false\n\n\n# # Configuration for the Kafka server to send metrics to\n# [[outputs.kafka]]\n#   ## URLs of kafka brokers\n#   brokers = [\"localhost:9092\"]\n#   ## Kafka topic for producer messages\n#   topic = \"telegraf\"\n#\n#   ## The value of this tag will be used as the topic.  If not set the 'topic'\n#   ## option is used.\n#   # topic_tag = \"\"\n#\n#   ## If true, the 'topic_tag' will be removed from to the metric.\n#   # exclude_topic_tag = false\n#\n#   ## Optional Client id\n#   # client_id = \"Telegraf\"\n#\n#   ## Set the minimal supported Kafka version.  Setting this enables the use of new\n#   ## Kafka features and APIs.  Of particular interest, lz4 compression\n#   ## requires at least version 0.10.0.0.\n#   ##   ex: version = \"1.1.0\"\n#   # version = \"\"\n#\n#   ## Optional topic suffix configuration.\n#   ## If the section is omitted, no suffix is used.\n#   ## Following topic suffix methods are supported:\n#   ##   measurement - suffix equals to separator + measurement's name\n#   ##   tags        - suffix equals to separator + specified tags' values\n#   ##                 interleaved with separator\n#\n#   ## Suffix equals to \"_\" + measurement name\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"measurement\"\n#   #   separator = \"_\"\n#\n#   ## Suffix equals to \"__\" + measurement's \"foo\" tag value.\n#   ##   If there's no such a tag, suffix equals to an empty string\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"tags\"\n#   #   keys = [\"foo\"]\n#   #   separator = \"__\"\n#\n#   ## Suffix equals to \"_\" + measurement's \"foo\" and \"bar\"\n#   ##   tag values, separated by \"_\". If there is no such tags,\n#   ##   their values treated as empty strings.\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"tags\"\n#   #   keys = [\"foo\", \"bar\"]\n#   #   separator = \"_\"\n#\n#   ## The routing tag specifies a tagkey on the metric whose value is used as\n#   ## the message key.  The message key is used to determine which partition to\n#   ## send the message to.  This tag is prefered over the routing_key option.\n#   routing_tag = \"host\"\n#\n#   ## The routing key is set as the message key and used to determine which\n#   ## partition to send the message to.  This value is only used when no\n#   ## routing_tag is set or as a fallback when the tag specified in routing tag\n#   ## is not found.\n#   ##\n#   ## If set to \"random\", a random value will be generated for each message.\n#   ##\n#   ## When unset, no message key is added and each message is routed to a random\n#   ## partition.\n#   ##\n#   ##   ex: routing_key = \"random\"\n#   ##       routing_key = \"telegraf\"\n#   # routing_key = \"\"\n#\n#   ## Compression codec represents the various compression codecs recognized by\n#   ## Kafka in messages.\n#   ##  0 : None\n#   ##  1 : Gzip\n#   ##  2 : Snappy\n#   ##  3 : LZ4\n#   ##  4 : ZSTD\n#   # compression_codec = 0\n#\n#   ## Idempotent Writes\n#   ## If enabled, exactly one copy of each message is written.\n#   # idempotent_writes = false\n#\n#   ##  RequiredAcks is used in Produce Requests to tell the broker how many\n#   ##  replica acknowledgements it must see before responding\n#   ##   0 : the producer never waits for an acknowledgement from the broker.\n#   ##       This option provides the lowest latency but the weakest durability\n#   ##       guarantees (some data will be lost when a server fails).\n#   ##   1 : the producer gets an acknowledgement after the leader replica has\n#   ##       received the data. This option provides better durability as the\n#   ##       client waits until the server acknowledges the request as successful\n#   ##       (only messages that were written to the now-dead leader but not yet\n#   ##       replicated will be lost).\n#   ##   -1: the producer gets an acknowledgement after all in-sync replicas have\n#   ##       received the data. This option provides the best durability, we\n#   ##       guarantee that no messages will be lost as long as at least one in\n#   ##       sync replica remains.\n#   # required_acks = -1\n#\n#   ## The maximum number of times to retry sending a metric before failing\n#   ## until the next flush.\n#   # max_retry = 3\n#\n#   ## The maximum permitted size of a message. Should be set equal to or\n#   ## smaller than the broker's 'message.max.bytes'.\n#   # max_message_bytes = 1000000\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional SOCKS5 proxy to use when connecting to brokers\n#   # socks5_enabled = true\n#   # socks5_address = \"127.0.0.1:1080\"\n#   # socks5_username = \"alice\"\n#   # socks5_password = \"pass123\"\n#\n#   ## Optional SASL Config\n#   # sasl_username = \"kafka\"\n#   # sasl_password = \"secret\"\n#\n#   ## Optional SASL:\n#   ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n#   ## (defaults to PLAIN)\n#   # sasl_mechanism = \"\"\n#\n#   ## used if sasl_mechanism is GSSAPI (experimental)\n#   # sasl_gssapi_service_name = \"\"\n#   # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n#   # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n#   # sasl_gssapi_kerberos_config_path = \"/\"\n#   # sasl_gssapi_realm = \"realm\"\n#   # sasl_gssapi_key_tab_path = \"\"\n#   # sasl_gssapi_disable_pafxfast = false\n#\n#   ## used if sasl_mechanism is OAUTHBEARER (experimental)\n#   # sasl_access_token = \"\"\n#\n#   ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n#   # sasl_version = 1\n#\n#   # Disable Kafka metadata full fetch\n#   # metadata_full = false\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n\n\n# # Configuration for the AWS Kinesis output.\n# [[outputs.kinesis]]\n#   ## Amazon REGION of kinesis endpoint.\n#   region = \"ap-southeast-2\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Kinesis StreamName must exist prior to starting telegraf.\n#   streamname = \"StreamName\"\n#\n#   ## The partition key can be calculated using one of several methods:\n#   ##\n#   ## Use a static value for all writes:\n#   #  [outputs.kinesis.partition]\n#   #    method = \"static\"\n#   #    key = \"howdy\"\n#   #\n#   ## Use a random partition key on each write:\n#   #  [outputs.kinesis.partition]\n#   #    method = \"random\"\n#   #\n#   ## Use the measurement name as the partition key:\n#   #  [outputs.kinesis.partition]\n#   #    method = \"measurement\"\n#   #\n#   ## Use the value of a tag for all writes, if the tag is not set the empty\n#   ## default option will be used. When no default, defaults to \"telegraf\"\n#   #  [outputs.kinesis.partition]\n#   #    method = \"tag\"\n#   #    key = \"host\"\n#   #    default = \"mykey\"\n#\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n#\n#   ## debug will show upstream aws messages.\n#   debug = false\n\n\n# # Configuration for Librato API to send metrics to.\n# [[outputs.librato]]\n#   ## Librato API Docs\n#   ## http://dev.librato.com/v1/metrics-authentication\n#   ## Librato API user\n#   api_user = \"telegraf@influxdb.com\" # required.\n#   ## Librato API token\n#   api_token = \"my-secret-token\" # required.\n#   ## Debug\n#   # debug = false\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n#   ## Output source Template (same as graphite buckets)\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n#   ## This template is used in librato's source (not metric's name)\n#   template = \"host\"\n#\n\n\n# # Send aggregate metrics to Logz.io\n# [[outputs.logzio]]\n#   ## Connection timeout, defaults to \"5s\" if not set.\n#   timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Logz.io account token\n#   token = \"your logz.io token\" # required\n#\n#   ## Use your listener URL for your Logz.io account region.\n#   # url = \"https://listener.logz.io:8071\"\n\n\n# # Send logs to Loki\n# [[outputs.loki]]\n#   ## The domain of Loki\n#   domain = \"https://loki.domain.tld\"\n#\n#   ## Endpoint to write api\n#   # endpoint = \"/loki/api/v1/push\"\n#\n#   ## Connection timeout, defaults to \"5s\" if not set.\n#   # timeout = \"5s\"\n#\n#   ## Basic auth credential\n#   # username = \"loki\"\n#   # password = \"pass\"\n#\n#   ## Additional HTTP headers\n#   # http_headers = {\"X-Scope-OrgID\" = \"1\"}\n#\n#   ## If the request must be gzip encoded\n#   # gzip_request = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n\n\n# # Sends metrics to MongoDB\n# [[outputs.mongodb]]\n#   # connection string examples for mongodb\n#   dsn = \"mongodb://localhost:27017\"\n#   # dsn = \"mongodb://mongod1:27017,mongod2:27017,mongod3:27017/admin\u0026replicaSet=myReplSet\u0026w=1\"\n#\n#   # overrides serverSelectionTimeoutMS in dsn if set\n#   # timeout = \"30s\"\n#\n#   # default authentication, optional\n#   # authentication = \"NONE\"\n#\n#   # for SCRAM-SHA-256 authentication\n#   # authentication = \"SCRAM\"\n#   # username = \"root\"\n#   # password = \"***\"\n#\n#   # for x509 certificate authentication\n#   # authentication = \"X509\"\n#   # tls_ca = \"ca.pem\"\n#   # tls_key = \"client.pem\"\n#   # # tls_key_pwd = \"changeme\" # required for encrypted tls_key\n#   # insecure_skip_verify = false\n#\n#   # database to store measurements and time series collections\n#   # database = \"telegraf\"\n#\n#   # granularity can be seconds, minutes, or hours.\n#   # configuring this value will be based on your input collection frequency.\n#   # see https://docs.mongodb.com/manual/core/timeseries-collections/#create-a-time-series-collection\n#   # granularity = \"seconds\"\n#\n#   # optionally set a TTL to automatically expire documents from the measurement collections.\n#   # ttl = \"360h\"\n\n\n# # Configuration for MQTT server to send metrics to\n# [[outputs.mqtt]]\n#   ## MQTT Brokers\n#   ## The list of brokers should only include the hostname or IP address and the\n#   ## port to the broker. This should follow the format '{host}:{port}'. For\n#   ## example, \"localhost:1883\" or \"127.0.0.1:8883\".\n#   servers = [\"localhost:1883\"]\n#\n#   ## MQTT Topic for Producer Messages\n#   ## MQTT outputs send metrics to this topic format:\n#   ## \u003ctopic_prefix\u003e/\u003chostname\u003e/\u003cpluginname\u003e/ (e.g. prefix/web01.example.com/mem)\n#   topic_prefix = \"telegraf\"\n#\n#   ## QoS policy for messages\n#   ## The mqtt QoS policy for sending messages.\n#   ## See https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.0.0/com.ibm.mq.dev.doc/q029090_.htm\n#   ##   0 = at most once\n#   ##   1 = at least once\n#   ##   2 = exactly once\n#   # qos = 2\n#\n#   ## Keep Alive\n#   ## Defines the maximum length of time that the broker and client may not\n#   ## communicate. Defaults to 0 which turns the feature off.\n#   ##\n#   ## For version v2.0.12 and later mosquitto there is a bug\n#   ## (see https://github.com/eclipse/mosquitto/issues/2117), which requires\n#   ## this to be non-zero. As a reference eclipse/paho.mqtt.golang defaults to 30.\n#   # keep_alive = 0\n#\n#   ## username and password to connect MQTT server.\n#   # username = \"telegraf\"\n#   # password = \"metricsmetricsmetricsmetrics\"\n#\n#   ## client ID\n#   ## The unique client id to connect MQTT server. If this parameter is not set\n#   ## then a random ID is generated.\n#   # client_id = \"\"\n#\n#   ## Timeout for write operations. default: 5s\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## When true, metrics will be sent in one MQTT message per flush. Otherwise,\n#   ## metrics are written one metric per MQTT message.\n#   # batch = false\n#\n#   ## When true, metric will have RETAIN flag set, making broker cache entries until someone\n#   ## actually reads it\n#   # retain = false\n#\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send telegraf measurements to NATS\n# [[outputs.nats]]\n#   ## URLs of NATS servers\n#   servers = [\"nats://localhost:4222\"]\n#\n#   ## Optional client name\n#   # name = \"\"\n#\n#   ## Optional credentials\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional NATS 2.0 and NATS NGS compatible user credentials\n#   # credentials = \"/etc/telegraf/nats.creds\"\n#\n#   ## NATS subject for producer messages\n#   subject = \"telegraf\"\n#\n#   ## Use Transport Layer Security\n#   # secure = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send metrics to New Relic metrics endpoint\n# [[outputs.newrelic]]\n#   ## The 'insights_key' parameter requires a NR license key.\n#   ## New Relic recommends you create one\n#   ## with a convenient name such as TELEGRAF_INSERT_KEY.\n#   ## reference: https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#ingest-license-key\n#   # insights_key = \"New Relic License Key Here\"\n#\n#   ## Prefix to add to add to metric name for easy identification.\n#   ## This is very useful if your metric names are ambiguous.\n#   # metric_prefix = \"\"\n#\n#   ## Timeout for writes to the New Relic API.\n#   # timeout = \"15s\"\n#\n#   ## HTTP Proxy override. If unset use values from the standard\n#   ## proxy environment variables to determine proxy, if any.\n#   # http_proxy = \"http://corporate.proxy:3128\"\n#\n#   ## Metric URL override to enable geographic location endpoints.\n#   # If not set use values from the standard\n#   # metric_url = \"https://metric-api.newrelic.com/metric/v1\"\n\n\n# # Send telegraf measurements to NSQD\n# [[outputs.nsq]]\n#   ## Location of nsqd instance listening on TCP\n#   server = \"localhost:4150\"\n#   ## NSQ topic for producer messages\n#   topic = \"telegraf\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send OpenTelemetry metrics over gRPC\n# [[outputs.opentelemetry]]\n#   ## Override the default (localhost:4317) OpenTelemetry gRPC service\n#   ## address:port\n#   # service_address = \"localhost:4317\"\n#\n#   ## Override the default (5s) request timeout\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config.\n#   ##\n#   ## Root certificates for verifying server certificates encoded in PEM format.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   ## The public and private keypairs for the client encoded in PEM format.\n#   ## May contain intermediate certificates.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS, but skip TLS chain and host verification.\n#   # insecure_skip_verify = false\n#   ## Send the specified TLS server name via SNI.\n#   # tls_server_name = \"foo.example.com\"\n#\n#   ## Override the default (gzip) compression used to send data.\n#   ## Supports: \"gzip\", \"none\"\n#   # compression = \"gzip\"\n#\n#   ## Additional OpenTelemetry resource attributes\n#   # [outputs.opentelemetry.attributes]\n#   # \"service.name\" = \"demo\"\n#\n#   ## Additional gRPC request metadata\n#   # [outputs.opentelemetry.headers]\n#   # key1 = \"value1\"\n\n\n# # Configuration for OpenTSDB server to send metrics to\n# [[outputs.opentsdb]]\n#   ## prefix for metrics keys\n#   prefix = \"my.specific.prefix.\"\n#\n#   ## DNS name of the OpenTSDB server\n#   ## Using \"opentsdb.example.com\" or \"tcp://opentsdb.example.com\" will use the\n#   ## telnet API. \"http://opentsdb.example.com\" will use the Http API.\n#   host = \"opentsdb.example.com\"\n#\n#   ## Port of the OpenTSDB server\n#   port = 4242\n#\n#   ## Number of data points to send to OpenTSDB in Http requests.\n#   ## Not used with telnet API.\n#   http_batch_size = 50\n#\n#   ## URI Path for Http requests to OpenTSDB.\n#   ## Used in cases where OpenTSDB is located behind a reverse proxy.\n#   http_path = \"/api/put\"\n#\n#   ## Debug true - Prints OpenTSDB communication\n#   debug = false\n#\n#   ## Separator separates measurement name from field\n#   separator = \"_\"\n\n\n# # Configuration for the Prometheus client to spawn\n# [[outputs.prometheus_client]]\n#   ## Address to listen on\n#   listen = \":9273\"\n#\n#   ## Metric version controls the mapping from Telegraf metrics into\n#   ## Prometheus format.  When using the prometheus input, use the same value in\n#   ## both plugins to ensure metrics are round-tripped without modification.\n#   ##\n#   ##   example: metric_version = 1;\n#   ##            metric_version = 2; recommended version\n#   # metric_version = 1\n#\n#   ## Use HTTP Basic Authentication.\n#   # basic_username = \"Foo\"\n#   # basic_password = \"Bar\"\n#\n#   ## If set, the IP Ranges which are allowed to access metrics.\n#   ##   ex: ip_range = [\"192.168.0.0/24\", \"192.168.1.0/30\"]\n#   # ip_range = []\n#\n#   ## Path to publish the metrics on.\n#   # path = \"/metrics\"\n#\n#   ## Expiration interval for each metric. 0 == no expiration\n#   # expiration_interval = \"60s\"\n#\n#   ## Collectors to enable, valid entries are \"gocollector\" and \"process\".\n#   ## If unset, both are enabled.\n#   # collectors_exclude = [\"gocollector\", \"process\"]\n#\n#   ## Send string metrics as Prometheus labels.\n#   ## Unless set to false all string metrics will be sent as labels.\n#   # string_as_label = true\n#\n#   ## If set, enable TLS with the given certificate.\n#   # tls_cert = \"/etc/ssl/telegraf.crt\"\n#   # tls_key = \"/etc/ssl/telegraf.key\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Export metric collection time.\n#   # export_timestamp = false\n\n\n# # Configuration for the Riemann server to send metrics to\n# [[outputs.riemann]]\n#   ## The full TCP or UDP URL of the Riemann server\n#   url = \"tcp://localhost:5555\"\n#\n#   ## Riemann event TTL, floating-point time in seconds.\n#   ## Defines how long that an event is considered valid for in Riemann\n#   # ttl = 30.0\n#\n#   ## Separator to use between measurement and field name in Riemann service name\n#   ## This does not have any effect if 'measurement_as_attribute' is set to 'true'\n#   separator = \"/\"\n#\n#   ## Set measurement name as Riemann attribute 'measurement', instead of prepending it to the Riemann service name\n#   # measurement_as_attribute = false\n#\n#   ## Send string metrics as Riemann event states.\n#   ## Unless enabled all string metrics will be ignored\n#   # string_as_state = false\n#\n#   ## A list of tag keys whose values get sent as Riemann tags.\n#   ## If empty, all Telegraf tag values will be sent as tags\n#   # tag_keys = [\"telegraf\",\"custom_tag\"]\n#\n#   ## Additional Riemann tags to send.\n#   # tags = [\"telegraf-output\"]\n#\n#   ## Description for Riemann event\n#   # description_text = \"metrics collected from telegraf\"\n#\n#   ## Riemann client write timeout, defaults to \"5s\" if not set.\n#   # timeout = \"5s\"\n\n\n# # Configuration for the Riemann server to send metrics to\n# [[outputs.riemann_legacy]]\n#   ## DEPRECATED: The 'riemann_legacy' plugin is deprecated in version 1.3.0, use 'outputs.riemann' instead (see https://github.com/influxdata/telegraf/issues/1878).\n#   ## URL of server\n#   url = \"localhost:5555\"\n#   ## transport protocol to use either tcp or udp\n#   transport = \"tcp\"\n#   ## separator to use between input name and field name in Riemann service name\n#   separator = \" \"\n\n\n# # Send aggregate metrics to Sensu Monitor\n# [[outputs.sensu]]\n#   ## BACKEND API URL is the Sensu Backend API root URL to send metrics to\n#   ## (protocol, host, and port only). The output plugin will automatically\n#   ## append the corresponding backend API path\n#   ## /api/core/v2/namespaces/:entity_namespace/events/:entity_name/:check_name).\n#   ##\n#   ## Backend Events API reference:\n#   ## https://docs.sensu.io/sensu-go/latest/api/events/\n#   ##\n#   ## AGENT API URL is the Sensu Agent API root URL to send metrics to\n#   ## (protocol, host, and port only). The output plugin will automatically\n#   ## append the correspeonding agent API path (/events).\n#   ##\n#   ## Agent API Events API reference:\n#   ## https://docs.sensu.io/sensu-go/latest/api/events/\n#   ##\n#   ## NOTE: if backend_api_url and agent_api_url and api_key are set, the output\n#   ## plugin will use backend_api_url. If backend_api_url and agent_api_url are\n#   ## not provided, the output plugin will default to use an agent_api_url of\n#   ## http://127.0.0.1:3031\n#   ##\n#   # backend_api_url = \"http://127.0.0.1:8080\"\n#   # agent_api_url = \"http://127.0.0.1:3031\"\n#\n#   ## API KEY is the Sensu Backend API token\n#   ## Generate a new API token via:\n#   ##\n#   ## $ sensuctl cluster-role create telegraf --verb create --resource events,entities\n#   ## $ sensuctl cluster-role-binding create telegraf --cluster-role telegraf --group telegraf\n#   ## $ sensuctl user create telegraf --group telegraf --password REDACTED\n#   ## $ sensuctl api-key grant telegraf\n#   ##\n#   ## For more information on Sensu RBAC profiles \u0026 API tokens, please visit:\n#   ## - https://docs.sensu.io/sensu-go/latest/reference/rbac/\n#   ## - https://docs.sensu.io/sensu-go/latest/reference/apikeys/\n#   ##\n#   # api_key = \"SENSU_API_KEY\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Timeout for HTTP message\n#   # timeout = \"5s\"\n#\n#   ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## Sensu Event details\n#   ##\n#   ## Below are the event details to be sent to Sensu.  The main portions of the\n#   ## event are the check, entity, and metrics specifications. For more information\n#   ## on Sensu events and its components, please visit:\n#   ## - Events - https://docs.sensu.io/sensu-go/latest/reference/events\n#   ## - Checks -  https://docs.sensu.io/sensu-go/latest/reference/checks\n#   ## - Entities - https://docs.sensu.io/sensu-go/latest/reference/entities\n#   ## - Metrics - https://docs.sensu.io/sensu-go/latest/reference/events#metrics\n#   ##\n#   ## Check specification\n#   ## The check name is the name to give the Sensu check associated with the event\n#   ## created. This maps to check.metatadata.name in the event.\n#   [outputs.sensu.check]\n#   name = \"telegraf\"\n#\n#   ## Entity specification\n#   ## Configure the entity name and namespace, if necessary. This will be part of\n#   ## the entity.metadata in the event.\n#   ##\n#   ## NOTE: if the output plugin is configured to send events to a\n#   ## backend_api_url and entity_name is not set, the value returned by\n#   ## os.Hostname() will be used; if the output plugin is configured to send\n#   ## events to an agent_api_url, entity_name and entity_namespace are not used.\n#   # [outputs.sensu.entity]\n#   #   name = \"server-01\"\n#   #   namespace = \"default\"\n#\n#   ## Metrics specification\n#   ## Configure the tags for the metrics that are sent as part of the Sensu event\n#   # [outputs.sensu.tags]\n#   #   source = \"telegraf\"\n#\n#   ## Configure the handler(s) for processing the provided metrics\n#   # [outputs.sensu.metrics]\n#   #   handlers = [\"influxdb\",\"elasticsearch\"]\n\n\n# # Send metrics and events to SignalFx\n# [[outputs.signalfx]]\n#     ## SignalFx Org Access Token\n#     access_token = \"my-secret-token\"\n#\n#     ## The SignalFx realm that your organization resides in\n#     signalfx_realm = \"us9\"  # Required if ingest_url is not set\n#\n#     ## You can optionally provide a custom ingest url instead of the\n#     ## signalfx_realm option above if you are using a gateway or proxy\n#     ## instance.  This option takes precident over signalfx_realm.\n#     ingest_url = \"https://my-custom-ingest/\"\n#\n#     ## Event typed metrics are omitted by default,\n#     ## If you require an event typed metric you must specify the\n#     ## metric name in the following list.\n#     included_event_names = [\"plugin.metric_name\"]\n\n\n# # Generic socket writer capable of handling multiple socket types.\n# [[outputs.socket_writer]]\n#   ## URL to connect to\n#   # address = \"tcp://127.0.0.1:8094\"\n#   # address = \"tcp://example.com:http\"\n#   # address = \"tcp4://127.0.0.1:8094\"\n#   # address = \"tcp6://127.0.0.1:8094\"\n#   # address = \"tcp6://[2001:db8::1]:8094\"\n#   # address = \"udp://127.0.0.1:8094\"\n#   # address = \"udp4://127.0.0.1:8094\"\n#   # address = \"udp6://127.0.0.1:8094\"\n#   # address = \"unix:///tmp/telegraf.sock\"\n#   # address = \"unixgram:///tmp/telegraf.sock\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Period between keep alive probes.\n#   ## Only applies to TCP sockets.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n#\n#   ## Content encoding for packet-based connections (i.e. UDP, unixgram).\n#   ## Can be set to \"gzip\" or to \"identity\" to apply no encoding.\n#   ##\n#   # content_encoding = \"identity\"\n#\n#   ## Data format to generate.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   # data_format = \"influx\"\n\n\n# # Send metrics to SQL Database\n# [[outputs.sql]]\n#   ## Database driver\n#   ## Valid options: mssql (Microsoft SQL Server), mysql (MySQL), pgx (Postgres),\n#   ##  sqlite (SQLite3), snowflake (snowflake.com) clickhouse (ClickHouse)\n#   # driver = \"\"\n#\n#   ## Data source name\n#   ## The format of the data source name is different for each database driver.\n#   ## See the plugin readme for details.\n#   # data_source_name = \"\"\n#\n#   ## Timestamp column name\n#   # timestamp_column = \"timestamp\"\n#\n#   ## Table creation template\n#   ## Available template variables:\n#   ##  {TABLE} - table name as a quoted identifier\n#   ##  {TABLELITERAL} - table name as a quoted string literal\n#   ##  {COLUMNS} - column definitions (list of quoted identifiers and types)\n#   # table_template = \"CREATE TABLE {TABLE}({COLUMNS})\"\n#\n#   ## Table existence check template\n#   ## Available template variables:\n#   ##  {TABLE} - tablename as a quoted identifier\n#   # table_exists_template = \"SELECT 1 FROM {TABLE} LIMIT 1\"\n#\n#   ## Initialization SQL\n#   # init_sql = \"\"\n#\n#   ## Metric type to SQL type conversion\n#   ## The values on the left are the data types Telegraf has and the values on\n#   ## the right are the data types Telegraf will use when sending to a database.\n#   ##\n#   ## The database values used must be data types the destination database\n#   ## understands. It is up to the user to ensure that the selected data type is\n#   ## available in the database they are using. Refer to your database\n#   ## documentation for what data types are available and supported.\n#   #[outputs.sql.convert]\n#   #  integer              = \"INT\"\n#   #  real                 = \"DOUBLE\"\n#   #  text                 = \"TEXT\"\n#   #  timestamp            = \"TIMESTAMP\"\n#   #  defaultvalue         = \"TEXT\"\n#   #  unsigned             = \"UNSIGNED\"\n#   #  bool                 = \"BOOL\"\n#\n#   ## This setting controls the behavior of the unsigned value. By default the\n#   ## setting will take the integer value and append the unsigned value to it. The other\n#   ## option is \"literal\", which will use the actual value the user provides to\n#   ## the unsigned option. This is useful for a database like ClickHouse where\n#   ## the unsigned value should use a value like \"uint64\".\n#   # conversion_style = \"unsigned_suffix\"\n\n\n# # Configuration for Google Cloud Stackdriver to send metrics to\n# [[outputs.stackdriver]]\n#   ## GCP Project\n#   project = \"erudite-bloom-151019\"\n#\n#   ## The namespace for the metric descriptor\n#   namespace = \"telegraf\"\n#\n#   ## Custom resource type\n#   # resource_type = \"generic_node\"\n#\n#   ## Additional resource labels\n#   # [outputs.stackdriver.resource_labels]\n#   #   node_id = \"$HOSTNAME\"\n#   #   namespace = \"myapp\"\n#   #   location = \"eu-north0\"\n\n\n# # A plugin that can transmit metrics to Sumo Logic HTTP Source\n# [[outputs.sumologic]]\n#   ## Unique URL generated for your HTTP Metrics Source.\n#   ## This is the address to send metrics to.\n#   # url = \"https://events.sumologic.net/receiver/v1/http/\u003cUniqueHTTPCollectorCode\u003e\"\n#\n#   ## Data format to be used for sending metrics.\n#   ## This will set the \"Content-Type\" header accordingly.\n#   ## Currently supported formats:\n#   ## * graphite - for Content-Type of application/vnd.sumologic.graphite\n#   ## * carbon2 - for Content-Type of application/vnd.sumologic.carbon2\n#   ## * prometheus - for Content-Type of application/vnd.sumologic.prometheus\n#   ##\n#   ## More information can be found at:\n#   ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#content-type-headers-for-metrics\n#   ##\n#   ## NOTE:\n#   ## When unset, telegraf will by default use the influx serializer which is currently unsupported\n#   ## in HTTP Source.\n#   data_format = \"carbon2\"\n#\n#   ## Timeout used for HTTP request\n#   # timeout = \"5s\"\n#\n#   ## Max HTTP request body size in bytes before compression (if applied).\n#   ## By default 1MB is recommended.\n#   ## NOTE:\n#   ## Bear in mind that in some serializer a metric even though serialized to multiple\n#   ## lines cannot be split any further so setting this very low might not work\n#   ## as expected.\n#   # max_request_body_size = 1000000\n#\n#   ## Additional, Sumo specific options.\n#   ## Full list can be found here:\n#   ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#supported-http-headers\n#\n#   ## Desired source name.\n#   ## Useful if you want to override the source name configured for the source.\n#   # source_name = \"\"\n#\n#   ## Desired host name.\n#   ## Useful if you want to override the source host configured for the source.\n#   # source_host = \"\"\n#\n#   ## Desired source category.\n#   ## Useful if you want to override the source category configured for the source.\n#   # source_category = \"\"\n#\n#   ## Comma-separated key=value list of dimensions to apply to every metric.\n#   ## Custom dimensions will allow you to query your metrics at a more granular level.\n#   # dimensions = \"\"\n\n\n# # Configuration for Syslog server to send metrics to\n# [[outputs.syslog]]\n#   ## URL to connect to\n#   ## ex: address = \"tcp://127.0.0.1:8094\"\n#   ## ex: address = \"tcp4://127.0.0.1:8094\"\n#   ## ex: address = \"tcp6://127.0.0.1:8094\"\n#   ## ex: address = \"tcp6://[2001:db8::1]:8094\"\n#   ## ex: address = \"udp://127.0.0.1:8094\"\n#   ## ex: address = \"udp4://127.0.0.1:8094\"\n#   ## ex: address = \"udp6://127.0.0.1:8094\"\n#   address = \"tcp://127.0.0.1:8094\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Period between keep alive probes.\n#   ## Only applies to TCP sockets.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n#\n#   ## The framing technique with which it is expected that messages are\n#   ## transported (default = \"octet-counting\").  Whether the messages come\n#   ## using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n#   ## or the non-transparent framing technique (RFC6587#section-3.4.2).  Must\n#   ## be one of \"octet-counting\", \"non-transparent\".\n#   # framing = \"octet-counting\"\n#\n#   ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n#   ## Must be one of \"LF\", or \"NUL\".\n#   # trailer = \"LF\"\n#\n#   ## SD-PARAMs settings\n#   ## Syslog messages can contain key/value pairs within zero or more\n#   ## structured data sections.  For each unrecognized metric tag/field a\n#   ## SD-PARAMS is created.\n#   ##\n#   ## Example:\n#   ##   [[outputs.syslog]]\n#   ##     sdparam_separator = \"_\"\n#   ##     default_sdid = \"default@32473\"\n#   ##     sdids = [\"foo@123\", \"bar@456\"]\n#   ##\n#   ##   input =\u003e xyzzy,x=y foo@123_value=42,bar@456_value2=84,something_else=1\n#   ##   output (structured data only) =\u003e [foo@123 value=42][bar@456 value2=84][default@32473 something_else=1 x=y]\n#\n#   ## SD-PARAMs separator between the sdid and tag/field key (default = \"_\")\n#   # sdparam_separator = \"_\"\n#\n#   ## Default sdid used for tags/fields that don't contain a prefix defined in\n#   ## the explicit sdids setting below If no default is specified, no SD-PARAMs\n#   ## will be used for unrecognized field.\n#   # default_sdid = \"default@32473\"\n#\n#   ## List of explicit prefixes to extract from tag/field keys and use as the\n#   ## SDID, if they match (see above example for more details):\n#   # sdids = [\"foo@123\", \"bar@456\"]\n#\n#   ## Default severity value. Severity and Facility are used to calculate the\n#   ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field\n#   ## with key \"severity_code\" is defined.  If unset, 5 (notice) is the default\n#   # default_severity_code = 5\n#\n#   ## Default facility value. Facility and Severity are used to calculate the\n#   ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field with\n#   ## key \"facility_code\" is defined.  If unset, 1 (user-level) is the default\n#   # default_facility_code = 1\n#\n#   ## Default APP-NAME value (RFC5424#section-6.2.5)\n#   ## Used when no metric tag with key \"appname\" is defined.\n#   ## If unset, \"Telegraf\" is the default\n#   # default_appname = \"Telegraf\"\n\n\n# # Configuration for Amazon Timestream output.\n# [[outputs.timestream]]\n#   ## Amazon Region\n#   region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order:\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Timestream database where the metrics will be inserted.\n#   ## The database must exist prior to starting Telegraf.\n#   database_name = \"yourDatabaseNameHere\"\n#\n#   ## Specifies if the plugin should describe the Timestream database upon starting\n#   ## to validate if it has access necessary permissions, connection, etc., as a safety check.\n#   ## If the describe operation fails, the plugin will not start\n#   ## and therefore the Telegraf agent will not start.\n#   describe_database_on_start = false\n#\n#   ## The mapping mode specifies how Telegraf records are represented in Timestream.\n#   ## Valid values are: single-table, multi-table.\n#   ## For example, consider the following data in line protocol format:\n#   ## weather,location=us-midwest,season=summer temperature=82,humidity=71 1465839830100400200\n#   ## airquality,location=us-west no2=5,pm25=16 1465839830100400200\n#   ## where weather and airquality are the measurement names, location and season are tags,\n#   ## and temperature, humidity, no2, pm25 are fields.\n#   ## In multi-table mode:\n#   ##  - first line will be ingested to table named weather\n#   ##  - second line will be ingested to table named airquality\n#   ##  - the tags will be represented as dimensions\n#   ##  - first table (weather) will have two records:\n#   ##      one with measurement name equals to temperature,\n#   ##      another with measurement name equals to humidity\n#   ##  - second table (airquality) will have two records:\n#   ##      one with measurement name equals to no2,\n#   ##      another with measurement name equals to pm25\n#   ##  - the Timestream tables from the example will look like this:\n#   ##      TABLE \"weather\":\n#   ##        time | location | season | measure_name | measure_value::bigint\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | temperature | 82\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | humidity | 71\n#   ##      TABLE \"airquality\":\n#   ##        time | location | measure_name | measure_value::bigint\n#   ##        2016-06-13 17:43:50 | us-west | no2 | 5\n#   ##        2016-06-13 17:43:50 | us-west | pm25 | 16\n#   ## In single-table mode:\n#   ##  - the data will be ingested to a single table, which name will be valueOf(single_table_name)\n#   ##  - measurement name will stored in dimension named valueOf(single_table_dimension_name_for_telegraf_measurement_name)\n#   ##  - location and season will be represented as dimensions\n#   ##  - temperature, humidity, no2, pm25 will be represented as measurement name\n#   ##  - the Timestream table from the example will look like this:\n#   ##      Assuming:\n#   ##        - single_table_name = \"my_readings\"\n#   ##        - single_table_dimension_name_for_telegraf_measurement_name = \"namespace\"\n#   ##      TABLE \"my_readings\":\n#   ##        time | location | season | namespace | measure_name | measure_value::bigint\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | weather | temperature | 82\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | weather | humidity | 71\n#   ##        2016-06-13 17:43:50 | us-west | NULL | airquality | no2 | 5\n#   ##        2016-06-13 17:43:50 | us-west | NULL | airquality | pm25 | 16\n#   ## In most cases, using multi-table mapping mode is recommended.\n#   ## However, you can consider using single-table in situations when you have thousands of measurement names.\n#   mapping_mode = \"multi-table\"\n#\n#   ## Only valid and required for mapping_mode = \"single-table\"\n#   ## Specifies the Timestream table where the metrics will be uploaded.\n#   # single_table_name = \"yourTableNameHere\"\n#\n#   ## Only valid and required for mapping_mode = \"single-table\"\n#   ## Describes what will be the Timestream dimension name for the Telegraf\n#   ## measurement name.\n#   # single_table_dimension_name_for_telegraf_measurement_name = \"namespace\"\n#\n#   ## Specifies if the plugin should create the table, if the table do not exist.\n#   ## The plugin writes the data without prior checking if the table exists.\n#   ## When the table does not exist, the error returned from Timestream will cause\n#   ## the plugin to create the table, if this parameter is set to true.\n#   create_table_if_not_exists = true\n#\n#   ## Only valid and required if create_table_if_not_exists = true\n#   ## Specifies the Timestream table magnetic store retention period in days.\n#   ## Check Timestream documentation for more details.\n#   create_table_magnetic_store_retention_period_in_days = 365\n#\n#   ## Only valid and required if create_table_if_not_exists = true\n#   ## Specifies the Timestream table memory store retention period in hours.\n#   ## Check Timestream documentation for more details.\n#   create_table_memory_store_retention_period_in_hours = 24\n#\n#   ## Only valid and optional if create_table_if_not_exists = true\n#   ## Specifies the Timestream table tags.\n#   ## Check Timestream documentation for more details\n#   # create_table_tags = { \"foo\" = \"bar\", \"environment\" = \"dev\"}\n#\n#   ## Specify the maximum number of parallel go routines to ingest/write data\n#   ## If not specified, defaulted to 1 go routines\n#   max_write_go_routines = 25\n\n\n# # Write metrics to Warp 10\n# [[outputs.warp10]]\n#   # Prefix to add to the measurement.\n#   prefix = \"telegraf.\"\n#\n#   # URL of the Warp 10 server\n#   warp_url = \"http://localhost:8080\"\n#\n#   # Write token to access your app on warp 10\n#   token = \"Token\"\n#\n#   # Warp 10 query timeout\n#   # timeout = \"15s\"\n#\n#   ## Print Warp 10 error body\n#   # print_error_body = false\n#\n#   ## Max string error size\n#   # max_string_error_size = 511\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Configuration for Wavefront server to send metrics to\n# [[outputs.wavefront]]\n#   ## Url for Wavefront Direct Ingestion. For Wavefront Proxy Ingestion, see\n#   ## the 'host' and 'port' optioins below.\n#   url = \"https://metrics.wavefront.com\"\n#\n#   ## Authentication Token for Wavefront. Only required if using Direct Ingestion\n#   #token = \"DUMMY_TOKEN\"\n#\n#   ## DNS name of the wavefront proxy server. Do not use if url is specified\n#   #host = \"wavefront.example.com\"\n#\n#   ## Port that the Wavefront proxy server listens on. Do not use if url is specified\n#   #port = 2878\n#\n#   ## prefix for metrics keys\n#   #prefix = \"my.specific.prefix.\"\n#\n#   ## whether to use \"value\" for name of simple fields. default is false\n#   #simple_fields = false\n#\n#   ## character to use between metric and field name.  default is . (dot)\n#   #metric_separator = \".\"\n#\n#   ## Convert metric name paths to use metricSeparator character\n#   ## When true will convert all _ (underscore) characters in final metric name. default is true\n#   #convert_paths = true\n#\n#   ## Use Strict rules to sanitize metric and tag names from invalid characters\n#   ## When enabled forward slash (/) and comma (,) will be accepted\n#   #use_strict = false\n#\n#   ## Use Regex to sanitize metric and tag names from invalid characters\n#   ## Regex is more thorough, but significantly slower. default is false\n#   #use_regex = false\n#\n#   ## point tags to use as the source name for Wavefront (if none found, host will be used)\n#   #source_override = [\"hostname\", \"address\", \"agent_host\", \"node_host\"]\n#\n#   ## whether to convert boolean values to numeric values, with false -\u003e 0.0 and true -\u003e 1.0. default is true\n#   #convert_bool = true\n#\n#   ## Truncate metric tags to a total of 254 characters for the tag name value. Wavefront will reject any\n#   ## data point exceeding this limit if not truncated. Defaults to 'false' to provide backwards compatibility.\n#   #truncate_tags = false\n#\n#   ## Flush the internal buffers after each batch. This effectively bypasses the background sending of metrics\n#   ## normally done by the Wavefront SDK. This can be used if you are experiencing buffer overruns. The sending\n#   ## of metrics will block for a longer time, but this will be handled gracefully by the internal buffering in\n#   ## Telegraf.\n#   #immediate_flush = true\n\n\n# # Generic WebSocket output writer.\n# [[outputs.websocket]]\n#   ## URL is the address to send metrics to. Make sure ws or wss scheme is used.\n#   url = \"ws://127.0.0.1:8080/telegraf\"\n#\n#   ## Timeouts (make sure read_timeout is larger than server ping interval or set to zero).\n#   # connect_timeout = \"30s\"\n#   # write_timeout = \"30s\"\n#   # read_timeout = \"30s\"\n#\n#   ## Optionally turn on using text data frames (binary by default).\n#   # use_text_frames = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional SOCKS5 proxy to use\n#   # socks5_enabled = true\n#   # socks5_address = \"127.0.0.1:1080\"\n#   # socks5_username = \"alice\"\n#   # socks5_password = \"pass123\"\n#\n#   ## Data format to output.\n#   ## Each data format has it's own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n#\n#   ## Additional HTTP Upgrade headers\n#   # [outputs.websocket.headers]\n#   #   Authorization = \"Bearer \u003cTOKEN\u003e\"\n\n\n# # Send aggregated metrics to Yandex.Cloud Monitoring\n# [[outputs.yandex_cloud_monitoring]]\n#   ## Timeout for HTTP writes.\n#   # timeout = \"20s\"\n#\n#   ## Yandex.Cloud monitoring API endpoint. Normally should not be changed\n#   # endpoint_url = \"https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write\"\n#\n#   ## All user metrics should be sent with \"custom\" service specified. Normally should not be changed\n#   # service = \"custom\"\n\n\n###############################################################################\n#                            PROCESSOR PLUGINS                                #\n###############################################################################\n\n\n# # Attach AWS EC2 metadata to metrics\n# [[processors.aws_ec2]]\n#   ## Instance identity document tags to attach to metrics.\n#   ## For more information see:\n#   ## https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html\n#   ##\n#   ## Available tags:\n#   ## * accountId\n#   ## * architecture\n#   ## * availabilityZone\n#   ## * billingProducts\n#   ## * imageId\n#   ## * instanceId\n#   ## * instanceType\n#   ## * kernelId\n#   ## * pendingTime\n#   ## * privateIp\n#   ## * ramdiskId\n#   ## * region\n#   ## * version\n#   imds_tags = []\n#\n#   ## EC2 instance tags retrieved with DescribeTags action.\n#   ## In case tag is empty upon retrieval it's omitted when tagging metrics.\n#   ## Note that in order for this to work, role attached to EC2 instance or AWS\n#   ## credentials available from the environment must have a policy attached, that\n#   ## allows ec2:DescribeTags.\n#   ##\n#   ## For more information see:\n#   ## https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeTags.html\n#   ec2_tags = []\n#\n#   ## Timeout for http requests made by against aws ec2 metadata endpoint.\n#   timeout = \"10s\"\n#\n#   ## ordered controls whether or not the metrics need to stay in the same order\n#   ## this plugin received them in. If false, this plugin will change the order\n#   ## with requests hitting cached results moving through immediately and not\n#   ## waiting on slower lookups. This may cause issues for you if you are\n#   ## depending on the order of metrics staying the same. If so, set this to true.\n#   ## Keeping the metrics ordered may be slightly slower.\n#   ordered = false\n#\n#   ## max_parallel_calls is the maximum number of AWS API calls to be in flight\n#   ## at the same time.\n#   ## It's probably best to keep this number fairly low.\n#   max_parallel_calls = 10\n\n\n# # Clone metrics and apply modifications.\n# [[processors.clone]]\n#   ## All modifications on inputs and aggregators can be overridden:\n#   # name_override = \"new_name\"\n#   # name_prefix = \"new_name_prefix\"\n#   # name_suffix = \"new_name_suffix\"\n#\n#   ## Tags to be added (all values must be strings)\n#   # [processors.clone.tags]\n#   #   additional_tag = \"tag_value\"\n\n\n# # Convert values to another metric value type\n# [[processors.converter]]\n#   ## Tags to convert\n#   ##\n#   ## The table key determines the target type, and the array of key-values\n#   ## select the keys to convert.  The array may contain globs.\n#   ##   \u003ctarget-type\u003e = [\u003ctag-key\u003e...]\n#   [processors.converter.tags]\n#     measurement = []\n#     string = []\n#     integer = []\n#     unsigned = []\n#     boolean = []\n#     float = []\n#\n#   ## Fields to convert\n#   ##\n#   ## The table key determines the target type, and the array of key-values\n#   ## select the keys to convert.  The array may contain globs.\n#   ##   \u003ctarget-type\u003e = [\u003cfield-key\u003e...]\n#   [processors.converter.fields]\n#     measurement = []\n#     tag = []\n#     string = []\n#     integer = []\n#     unsigned = []\n#     boolean = []\n#     float = []\n\n\n# # Dates measurements, tags, and fields that pass through this filter.\n# [[processors.date]]\n# \t## New tag to create\n# \ttag_key = \"month\"\n#\n# \t## New field to create (cannot set both field_key and tag_key)\n# \t# field_key = \"month\"\n#\n# \t## Date format string, must be a representation of the Go \"reference time\"\n# \t## which is \"Mon Jan 2 15:04:05 -0700 MST 2006\".\n# \tdate_format = \"Jan\"\n#\n# \t## If destination is a field, date format can also be one of\n# \t## \"unix\", \"unix_ms\", \"unix_us\", or \"unix_ns\", which will insert an integer field.\n# \t# date_format = \"unix\"\n#\n# \t## Offset duration added to the date string when writing the new tag.\n# \t# date_offset = \"0s\"\n#\n# \t## Timezone to use when creating the tag or field using a reference time\n# \t## string.  This can be set to one of \"UTC\", \"Local\", or to a location name\n# \t## in the IANA Time Zone database.\n# \t##   example: timezone = \"America/Los_Angeles\"\n# \t# timezone = \"UTC\"\n\n\n# # Filter metrics with repeating field values\n# [[processors.dedup]]\n#   ## Maximum time to suppress output\n#   dedup_interval = \"600s\"\n\n\n# # Defaults sets default value(s) for specified fields that are not set on incoming metrics.\n# [[processors.defaults]]\n#   ## Ensures a set of fields always exists on your metric(s) with their\n#   ## respective default value.\n#   ## For any given field pair (key = default), if it's not set, a field\n#   ## is set on the metric with the specified default.\n#   ##\n#   ## A field is considered not set if it is nil on the incoming metric;\n#   ## or it is not nil but its value is an empty string or is a string\n#   ## of one or more spaces.\n#   ##   \u003ctarget-field\u003e = \u003cvalue\u003e\n#   # [processors.defaults.fields]\n#   #   field_1 = \"bar\"\n#   #   time_idle = 0\n#   #   is_error = true\n\n\n# # Map enum values according to given table.\n# [[processors.enum]]\n#   [[processors.enum.mapping]]\n#     ## Name of the field to map. Globs accepted.\n#     field = \"status\"\n#\n#     ## Name of the tag to map. Globs accepted.\n#     # tag = \"status\"\n#\n#     ## Destination tag or field to be used for the mapped value.  By default the\n#     ## source tag or field is used, overwriting the original value.\n#     dest = \"status_code\"\n#\n#     ## Default value to be used for all values not contained in the mapping\n#     ## table.  When unset, the unmodified value for the field will be used if no\n#     ## match is found.\n#     # default = 0\n#\n#     ## Table of mappings\n#     [processors.enum.mapping.value_mappings]\n#       green = 1\n#       amber = 2\n#       red = 3\n\n\n# # Run executable as long-running processor plugin\n# [[processors.execd]]\n# \t## Program to run as daemon\n# \t## eg: command = [\"/path/to/your_program\", \"arg1\", \"arg2\"]\n# \tcommand = [\"cat\"]\n#\n#   ## Delay before the process is restarted after an unexpected termination\n#   restart_delay = \"10s\"\n\n\n# # Performs file path manipulations on tags and fields\n# [[processors.filepath]]\n#   ## Treat the tag value as a path and convert it to its last element, storing the result in a new tag\n#   # [[processors.filepath.basename]]\n#   #   tag = \"path\"\n#   #   dest = \"basepath\"\n#\n#   ## Treat the field value as a path and keep all but the last element of path, typically the path's directory\n#   # [[processors.filepath.dirname]]\n#   #   field = \"path\"\n#\n#   ## Treat the tag value as a path, converting it to its the last element without its suffix\n#   # [[processors.filepath.stem]]\n#   #   tag = \"path\"\n#\n#   ## Treat the tag value as a path, converting it to the shortest path name equivalent\n#   ## to path by purely lexical processing\n#   # [[processors.filepath.clean]]\n#   #   tag = \"path\"\n#\n#   ## Treat the tag value as a path, converting it to a relative path that is lexically\n#   ## equivalent to the source path when joined to 'base_path'\n#   # [[processors.filepath.rel]]\n#   #   tag = \"path\"\n#   #   base_path = \"/var/log\"\n#\n#   ## Treat the tag value as a path, replacing each separator character in path with a '/' character. Has only\n#   ## effect on Windows\n#   # [[processors.filepath.toslash]]\n#   #   tag = \"path\"\n\n\n# # Add a tag of the network interface name looked up over SNMP by interface number\n# [[processors.ifname]]\n#   ## Name of tag holding the interface number\n#   # tag = \"ifIndex\"\n#\n#   ## Name of output tag where service name will be added\n#   # dest = \"ifName\"\n#\n#   ## Name of tag of the SNMP agent to request the interface name from\n#   # agent = \"agent\"\n#\n#   ## Timeout for each request.\n#   # timeout = \"5s\"\n#\n#   ## SNMP version; can be 1, 2, or 3.\n#   # version = 2\n#\n#   ## SNMP community string.\n#   # community = \"public\"\n#\n#   ## Number of retries to attempt.\n#   # retries = 3\n#\n#   ## The GETBULK max-repetitions parameter.\n#   # max_repetitions = 10\n#\n#   ## SNMPv3 authentication and encryption options.\n#   ##\n#   ## Security Name.\n#   # sec_name = \"myuser\"\n#   ## Authentication protocol; one of \"MD5\", \"SHA\", or \"\".\n#   # auth_protocol = \"MD5\"\n#   ## Authentication password.\n#   # auth_password = \"pass\"\n#   ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n#   # sec_level = \"authNoPriv\"\n#   ## Context Name.\n#   # context_name = \"\"\n#   ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\" or \"\".\n#   # priv_protocol = \"\"\n#   ## Privacy password used for encrypted messages.\n#   # priv_password = \"\"\n#\n#   ## max_parallel_lookups is the maximum number of SNMP requests to\n#   ## make at the same time.\n#   # max_parallel_lookups = 100\n#\n#   ## ordered controls whether or not the metrics need to stay in the\n#   ## same order this plugin received them in. If false, this plugin\n#   ## may change the order when data is cached.  If you need metrics to\n#   ## stay in order set this to true.  keeping the metrics ordered may\n#   ## be slightly slower\n#   # ordered = false\n#\n#   ## cache_ttl is the amount of time interface names are cached for a\n#   ## given agent.  After this period elapses if names are needed they\n#   ## will be retrieved again.\n#   # cache_ttl = \"8h\"\n\n\n# # Adds noise to numerical fields\n# [[processors.noise]]\n#     ## Specified the type of the random distribution.\n#     ## Can be \"laplacian\", \"gaussian\" or \"uniform\".\n#     # type = \"laplacian\n#\n#     ## Center of the distribution.\n#     ## Only used for Laplacian and Gaussian distributions.\n#     # mu = 0.0\n#\n#     ## Scale parameter for the Laplacian or Gaussian distribution\n#     # scale = 1.0\n#\n#     ## Upper and lower bound of the Uniform distribution\n#     # min = -1.0\n#     # max = 1.0\n#\n#     ## Apply the noise only to numeric fields matching the filter criteria below.\n#     ## Excludes takes precedence over includes.\n#     # include_fields = []\n#     # exclude_fields = []\n\n\n# # Apply metric modifications using override semantics.\n# [[processors.override]]\n#   ## All modifications on inputs and aggregators can be overridden:\n#   # name_override = \"new_name\"\n#   # name_prefix = \"new_name_prefix\"\n#   # name_suffix = \"new_name_suffix\"\n#\n#   ## Tags to be added (all values must be strings)\n#   # [processors.override.tags]\n#   #   additional_tag = \"tag_value\"\n\n\n# # Parse a value in a specified field/tag(s) and add the result in a new metric\n# [[processors.parser]]\n#   ## The name of the fields whose value will be parsed.\n#   parse_fields = []\n#\n#   ## If true, incoming metrics are not emitted.\n#   drop_original = false\n#\n#   ## If set to override, emitted metrics will be merged by overriding the\n#   ## original metric using the newly parsed metrics.\n#   merge = \"override\"\n#\n#   ## The dataformat to be read from files\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Rotate a single valued metric into a multi field metric\n# [[processors.pivot]]\n#   ## Tag to use for naming the new field.\n#   tag_key = \"name\"\n#   ## Field to use as the value of the new field.\n#   value_key = \"value\"\n\n\n# # Given a tag/field of a TCP or UDP port number, add a tag/field of the service name looked up in the system services file\n# [[processors.port_name]]\n# [[processors.port_name]]\n#   ## Name of tag holding the port number\n#   # tag = \"port\"\n#   ## Or name of the field holding the port number\n#   # field = \"port\"\n#\n#   ## Name of output tag or field (depending on the source) where service name will be added\n#   # dest = \"service\"\n#\n#   ## Default tcp or udp\n#   # default_protocol = \"tcp\"\n#\n#   ## Tag containing the protocol (tcp or udp, case-insensitive)\n#   # protocol_tag = \"proto\"\n#\n#   ## Field containing the protocol (tcp or udp, case-insensitive)\n#   # protocol_field = \"proto\"\n\n\n# # Print all metrics that pass through this filter.\n# [[processors.printer]]\n\n\n# # Transforms tag and field values as well as measurement, tag and field names with regex pattern\n# [[processors.regex]]\n#   ## Tag and field conversions defined in a separate sub-tables\n#   # [[processors.regex.tags]]\n#   #   ## Tag to change\n#   #   key = \"resp_code\"\n#   #   ## Regular expression to match on a tag value\n#   #   pattern = \"^(\\\\d)\\\\d\\\\d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use 1\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"1xx\"\n#\n#   # [[processors.regex.fields]]\n#   #   ## Field to change\n#   #   key = \"request\"\n#   #   ## All the power of the Go regular expressions available here\n#   #   ## For example, named subgroups\n#   #   pattern = \"^/api(?P\u003cmethod\u003e/[\\\\w/]+)\\\\S*\"\n#   #   replacement = \"method\"\n#   #   ## If result_key is present, a new field will be created\n#   #   ## instead of changing existing field\n#   #   result_key = \"method\"\n#\n#   ## Multiple conversions may be applied for one field sequentially\n#   ## Let's extract one more value\n#   # [[processors.regex.fields]]\n#   #   key = \"request\"\n#   #   pattern = \".*category=(\\\\w+).*\"\n#   #   replacement = \"1\"\n#   #   result_key = \"search_category\"\n#\n#   ## Rename metric fields\n#   # [[processors.regex.field_rename]]\n#   #   ## Regular expression to match on a field name\n#   #   pattern = \"^search_(\\\\w+)d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use 1\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"1\"\n#   #   ## If the new field name already exists, you can either \"overwrite\" the\n#   #   ## existing one with the value of the renamed field OR you can \"keep\"\n#   #   ## both the existing and source field.\n#   #   # result_key = \"keep\"\n#\n#   ## Rename metric tags\n#   # [[processors.regex.tag_rename]]\n#   #   ## Regular expression to match on a tag name\n#   #   pattern = \"^search_(\\\\w+)d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use 1\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"1\"\n#   #   ## If the new tag name already exists, you can either \"overwrite\" the\n#   #   ## existing one with the value of the renamed tag OR you can \"keep\"\n#   #   ## both the existing and source tag.\n#   #   # result_key = \"keep\"\n#\n#   ## Rename metrics\n#   # [[processors.regex.metric_rename]]\n#   #   ## Regular expression to match on an metric name\n#   #   pattern = \"^search_(\\\\w+)d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use 1\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"1\"\n\n\n# # Rename measurements, tags, and fields that pass through this filter.\n# [[processors.rename]]\n\n\n# # ReverseDNS does a reverse lookup on IP addresses to retrieve the DNS name\n# [[processors.reverse_dns]]\n#   ## For optimal performance, you may want to limit which metrics are passed to this\n#   ## processor. eg:\n#   ## namepass = [\"my_metric_*\"]\n#\n#   ## cache_ttl is how long the dns entries should stay cached for.\n#   ## generally longer is better, but if you expect a large number of diverse lookups\n#   ## you'll want to consider memory use.\n#   cache_ttl = \"24h\"\n#\n#   ## lookup_timeout is how long should you wait for a single dns request to repsond.\n#   ## this is also the maximum acceptable latency for a metric travelling through\n#   ## the reverse_dns processor. After lookup_timeout is exceeded, a metric will\n#   ## be passed on unaltered.\n#   ## multiple simultaneous resolution requests for the same IP will only make a\n#   ## single rDNS request, and they will all wait for the answer for this long.\n#   lookup_timeout = \"3s\"\n#\n#   ## max_parallel_lookups is the maximum number of dns requests to be in flight\n#   ## at the same time. Requesting hitting cached values do not count against this\n#   ## total, and neither do mulptiple requests for the same IP.\n#   ## It's probably best to keep this number fairly low.\n#   max_parallel_lookups = 10\n#\n#   ## ordered controls whether or not the metrics need to stay in the same order\n#   ## this plugin received them in. If false, this plugin will change the order\n#   ## with requests hitting cached results moving through immediately and not\n#   ## waiting on slower lookups. This may cause issues for you if you are\n#   ## depending on the order of metrics staying the same. If so, set this to true.\n#   ## keeping the metrics ordered may be slightly slower.\n#   ordered = false\n#\n#   [[processors.reverse_dns.lookup]]\n#     ## get the ip from the field \"source_ip\", and put the result in the field \"source_name\"\n#     field = \"source_ip\"\n#     dest = \"source_name\"\n#\n#   [[processors.reverse_dns.lookup]]\n#     ## get the ip from the tag \"destination_ip\", and put the result in the tag\n#     ## \"destination_name\".\n#     tag = \"destination_ip\"\n#     dest = \"destination_name\"\n#\n#     ## If you would prefer destination_name to be a field instead, you can use a\n#     ## processors.converter after this one, specifying the order attribute.\n\n\n# # Add the S2 Cell ID as a tag based on latitude and longitude fields\n# [[processors.s2geo]]\n#   ## The name of the lat and lon fields containing WGS-84 latitude and\n#   ## longitude in decimal degrees.\n#   # lat_field = \"lat\"\n#   # lon_field = \"lon\"\n#\n#   ## New tag to create\n#   # tag_key = \"s2_cell_id\"\n#\n#   ## Cell level (see https://s2geometry.io/resources/s2cell_statistics.html)\n#   # cell_level = 9\n\n\n# # Process metrics using a Starlark script\n# [[processors.starlark]]\n#   ## The Starlark source can be set as a string in this configuration file, or\n#   ## by referencing a file containing the script.  Only one source or script\n#   ## should be set at once.\n#   ##\n#   ## Source of the Starlark script.\n#   source = '''\n# def apply(metric):\n# \treturn metric\n# '''\n#\n#   ## File containing a Starlark script.\n#   # script = \"/usr/local/bin/myscript.star\"\n#\n#   ## The constants of the Starlark script.\n#   # [processors.starlark.constants]\n#   #   max_size = 10\n#   #   threshold = 0.75\n#   #   default_name = \"Julia\"\n#   #   debug_mode = true\n\n\n# # Perform string processing on tags, fields, and measurements\n# [[processors.strings]]\n#   ## Convert a tag value to uppercase\n#   # [[processors.strings.uppercase]]\n#   #   tag = \"method\"\n#\n#   ## Convert a field value to lowercase and store in a new field\n#   # [[processors.strings.lowercase]]\n#   #   field = \"uri_stem\"\n#   #   dest = \"uri_stem_normalised\"\n#\n#   ## Convert a field value to titlecase\n#   # [[processors.strings.titlecase]]\n#   #   field = \"status\"\n#\n#   ## Trim leading and trailing whitespace using the default cutset\n#   # [[processors.strings.trim]]\n#   #   field = \"message\"\n#\n#   ## Trim leading characters in cutset\n#   # [[processors.strings.trim_left]]\n#   #   field = \"message\"\n#   #   cutset = \"\\t\"\n#\n#   ## Trim trailing characters in cutset\n#   # [[processors.strings.trim_right]]\n#   #   field = \"message\"\n#   #   cutset = \"\\r\\n\"\n#\n#   ## Trim the given prefix from the field\n#   # [[processors.strings.trim_prefix]]\n#   #   field = \"my_value\"\n#   #   prefix = \"my_\"\n#\n#   ## Trim the given suffix from the field\n#   # [[processors.strings.trim_suffix]]\n#   #   field = \"read_count\"\n#   #   suffix = \"_count\"\n#\n#   ## Replace all non-overlapping instances of old with new\n#   # [[processors.strings.replace]]\n#   #   measurement = \"*\"\n#   #   old = \":\"\n#   #   new = \"_\"\n#\n#   ## Trims strings based on width\n#   # [[processors.strings.left]]\n#   #   field = \"message\"\n#   #   width = 10\n#\n#   ## Decode a base64 encoded utf-8 string\n#   # [[processors.strings.base64decode]]\n#   #   field = \"message\"\n#\n#   ## Sanitize a string to ensure it is a valid utf-8 string\n#   ## Each run of invalid UTF-8 byte sequences is replaced by the replacement string, which may be empty\n#   # [[processors.strings.valid_utf8]]\n#   #   field = \"message\"\n#   #   replacement = \"\"\n\n\n# # Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.\n# [[processors.tag_limit]]\n#   ## Maximum number of tags to preserve\n#   limit = 10\n#\n#   ## List of tags to preferentially preserve\n#   keep = [\"foo\", \"bar\", \"baz\"]\n\n\n# # Uses a Go template to create a new tag\n# [[processors.template]]\n#   ## Tag to set with the output of the template.\n#   tag = \"topic\"\n#\n#   ## Go template used to create the tag value.  In order to ease TOML\n#   ## escaping requirements, you may wish to use single quotes around the\n#   ## template string.\n#   template = '{{ .Tag \"hostname\" }}.{{ .Tag \"level\" }}'\n\n\n# # Print all metrics that pass through this filter.\n# [[processors.topk]]\n#   ## How many seconds between aggregations\n#   # period = 10\n#\n#   ## How many top metrics to return\n#   # k = 10\n#\n#   ## Over which tags should the aggregation be done. Globs can be specified, in\n#   ## which case any tag matching the glob will aggregated over. If set to an\n#   ## empty list is no aggregation over tags is done\n#   # group_by = ['*']\n#\n#   ## Over which fields are the top k are calculated\n#   # fields = [\"value\"]\n#\n#   ## What aggregation to use. Options: sum, mean, min, max\n#   # aggregation = \"mean\"\n#\n#   ## Instead of the top k largest metrics, return the bottom k lowest metrics\n#   # bottomk = false\n#\n#   ## The plugin assigns each metric a GroupBy tag generated from its name and\n#   ## tags. If this setting is different than \"\" the plugin will add a\n#   ## tag (which name will be the value of this setting) to each metric with\n#   ## the value of the calculated GroupBy tag. Useful for debugging\n#   # add_groupby_tag = \"\"\n#\n#   ## These settings provide a way to know the position of each metric in\n#   ## the top k. The 'add_rank_field' setting allows to specify for which\n#   ## fields the position is required. If the list is non empty, then a field\n#   ## will be added to each and every metric for each string present in this\n#   ## setting. This field will contain the ranking of the group that\n#   ## the metric belonged to when aggregated over that field.\n#   ## The name of the field will be set to the name of the aggregation field,\n#   ## suffixed with the string '_topk_rank'\n#   # add_rank_fields = []\n#\n#   ## These settings provide a way to know what values the plugin is generating\n#   ## when aggregating metrics. The 'add_aggregate_field' setting allows to\n#   ## specify for which fields the final aggregation value is required. If the\n#   ## list is non empty, then a field will be added to each every metric for\n#   ## each field present in this setting. This field will contain\n#   ## the computed aggregation for the group that the metric belonged to when\n#   ## aggregated over that field.\n#   ## The name of the field will be set to the name of the aggregation field,\n#   ## suffixed with the string '_topk_aggregate'\n#   # add_aggregate_fields = []\n\n\n# # Rotate multi field metric into several single field metrics\n# [[processors.unpivot]]\n#   ## Tag to use for the name.\n#   tag_key = \"name\"\n#   ## Field to use for the name of the value.\n#   value_key = \"value\"\n\n\n###############################################################################\n#                            AGGREGATOR PLUGINS                               #\n###############################################################################\n\n\n# # Keep the aggregate basicstats of each metric passing through.\n# [[aggregators.basicstats]]\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## Configures which basic stats to push as fields\n#   # stats = [\"count\", \"min\", \"max\", \"mean\", \"stdev\", \"s2\", \"sum\"]\n\n\n# # Calculates a derivative for every field.\n# [[aggregators.derivative]]\n# \t## The period in which to flush the aggregator.\n# \tperiod = \"30s\"\n# \t##\n# \t## If true, the original metric will be dropped by the\n# \t## aggregator and will not get sent to the output plugins.\n# \tdrop_original = false\n# \t##\n# \t## This aggregator will estimate a derivative for each field, which is\n# \t## contained in both the first and last metric of the aggregation interval.\n# \t## Without further configuration the derivative will be calculated with\n# \t## respect to the time difference between these two measurements in seconds.\n# \t## The formula applied is for every field:\n# \t##\n# \t##               value_last - value_first\n# \t## derivative = --------------------------\n# \t##              time_difference_in_seconds\n# \t##\n# \t## The resulting derivative will be named *fieldname_rate*. The suffix\n# \t## \"_rate\" can be configured by the *suffix* parameter. When using a\n# \t## derivation variable you can include its name for more clarity.\n# \t# suffix = \"_rate\"\n# \t##\n# \t## As an abstraction the derivative can be calculated not only by the time\n# \t## difference but by the difference of a field, which is contained in the\n# \t## measurement. This field is assumed to be monotonously increasing. This\n# \t## feature is used by specifying a *variable*.\n# \t## Make sure the specified variable is not filtered and exists in the metrics\n# \t## passed to this aggregator!\n# \t# variable = \"\"\n# \t##\n# \t## When using a field as the derivation parameter the name of that field will\n# \t## be used for the resulting derivative, e.g. *fieldname_by_parameter*.\n# \t##\n# \t## Note, that the calculation is based on the actual timestamp of the\n# \t## measurements. When there is only one measurement during that period, the\n# \t## measurement will be rolled over to the next period. The maximum number of\n# \t## such roll-overs can be configured with a default of 10.\n# \t# max_roll_over = 10\n# \t##\n\n\n# # Report the final metric of a series\n# [[aggregators.final]]\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## The time that a series is not updated until considering it final.\n#   series_timeout = \"5m\"\n\n\n# # Create aggregate histograms.\n# [[aggregators.histogram]]\n#   ## The period in which to flush the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## If true, the histogram will be reset on flush instead\n#   ## of accumulating the results.\n#   reset = false\n#\n#   ## Whether bucket values should be accumulated. If set to false, \"gt\" tag will be added.\n#   ## Defaults to true.\n#   cumulative = true\n#\n#   ## Expiration interval for each histogram. The histogram will be expired if\n#   ## there are no changes in any buckets for this time interval. 0 == no expiration.\n#   # expiration_interval = \"0m\"\n#\n#   ## If true, aggregated histogram are pushed to output only if it was updated since\n#   ## previous push. Defaults to false.\n#   # push_only_on_update = false\n#\n#   ## Example config that aggregates all fields of the metric.\n#   # [[aggregators.histogram.config]]\n#   #   ## Right borders of buckets (with +Inf implicitly added).\n#   #   buckets = [0.0, 15.6, 34.5, 49.1, 71.5, 80.5, 94.5, 100.0]\n#   #   ## The name of metric.\n#   #   measurement_name = \"cpu\"\n#\n#   ## Example config that aggregates only specific fields of the metric.\n#   # [[aggregators.histogram.config]]\n#   #   ## Right borders of buckets (with +Inf implicitly added).\n#   #   buckets = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n#   #   ## The name of metric.\n#   #   measurement_name = \"diskio\"\n#   #   ## The concrete fields of metric\n#   #   fields = [\"io_time\", \"read_time\", \"write_time\"]\n\n\n# # Merge metrics into multifield metrics by series key\n# [[aggregators.merge]]\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = true\n\n\n# # Keep the aggregate min/max of each metric passing through.\n# [[aggregators.minmax]]\n#   ## General Aggregator Arguments:\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n\n\n# # Keep the aggregate quantiles of each metric passing through.\n# [[aggregators.quantile]]\n#   ## General Aggregator Arguments:\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## Quantiles to output in the range [0,1]\n#   # quantiles = [0.25, 0.5, 0.75]\n#\n#   ## Type of aggregation algorithm\n#   ## Supported are:\n#   ##  \"t-digest\" -- approximation using centroids, can cope with large number of samples\n#   ##  \"exact R7\" -- exact computation also used by Excel or NumPy (Hyndman \u0026 Fan 1996 R7)\n#   ##  \"exact R8\" -- exact computation (Hyndman \u0026 Fan 1996 R8)\n#   ## NOTE: Do not use \"exact\" algorithms with large number of samples\n#   ##       to not impair performance or memory consumption!\n#   # algorithm = \"t-digest\"\n#\n#   ## Compression for approximation (t-digest). The value needs to be\n#   ## greater or equal to 1.0. Smaller values will result in more\n#   ## performance but less accuracy.\n#   # compression = 100.0\n\n\n# # Aggregate metrics using a Starlark script\n# [[aggregators.starlark]]\n#   ## The Starlark source can be set as a string in this configuration file, or\n#   ## by referencing a file containing the script.  Only one source or script\n#   ## should be set at once.\n#   ##\n#   ## Source of the Starlark script.\n#   source = '''\n# state = {}\n#\n# def add(metric):\n#   state[\"last\"] = metric\n#\n# def push():\n#   return state.get(\"last\")\n#\n# def reset():\n#   state.clear()\n# '''\n#\n#   ## File containing a Starlark script.\n#   # script = \"/usr/local/bin/myscript.star\"\n#\n#   ## The constants of the Starlark script.\n#   # [aggregators.starlark.constants]\n#   #   max_size = 10\n#   #   threshold = 0.75\n#   #   default_name = \"Julia\"\n#   #   debug_mode = true\n\n\n# # Count the occurrence of values in fields.\n# [[aggregators.valuecounter]]\n#   ## General Aggregator Arguments:\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#   ## The fields for which the values will be counted\n#   fields = []\n\n\n###############################################################################\n#                            INPUT PLUGINS                                    #\n###############################################################################\n\n\n# Read metrics about cpu usage\n[[inputs.cpu]]\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states\n  report_active = false\n\n\n# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n\n# Read metrics about disk IO by device\n[[inputs.diskio]]\n  ## By default, telegraf will gather stats for all devices including\n  ## disk partitions.\n  ## Setting devices will restrict the stats to the specified devices.\n  # devices = [\"sda\", \"sdb\", \"vd*\"]\n  ## Uncomment the following line if you need disk serial numbers.\n  # skip_serial_number = false\n  #\n  ## On systems which support it, device metadata can be added in the form of\n  ## tags.\n  ## Currently only Linux is supported via udev properties. You can view\n  ## available properties for a device by running:\n  ## 'udevadm info -q property -n /dev/sda'\n  ## Note: Most, but not all, udev properties can be accessed this way. Properties\n  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.\n  # device_tags = [\"ID_FS_TYPE\", \"ID_FS_USAGE\"]\n  #\n  ## Using the same metadata source as device_tags, you can also customize the\n  ## name of the device via templates.\n  ## The 'name_templates' parameter is a list of templates to try and apply to\n\n  ## present for the device is used as the device name tag.\n  ## The typical use case is for LVM volumes, to get the VG/LV name instead of\n  ## the near-meaningless DM-0 name.\n  # name_templates = [\"$ID_FS_LABEL\",\"$DM_VG_NAME/$DM_LV_NAME\"]\n\n\n# Get kernel statistics from /proc/stat\n[[inputs.kernel]]\n  # no configuration\n\n\n# Read metrics about memory usage\n[[inputs.mem]]\n  # no configuration\n\n\n# Get the number of processes and group them by status\n[[inputs.processes]]\n  # no configuration\n\n\n# Read metrics about swap memory usage\n[[inputs.swap]]\n  # no configuration\n\n\n# Read metrics about system load \u0026 uptime\n[[inputs.system]]\n  ## Uncomment to remove deprecated metrics.\n  # fielddrop = [\"uptime_format\"]\n\n\n# # Gather ActiveMQ metrics\n# [[inputs.activemq]]\n#   ## ActiveMQ WebConsole URL\n#   url = \"http://127.0.0.1:8161\"\n#\n#   ## Credentials for basic HTTP authentication\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## Required ActiveMQ webadmin root path\n#   # webadmin = \"admin\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read stats from aerospike server(s)\n# [[inputs.aerospike]]\n#   ## Aerospike servers to connect to (with port)\n#   ## This plugin will query all namespaces the aerospike\n#   ## server has configured and get stats for them.\n#   servers = [\"localhost:3000\"]\n#\n#   # username = \"telegraf\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # enable_tls = false\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   # tls_name = \"tlsname\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n#\n#   # Feature Options\n#   # Add namespace variable to limit the namespaces executed on\n#   # Leave blank to do all\n#   # disable_query_namespaces = true # default false\n#   # namespaces = [\"namespace1\", \"namespace2\"]\n#\n#   # Enable set level telemetry\n#   # query_sets = true # default: false\n#   # Add namespace set combinations to limit sets executed on\n#   # Leave blank to do all sets\n#   # sets = [\"namespace1/set1\", \"namespace1/set2\", \"namespace3\"]\n#\n#   # Histograms\n#   # enable_ttl_histogram = true # default: false\n#   # enable_object_size_linear_histogram = true # default: false\n#\n#   # by default, aerospike produces a 100 bucket histogram\n#   # this is not great for most graphing tools, this will allow\n#   # the ability to squash this to a smaller number of buckets\n#   # To have a balanced histogram, the number of buckets chosen\n#   # should divide evenly into 100.\n#   # num_histogram_buckets = 100 # default: 10\n\n\n# # Query statistics from AMD Graphics cards using rocm-smi binary\n# [[inputs.amd_rocm_smi]]\n# ## Optional: path to rocm-smi binary, defaults to $PATH via exec.LookPath\n# # bin_path = \"/opt/rocm/bin/rocm-smi\"\n#\n# ## Optional: timeout for GPU polling\n# # timeout = \"5s\"\n\n\n# # Read Apache status information (mod_status)\n# [[inputs.apache]]\n#   ## An array of URLs to gather from, must be directed at the machine\n#   ## readable version of the mod_status page including the auto query string.\n#   ## Default is \"http://localhost/server-status?auto\".\n#   urls = [\"http://localhost/server-status?auto\"]\n#\n#   ## Credentials for basic HTTP authentication.\n#   # username = \"myuser\"\n#   # password = \"mypassword\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Monitor APC UPSes connected to apcupsd\n# [[inputs.apcupsd]]\n#   # A list of running apcupsd server to connect to.\n#   # If not provided will default to tcp://127.0.0.1:3551\n#   servers = [\"tcp://127.0.0.1:3551\"]\n#\n#   ## Timeout for dialing server.\n#   timeout = \"5s\"\n\n\n# # Gather metrics from Apache Aurora schedulers\n# [[inputs.aurora]]\n#   ## Schedulers are the base addresses of your Aurora Schedulers\n#   schedulers = [\"http://127.0.0.1:8081\"]\n#\n#   ## Set of role types to collect metrics from.\n#   ##\n#   ## The scheduler roles are checked each interval by contacting the\n#   ## scheduler nodes; zookeeper is not contacted.\n#   # roles = [\"leader\", \"follower\"]\n#\n#   ## Timeout is the max time for total network operations.\n#   # timeout = \"5s\"\n#\n#   ## Username and password are sent using HTTP Basic Auth.\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Gather Azure Storage Queue metrics\n# [[inputs.azure_storage_queue]]\n#   ## Required Azure Storage Account name\n#   account_name = \"mystorageaccount\"\n#\n#   ## Required Azure Storage Account access key\n#   account_key = \"storageaccountaccesskey\"\n#\n#   ## Set to false to disable peeking age of oldest message (executes faster)\n#   # peek_oldest_message_age = true\n\n\n# # Read metrics of bcache from stats_total and dirty_data\n# [[inputs.bcache]]\n#   ## Bcache sets path\n#   ## If not specified, then default is:\n#   bcachePath = \"/sys/fs/bcache\"\n#\n#   ## By default, Telegraf gather stats for all bcache devices\n#   ## Setting devices will restrict the stats to the specified\n#   ## bcache devices.\n#   bcacheDevs = [\"bcache0\"]\n\n\n# # Collects Beanstalkd server and tubes stats\n# [[inputs.beanstalkd]]\n#   ## Server to collect data from\n#   server = \"localhost:11300\"\n#\n#   ## List of tubes to gather stats about.\n#   ## If no tubes specified then data gathered for each tube on server reported by list-tubes command\n#   tubes = [\"notifications\"]\n\n\n# # Read metrics exposed by Beat\n# [[inputs.beat]]\n#   ## An URL from which to read Beat-formatted JSON\n#   ## Default is \"http://127.0.0.1:5066\".\n#   url = \"http://127.0.0.1:5066\"\n#\n#   ## Enable collection of the listed stats\n#   ## An empty list means collect all. Available options are currently\n#   ## \"beat\", \"libbeat\", \"system\" and \"filebeat\".\n#   # include = [\"beat\", \"libbeat\", \"filebeat\"]\n#\n#   ## HTTP method\n#   # method = \"GET\"\n#\n#   ## Optional HTTP headers\n#   # headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## Override HTTP \"Host\" header\n#   # host_header = \"logstash.example.com\"\n#\n#   ## Timeout for HTTP requests\n#   # timeout = \"5s\"\n#\n#   ## Optional HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read BIND nameserver XML statistics\n# [[inputs.bind]]\n#   ## An array of BIND XML statistics URI to gather stats.\n#   ## Default is \"http://localhost:8053/xml/v3\".\n#   # urls = [\"http://localhost:8053/xml/v3\"]\n#   # gather_memory_contexts = false\n#   # gather_views = false\n#\n#   ## Timeout for http requests made by bind nameserver\n#   # timeout = \"4s\"\n\n\n# # Collect bond interface status, slaves statuses and failures count\n# [[inputs.bond]]\n#   ## Sets 'proc' directory path\n#   ## If not specified, then default is /proc\n#   # host_proc = \"/proc\"\n#\n#   ## Sets 'sys' directory path\n#   ## If not specified, then default is /sys\n#   # host_sys = \"/sys\"\n#\n#   ## By default, telegraf gather stats for all bond interfaces\n#   ## Setting interfaces will restrict the stats to the specified\n#   ## bond interfaces.\n#   # bond_interfaces = [\"bond0\"]\n#\n#   ## Tries to collect additional bond details from /sys/class/net/{bond}\n#   ## currently only useful for LACP (mode 4) bonds\n#   # collect_sys_details = false\n#\n\n\n# # Collect Kafka topics and consumers status from Burrow HTTP API.\n# [[inputs.burrow]]\n#   ## Burrow API endpoints in format \"schema://host:port\".\n#   ## Default is \"http://localhost:8000\".\n#   servers = [\"http://localhost:8000\"]\n#\n#   ## Override Burrow API prefix.\n#   ## Useful when Burrow is behind reverse-proxy.\n#   # api_prefix = \"/v3/kafka\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Limit per-server concurrent connections.\n#   ## Useful in case of large number of topics or consumer groups.\n#   # concurrent_connections = 20\n#\n#   ## Filter clusters, default is no filtering.\n#   ## Values can be specified as glob patterns.\n#   # clusters_include = []\n#   # clusters_exclude = []\n#\n#   ## Filter consumer groups, default is no filtering.\n#   ## Values can be specified as glob patterns.\n#   # groups_include = []\n#   # groups_exclude = []\n#\n#   ## Filter topics, default is no filtering.\n#   ## Values can be specified as glob patterns.\n#   # topics_include = []\n#   # topics_exclude = []\n#\n#   ## Credentials for basic HTTP authentication.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional SSL config\n#   # ssl_ca = \"/etc/telegraf/ca.pem\"\n#   # ssl_cert = \"/etc/telegraf/cert.pem\"\n#   # ssl_key = \"/etc/telegraf/key.pem\"\n#   # insecure_skip_verify = false\n\n\n# # Collects performance metrics from the MON, OSD, MDS and RGW nodes in a Ceph storage cluster.\n# [[inputs.ceph]]\n#   ## This is the recommended interval to poll.  Too frequent and you will lose\n#   ## data points due to timeouts during rebalancing and recovery\n#   interval = '1m'\n#\n#   ## All configuration values are optional, defaults are shown below\n#\n#   ## location of ceph binary\n#   ceph_binary = \"/usr/bin/ceph\"\n#\n#   ## directory in which to look for socket files\n#   socket_dir = \"/var/run/ceph\"\n#\n#   ## prefix of MON and OSD socket files, used to determine socket type\n#   mon_prefix = \"ceph-mon\"\n#   osd_prefix = \"ceph-osd\"\n#   mds_prefix = \"ceph-mds\"\n#   rgw_prefix = \"ceph-client\"\n#\n#   ## suffix used to identify socket files\n#   socket_suffix = \"asok\"\n#\n#   ## Ceph user to authenticate as, ceph will search for the corresponding keyring\n#   ## e.g. client.admin.keyring in /etc/ceph, or the explicit path defined in the\n#   ## client section of ceph.conf for example:\n#   ##\n#   ##     [client.telegraf]\n#   ##         keyring = /etc/ceph/client.telegraf.keyring\n#   ##\n#   ## Consult the ceph documentation for more detail on keyring generation.\n#   ceph_user = \"client.admin\"\n#\n#   ## Ceph configuration to use to locate the cluster\n#   ceph_config = \"/etc/ceph/ceph.conf\"\n#\n#   ## Whether to gather statistics via the admin socket\n#   gather_admin_socket_stats = true\n#\n#   ## Whether to gather statistics via ceph commands, requires ceph_user and ceph_config\n#   ## to be specified\n#   gather_cluster_stats = false\n\n\n# # Read specific statistics per cgroup\n# [[inputs.cgroup]]\n#   ## Directories in which to look for files, globs are supported.\n#   ## Consider restricting paths to the set of cgroups you really\n#   ## want to monitor if you have a large number of cgroups, to avoid\n#   ## any cardinality issues.\n#   # paths = [\n#   #   \"/sys/fs/cgroup/memory\",\n#   #   \"/sys/fs/cgroup/memory/child1\",\n#   #   \"/sys/fs/cgroup/memory/child2/*\",\n#   # ]\n#   ## cgroup stat fields, as file names, globs are supported.\n#   ## these file names are appended to each path from above.\n#   # files = [\"memory.*usage*\", \"memory.limit_in_bytes\"]\n\n\n# # Get standard chrony metrics, requires chronyc executable.\n# [[inputs.chrony]]\n#   ## If true, chronyc tries to perform a DNS lookup for the time server.\n#   # dns_lookup = false\n\n\n# # Pull Metric Statistics from Amazon CloudWatch\n# [[inputs.cloudwatch]]\n#   ## Amazon Region\n#   region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   # access_key = \"\"\n#   # secret_key = \"\"\n#   # token = \"\"\n#   # role_arn = \"\"\n#   # web_identity_token_file = \"\"\n#   # role_session_name = \"\"\n#   # profile = \"\"\n#   # shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n#   # http_proxy_url = \"http://localhost:8888\"\n#\n#   # The minimum period for Cloudwatch metrics is 1 minute (60s). However not all\n#   # metrics are made available to the 1 minute period. Some are collected at\n#   # 3 minute, 5 minute, or larger intervals. See https://aws.amazon.com/cloudwatch/faqs/#monitoring.\n#   # Note that if a period is configured that is smaller than the minimum for a\n#   # particular metric, that metric will not be returned by the Cloudwatch API\n#   # and will not be collected by Telegraf.\n#   #\n#   ## Requested CloudWatch aggregation Period (required - must be a multiple of 60s)\n#   period = \"5m\"\n#\n#   ## Collection Delay (required - must account for metrics availability via CloudWatch API)\n#   delay = \"5m\"\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n#   ## gaps or overlap in pulled data\n#   interval = \"5m\"\n#\n#   ## Recommended if \"delay\" and \"period\" are both within 3 hours of request time. Invalid values will be ignored.\n#   ## Recently Active feature will only poll for CloudWatch ListMetrics values that occurred within the last 3 Hours.\n#   ## If enabled, it will reduce total API usage of the CloudWatch ListMetrics API and require less memory to retain.\n#   ## Do not enable if \"period\" or \"delay\" is longer than 3 hours, as it will not return data more than 3 hours old.\n#   ## See https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_ListMetrics.html\n#   #recently_active = \"PT3H\"\n#\n#   ## Configure the TTL for the internal cache of metrics.\n#   # cache_ttl = \"1h\"\n#\n#   ## Metric Statistic Namespaces (required)\n#   namespaces = [\"AWS/ELB\"]\n#   # A single metric statistic namespace that will be appended to namespaces on startup\n#   # namespace = \"AWS/ELB\"\n#\n#   ## Maximum requests per second. Note that the global default AWS rate limit is\n#   ## 50 reqs/sec, so if you define multiple namespaces, these should add up to a\n#   ## maximum of 50.\n#   ## See http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_limits.html\n#   # ratelimit = 25\n#\n#   ## Timeout for http requests made by the cloudwatch client.\n#   # timeout = \"5s\"\n#\n#   ## Namespace-wide statistic filters. These allow fewer queries to be made to\n#   ## cloudwatch.\n#   # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n#   # statistic_exclude = []\n#\n#   ## Metrics to Pull\n#   ## Defaults to all Metrics in Namespace if nothing is provided\n#   ## Refreshes Namespace available metrics every 1h\n#   #[[inputs.cloudwatch.metrics]]\n#   #  names = [\"Latency\", \"RequestCount\"]\n#   #\n#   #  ## Statistic filters for Metric.  These allow for retrieving specific\n#   #  ## statistics for an individual metric.\n#   #  # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n#   #  # statistic_exclude = []\n#   #\n#   #  ## Dimension filters for Metric.  All dimensions defined for the metric names\n#   #  ## must be specified in order to retrieve the metric statistics.\n#   #  ## 'value' has wildcard / 'glob' matching support such as 'p-*'.\n#   #  [[inputs.cloudwatch.metrics.dimensions]]\n#   #    name = \"LoadBalancerName\"\n#   #    value = \"p-example\"\n\n\n# # Collects conntrack stats from the configured directories and files.\n# [[inputs.conntrack]]\n#    ## The following defaults would work with multiple versions of conntrack.\n#    ## Note the nf_ and ip_ filename prefixes are mutually exclusive across\n#    ## kernel versions, as are the directory locations.\n#\n#    ## Superset of filenames to look for within the conntrack dirs.\n#    ## Missing files will be ignored.\n#    files = [\"ip_conntrack_count\",\"ip_conntrack_max\",\n#             \"nf_conntrack_count\",\"nf_conntrack_max\"]\n#\n#    ## Directories to search within for the conntrack files above.\n#    ## Missing directories will be ignored.\n#    dirs = [\"/proc/sys/net/ipv4/netfilter\",\"/proc/sys/net/netfilter\"]\n\n\n# # Gather health check statuses from services registered in Consul\n# [[inputs.consul]]\n#   ## Consul server address\n#   # address = \"localhost:8500\"\n#\n#   ## URI scheme for the Consul server, one of \"http\", \"https\"\n#   # scheme = \"http\"\n#\n#   ## Metric version controls the mapping from Consul metrics into\n#   ## Telegraf metrics.\n#   ##\n#   ##   example: metric_version = 1; deprecated in 1.15\n#   ##            metric_version = 2; recommended version\n#   # metric_version = 1\n#\n#   ## ACL token used in every request\n#   # token = \"\"\n#\n#   ## HTTP Basic Authentication username and password.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Data center to query the health checks from\n#   # datacenter = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n#\n#   ## Consul checks' tag splitting\n#   # When tags are formatted like \"key:value\" with \":\" as a delimiter then\n#   # they will be splitted and reported as proper key:value in Telegraf\n#   # tag_delimiter = \":\"\n\n\n# # Read metrics from the Consul API\n# [[inputs.consul_metrics]]\n#   ## URL for the Consul agent\n#   # url = \"http://127.0.0.1:8500\"\n#\n#   ## Use auth token for authorization.\n#   ## Only one of the options can be set. Leave empty to not use any token.\n#   # token_file = \"/path/to/auth/token\"\n#   ## OR\n#   # token = \"a1234567-40c7-9048-7bae-378687048181\"\n#\n#   ## Set timeout (default 5 seconds)\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n\n\n# # Read per-node and per-bucket metrics from Couchbase\n# [[inputs.couchbase]]\n#   ## specify servers via a url matching:\n#   ##  [protocol://][:password]@address[:port]\n#   ##  e.g.\n#   ##    http://couchbase-0.example.com/\n#   ##    http://admin:secret@couchbase-0.example.com:8091/\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no protocol is specified, HTTP is used.\n#   ## If no port is specified, 8091 is used.\n#   servers = [\"http://localhost:8091\"]\n#\n#   ## Filter bucket fields to include only here.\n#   # bucket_stats_included = [\"quota_percent_used\", \"ops_per_sec\", \"disk_fetches\", \"item_count\", \"disk_used\", \"data_used\", \"mem_used\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification (defaults to false)\n#   ## If set to false, tls_cert and tls_key are required\n#   # insecure_skip_verify = false\n\n\n# # Read CouchDB Stats from one or more servers\n# [[inputs.couchdb]]\n#   ## Works with CouchDB stats endpoints out of the box\n#   ## Multiple Hosts from which to read CouchDB stats:\n#   hosts = [\"http://localhost:8086/_stats\"]\n#\n#   ## Use HTTP Basic Authentication.\n#   # basic_username = \"telegraf\"\n#   # basic_password = \"p@ssw0rd\"\n\n\n# # Fetch metrics from a CSGO SRCDS\n# [[inputs.csgo]]\n#   ## Specify servers using the following format:\n#   ##    servers = [\n#   ##      [\"ip1:port1\", \"rcon_password1\"],\n#   ##      [\"ip2:port2\", \"rcon_password2\"],\n#   ##    ]\n#   #\n#   ## If no servers are specified, no data will be collected\n#   servers = []\n\n\n# # Input plugin for DC/OS metrics\n# [[inputs.dcos]]\n#   ## The DC/OS cluster URL.\n#   cluster_url = \"https://dcos-ee-master-1\"\n#\n#   ## The ID of the service account.\n#   service_account_id = \"telegraf\"\n#   ## The private key file for the service account.\n#   service_account_private_key = \"/etc/telegraf/telegraf-sa-key.pem\"\n#\n#   ## Path containing login token.  If set, will read on every gather.\n#   # token_file = \"/home/dcos/.dcos/token\"\n#\n#   ## In all filter options if both include and exclude are empty all items\n#   ## will be collected.  Arrays may contain glob patterns.\n#   ##\n#   ## Node IDs to collect metrics from.  If a node is excluded, no metrics will\n#   ## be collected for its containers or apps.\n#   # node_include = []\n#   # node_exclude = []\n#   ## Container IDs to collect container metrics from.\n#   # container_include = []\n#   # container_exclude = []\n#   ## Container IDs to collect app metrics from.\n#   # app_include = []\n#   # app_exclude = []\n#\n#   ## Maximum concurrent connections to the cluster.\n#   # max_connections = 10\n#   ## Maximum time to receive a response from cluster.\n#   # response_timeout = \"20s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n#\n#   ## Recommended filtering to reduce series cardinality.\n#   # [inputs.dcos.tagdrop]\n#   #   path = [\"/var/lib/mesos/slave/slaves/*\"]\n\n\n# # Read metrics from one or many disque servers\n# [[inputs.disque]]\n#   ## An array of URI to gather stats about. Specify an ip or hostname\n#   ## with optional port and password.\n#   ## ie disque://localhost, disque://10.10.3.33:18832, 10.0.0.1:10000, etc.\n#   ## If no servers are specified, then localhost is used as the host.\n#   servers = [\"localhost\"]\n\n\n# # Provide a native collection for dmsetup based statistics for dm-cache\n# [[inputs.dmcache]]\n#   ## Whether to report per-device stats or not\n#   per_device = true\n\n\n# # Query given DNS server and gives statistics\n# [[inputs.dns_query]]\n#   ## servers to query\n#   servers = [\"8.8.8.8\"]\n#\n#   ## Network is the network protocol name.\n#   # network = \"udp\"\n#\n#   ## Domains or subdomains to query.\n#   # domains = [\".\"]\n#\n#   ## Query record type.\n#   ## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n#   # record_type = \"A\"\n#\n#   ## Dns server port.\n#   # port = 53\n#\n#   ## Query timeout in seconds.\n#   # timeout = 2\n\n\n# # Read metrics about docker containers\n [[inputs.docker]]\n#   ## Docker Endpoint\n#   ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n#   ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n   endpoint = \"unix:///var/run/docker.sock\"\n#\n#   ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)\n#   gather_services = false\n#\n#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n#   source_tag = false\n#\n#   ## Containers to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all containers\n#   container_name_include = []\n#   container_name_exclude = []\n#\n#   ## Container states to include and exclude. Globs accepted.\n#   ## When empty only containers in the \"running\" state will be captured.\n#   ## example: container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n#   ## example: container_state_exclude = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n   container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n#   # container_state_exclude = []\n#\n#   ## Timeout for docker list, info, and stats commands\n   timeout = \"10s\"\n#\n#   ## Specifies for which classes a per-device metric should be issued\n#   ## Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...)\n#   ## Please note that this setting has no effect if 'perdevice' is set to 'true'\n#   # perdevice_include = [\"cpu\"]\n#\n#   ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice' values.\n#   ## Possible values are 'cpu', 'blkio' and 'network'\n#   ## Total 'cpu' is reported directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin.\n#   ## Please note that this setting has no effect if 'total' is set to 'false'\n#   # total_include = [\"cpu\", \"blkio\", \"network\"]\n#\n#   ## Which environment variables should we use as a tag\n#   ##tag_env = [\"JAVA_HOME\", \"HEAP_SIZE\"]\n#\n#   ## docker labels to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all labels as tags\n#   docker_label_include = []\n#   docker_label_exclude = []\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read statistics from one or many dovecot servers\n# [[inputs.dovecot]]\n#   ## specify dovecot servers via an address:port list\n#   ##  e.g.\n#   ##    localhost:24242\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   servers = [\"localhost:24242\"]\n#\n#   ## Type is one of \"user\", \"domain\", \"ip\", or \"global\"\n#   type = \"global\"\n#\n#   ## Wildcard matches like \"*.com\". An empty string \"\" is same as \"*\"\n#   ## If type = \"ip\" filters should be \u003cIP/network\u003e\n#   filters = [\"\"]\n\n\n# # Reads metrics from DPDK applications using v2 telemetry interface.\n# [[inputs.dpdk]]\n#   ## Path to DPDK telemetry socket. This shall point to v2 version of DPDK telemetry interface.\n#   # socket_path = \"/var/run/dpdk/rte/dpdk_telemetry.v2\"\n#\n#   ## Duration that defines how long the connected socket client will wait for a response before terminating connection.\n#   ## This includes both writing to and reading from socket. Since it's local socket access\n#   ## to a fast packet processing application, the timeout should be sufficient for most users.\n#   ## Setting the value to 0 disables the timeout (not recommended)\n#   # socket_access_timeout = \"200ms\"\n#\n#   ## Enables telemetry data collection for selected device types.\n#   ## Adding \"ethdev\" enables collection of telemetry from DPDK NICs (stats, xstats, link_status).\n#   ## Adding \"rawdev\" enables collection of telemetry from DPDK Raw Devices (xstats).\n#   # device_types = [\"ethdev\"]\n#\n#   ## List of custom, application-specific telemetry commands to query\n#   ## The list of available commands depend on the application deployed. Applications can register their own commands\n#   ##   via telemetry library API http://doc.dpdk.org/guides/prog_guide/telemetry_lib.html#registering-commands\n#   ## For e.g. L3 Forwarding with Power Management Sample Application this could be:\n#   ##   additional_commands = [\"/l3fwd-power/stats\"]\n#   # additional_commands = []\n#\n#   ## Allows turning off collecting data for individual \"ethdev\" commands.\n#   ## Remove \"/ethdev/link_status\" from list to start getting link status metrics.\n#   [inputs.dpdk.ethdev]\n#     exclude_commands = [\"/ethdev/link_status\"]\n#\n#   ## When running multiple instances of the plugin it's recommended to add a unique tag to each instance to identify\n#   ## metrics exposed by an instance of DPDK application. This is useful when multiple DPDK apps run on a single host.\n#   ##  [inputs.dpdk.tags]\n#   ##    dpdk_instance = \"my-fwd-app\"\n\n\n# # Read metrics about docker containers from Fargate/ECS v2, v3 meta endpoints.\n# [[inputs.ecs]]\n#   ## ECS metadata url.\n#   ## Metadata v2 API is used if set explicitly. Otherwise,\n#   ## v3 metadata endpoint API is used if available.\n#   # endpoint_url = \"\"\n#\n#   ## Containers to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all containers\n#   # container_name_include = []\n#   # container_name_exclude = []\n#\n#   ## Container states to include and exclude. Globs accepted.\n#   ## When empty only containers in the \"RUNNING\" state will be captured.\n#   ## Possible values are \"NONE\", \"PULLED\", \"CREATED\", \"RUNNING\",\n#   ## \"RESOURCES_PROVISIONED\", \"STOPPED\".\n#   # container_status_include = []\n#   # container_status_exclude = []\n#\n#   ## ecs labels to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all labels as tags\n#   ecs_label_include = [ \"com.amazonaws.ecs.*\" ]\n#   ecs_label_exclude = []\n#\n#   ## Timeout for queries.\n#   # timeout = \"5s\"\n\n\n# # Read stats from one or more Elasticsearch servers or clusters\n# [[inputs.elasticsearch]]\n#   ## specify a list of one or more Elasticsearch servers\n#   # you can add username and password to your url to use basic authentication:\n#   # servers = [\"http://user:pass@localhost:9200\"]\n#   servers = [\"http://localhost:9200\"]\n#\n#   ## Timeout for HTTP requests to the elastic search server(s)\n#   http_timeout = \"5s\"\n#\n#   ## When local is true (the default), the node will read only its own stats.\n#   ## Set local to false when you want to read the node stats from all nodes\n#   ## of the cluster.\n#   local = true\n#\n#   ## Set cluster_health to true when you want to also obtain cluster health stats\n#   cluster_health = false\n#\n#   ## Adjust cluster_health_level when you want to also obtain detailed health stats\n#   ## The options are\n#   ##  - indices (default)\n#   ##  - cluster\n#   # cluster_health_level = \"indices\"\n#\n#   ## Set cluster_stats to true when you want to also obtain cluster stats.\n#   cluster_stats = false\n#\n#   ## Only gather cluster_stats from the master node. To work this require local = true\n#   cluster_stats_only_from_master = true\n#\n#   ## Indices to collect; can be one or more indices names or _all\n#   ## Use of wildcards is allowed. Use a wildcard at the end to retrieve index names that end with a changing value, like a date.\n#   indices_include = [\"_all\"]\n#\n#   ## One of \"shards\", \"cluster\", \"indices\"\n#   indices_level = \"shards\"\n#\n#   ## node_stats is a list of sub-stats that you want to have gathered. Valid options\n#   ## are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\", \"fs\", \"transport\", \"http\",\n#   ## \"breaker\". Per default, all stats are gathered.\n#   # node_stats = [\"jvm\", \"http\"]\n#\n#   ## HTTP Basic Authentication username and password.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Sets the number of most recent indices to return for indices that are configured with a date-stamped suffix.\n#   ## Each 'indices_include' entry ending with a wildcard (*) or glob matching pattern will group together all indices that match it, and sort them\n#   ## by the date or number after the wildcard. Metrics then are gathered for only the 'num_most_recent_indices' amount of most recent indices.\n#   # num_most_recent_indices = 0\n\n\n# # Derive metrics from aggregating Elasticsearch query results\n# [[inputs.elasticsearch_query]]\n#   ## The full HTTP endpoint URL for your Elasticsearch instance\n#   ## Multiple urls can be specified as part of the same cluster,\n#   ## this means that only ONE of the urls will be written to each interval.\n#   urls = [ \"http://node1.es.example.com:9200\" ] # required.\n#\n#   ## Elasticsearch client timeout, defaults to \"5s\".\n#   # timeout = \"5s\"\n#\n#   ## Set to true to ask Elasticsearch a list of all cluster nodes,\n#   ## thus it is not necessary to list all nodes in the urls config option\n#   # enable_sniffer = false\n#\n#   ## Set the interval to check if the Elasticsearch nodes are available\n#   ## This option is only used if enable_sniffer is also set (0s to disable it)\n#   # health_check_interval = \"10s\"\n#\n#   ## HTTP basic authentication details (eg. when using x-pack)\n#   # username = \"telegraf\"\n#   # password = \"mypassword\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   [[inputs.elasticsearch_query.aggregation]]\n#     ## measurement name for the results of the aggregation query\n#     measurement_name = \"measurement\"\n#\n#     ## Elasticsearch indexes to query (accept wildcards).\n#     index = \"index-*\"\n#\n#     ## The date/time field in the Elasticsearch index (mandatory).\n#     date_field = \"@timestamp\"\n#\n#     ## If the field used for the date/time field in Elasticsearch is also using\n#     ## a custom date/time format it may be required to provide the format to\n#     ## correctly parse the field.\n#     ##\n#     ## If using one of the built in elasticsearch formats this is not required.\n#     # date_field_custom_format = \"\"\n#\n#     ## Time window to query (eg. \"1m\" to query documents from last minute).\n#     ## Normally should be set to same as collection interval\n#     query_period = \"1m\"\n#\n#     ## Lucene query to filter results\n#     # filter_query = \"*\"\n#\n#     ## Fields to aggregate values (must be numeric fields)\n#     # metric_fields = [\"metric\"]\n#\n#     ## Aggregation function to use on the metric fields\n#     ## Must be set if 'metric_fields' is set\n#     ## Valid values are: avg, sum, min, max, sum\n#     # metric_function = \"avg\"\n#\n#     ## Fields to be used as tags\n#     ## Must be text, non-analyzed fields. Metric aggregations are performed per tag\n#     # tags = [\"field.keyword\", \"field2.keyword\"]\n#\n#     ## Set to true to not ignore documents when the tag(s) above are missing\n#     # include_missing_tag = false\n#\n#     ## String value of the tag when the tag does not exist\n#     ## Used when include_missing_tag is true\n#     # missing_tag_value = \"null\"\n\n\n# # Returns ethtool statistics for given interfaces\n# [[inputs.ethtool]]\n#   ## List of interfaces to pull metrics for\n#   # interface_include = [\"eth0\"]\n#\n#   ## List of interfaces to ignore when pulling metrics.\n#   # interface_exclude = [\"eth1\"]\n#\n#   ## Some drivers declare statistics with extra whitespace, different spacing,\n#   ## and mix cases. This list, when enabled, can be used to clean the keys.\n#   ## Here are the current possible normalizations:\n#   ##  * snakecase: converts fooBarBaz to foo_bar_baz\n#   ##  * trim: removes leading and trailing whitespace\n#   ##  * lower: changes all capitalized letters to lowercase\n#   ##  * underscore: replaces spaces with underscores\n#   # normalize_keys = [\"snakecase\", \"trim\", \"lower\", \"underscore\"]\n\n\n# # Read metrics from one or more commands that can output to stdout\n# [[inputs.exec]]\n#   ## Commands array\n#   commands = [\n#     \"/tmp/test.sh\",\n#     \"/usr/bin/mycollector --foo=bar\",\n#     \"/tmp/collect_*.sh\"\n#   ]\n#\n#   ## Timeout for each command to complete.\n#   timeout = \"5s\"\n#\n#   ## measurement name suffix (for separating different commands)\n#   name_suffix = \"_mycollector\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read metrics from fail2ban.\n# [[inputs.fail2ban]]\n#   ## Use sudo to run fail2ban-client\n#   use_sudo = false\n\n\n# # Read devices value(s) from a Fibaro controller\n# [[inputs.fibaro]]\n#   ## Required Fibaro controller address/hostname.\n#   ## Note: at the time of writing this plugin, Fibaro only implemented http - no https available\n#   url = \"http://\u003ccontroller\u003e:80\"\n#\n#   ## Required credentials to access the API (http://\u003ccontroller/api/\u003ccomponent\u003e)\n#   username = \"\u003cusername\u003e\"\n#   password = \"\u003cpassword\u003e\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n\n\n# # Parse a complete file each interval\n# [[inputs.file]]\n#   ## Files to parse each interval.  Accept standard unix glob matching rules,\n#   ## as well as ** to match recursive files and directories.\n#   files = [\"/tmp/metrics.out\"]\n#\n#\n#   ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n#   ## to disable. Cautious when file name variation is high, this can increase the cardinality\n#   ## significantly. Read more about cardinality here:\n#   ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n#   # file_tag = \"\"\n#   #\n#\n#   ## Character encoding to use when interpreting the file contents.  Invalid\n#   ## characters are replaced using the unicode replacement character.  When set\n#   ## to the empty string the data is not decoded to text.\n#   ##   ex: character_encoding = \"utf-8\"\n#   ##       character_encoding = \"utf-16le\"\n#   ##       character_encoding = \"utf-16be\"\n#   ##       character_encoding = \"\"\n#   # character_encoding = \"\"\n#\n#   ## The dataformat to be read from files\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Count files in a directory\n# [[inputs.filecount]]\n#   ## Directories to gather stats about.\n#   ## This accept standard unit glob matching rules, but with the addition of\n#   ## ** as a \"super asterisk\". ie:\n#   ##   /var/log/**    -\u003e recursively find all directories in /var/log and count files in each directories\n#   ##   /var/log/*/*   -\u003e find all directories with a parent dir in /var/log and count files in each directories\n#   ##   /var/log       -\u003e count all files in /var/log and all of its subdirectories\n#   directories = [\"/var/cache/apt/archives\"]\n#\n#   ## Only count files that match the name pattern. Defaults to \"*\".\n#   name = \"*.deb\"\n#\n#   ## Count files in subdirectories. Defaults to true.\n#   recursive = false\n#\n#   ## Only count regular files. Defaults to true.\n#   regular_only = true\n#\n#   ## Follow all symlinks while walking the directory tree. Defaults to false.\n#   follow_symlinks = false\n#\n#   ## Only count files that are at least this size. If size is\n#   ## a negative number, only count files that are smaller than the\n#   ## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n#   ## Without quotes and units, interpreted as size in bytes.\n#   size = \"0B\"\n#\n#   ## Only count files that have not been touched for at least this\n#   ## duration. If mtime is negative, only count files that have been\n#   ## touched in this duration. Defaults to \"0s\".\n#   mtime = \"0s\"\n\n\n# # Read stats about given file(s)\n# [[inputs.filestat]]\n#   ## Files to gather stats about.\n#   ## These accept standard unix glob matching rules, but with the addition of\n#   ## ** as a \"super asterisk\". ie:\n#   ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n#   ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n#   ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n#   ##\n#   ## See https://github.com/gobwas/glob for more examples\n#   ##\n#   files = [\"/var/log/**.log\"]\n#\n#   ## If true, read the entire file and calculate an md5 checksum.\n#   md5 = false\n\n\n# # Read real time temps from fireboard.io servers\n# [[inputs.fireboard]]\n#   ## Specify auth token for your account\n#   auth_token = \"invalidAuthToken\"\n#   ## You can override the fireboard server URL if necessary\n#   # url = https://fireboard.io/api/v1/devices.json\n#   ## You can set a different http_timeout if you need to\n#   ## You should set a string using an number and time indicator\n#   ## for example \"12s\" for 12 seconds.\n#   # http_timeout = \"4s\"\n\n\n# # Read metrics exposed by fluentd in_monitor plugin\n# [[inputs.fluentd]]\n#   ## This plugin reads information exposed by fluentd (using /api/plugins.json endpoint).\n#   ##\n#   ## Endpoint:\n#   ## - only one URI is allowed\n#   ## - https is not supported\n#   endpoint = \"http://localhost:24220/api/plugins.json\"\n#\n#   ## Define which plugins have to be excluded (based on \"type\" field - e.g. monitor_agent)\n#   exclude = [\n# \t  \"monitor_agent\",\n# \t  \"dummy\",\n#   ]\n\n\n# # Gather repository information from GitHub hosted repositories.\n# [[inputs.github]]\n#   ## List of repositories to monitor.\n#   repositories = [\n# \t  \"influxdata/telegraf\",\n# \t  \"influxdata/influxdb\"\n#   ]\n#\n#   ## Github API access token.  Unauthenticated requests are limited to 60 per hour.\n#   # access_token = \"\"\n#\n#   ## Github API enterprise url. Github Enterprise accounts must specify their base url.\n#   # enterprise_base_url = \"\"\n#\n#   ## Timeout for HTTP requests.\n#   # http_timeout = \"5s\"\n#\n#   ## List of additional fields to query.\n# \t## NOTE: Getting those fields might involve issuing additional API-calls, so please\n# \t##       make sure you do not exceed the rate-limit of GitHub.\n# \t##\n# \t## Available fields are:\n# \t## \t- pull-requests\t\t\t-- number of open and closed pull requests (2 API-calls per repository)\n#   # additional_fields = []\n\n\n# # Read flattened metrics from one or more GrayLog HTTP endpoints\n# [[inputs.graylog]]\n#   ## API endpoint, currently supported API:\n#   ##\n#   ##   - multiple  (e.g. http://\u003chost\u003e:9000/api/system/metrics/multiple)\n#   ##   - namespace (e.g. http://\u003chost\u003e:9000/api/system/metrics/namespace/{namespace})\n#   ##\n#   ## For namespace endpoint, the metrics array will be ignored for that call.\n#   ## Endpoint can contain namespace and multiple type calls.\n#   ##\n#   ## Please check http://[graylog-server-ip]:9000/api/api-browser for full list\n#   ## of endpoints\n#   servers = [\n#     \"http://[graylog-server-ip]:9000/api/system/metrics/multiple\",\n#   ]\n#\n#   ## Set timeout (default 5 seconds)\n#   # timeout = \"5s\"\n#\n#   ## Metrics list\n#   ## List of metrics can be found on Graylog webservice documentation.\n#   ## Or by hitting the web service api at:\n#   ##   http://[graylog-host]:9000/api/system/metrics\n#   metrics = [\n#     \"jvm.cl.loaded\",\n#     \"jvm.memory.pools.Metaspace.committed\"\n#   ]\n#\n#   ## Username and password\n#   username = \"\"\n#   password = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics of haproxy, via socket or csv stats page\n# [[inputs.haproxy]]\n#   ## An array of address to gather stats about. Specify an ip on hostname\n#   ## with optional port. ie localhost, 10.10.3.33:1936, etc.\n#   ## Make sure you specify the complete path to the stats endpoint\n#   ## including the protocol, ie http://10.10.3.33:1936/haproxy?stats\n#\n#   ## If no servers are specified, then default to 127.0.0.1:1936/haproxy?stats\n#   servers = [\"http://myhaproxy.com:1936/haproxy?stats\"]\n#\n#   ## Credentials for basic HTTP authentication\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## You can also use local socket with standard wildcard globbing.\n#   ## Server address not starting with 'http' will be treated as a possible\n#   ## socket, so both examples below are valid.\n#   # servers = [\"socket:/run/haproxy/admin.sock\", \"/run/haproxy/*.sock\"]\n#\n#   ## By default, some of the fields are renamed from what haproxy calls them.\n#   ## Setting this option to true results in the plugin keeping the original\n#   ## field names.\n#   # keep_field_names = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Monitor disks' temperatures using hddtemp\n# [[inputs.hddtemp]]\n#   ## By default, telegraf gathers temps data from all disks detected by the\n#   ## hddtemp.\n#   ##\n#   ## Only collect temps from the selected disks.\n#   ##\n#   ## A * as the device name will return the temperature values of all disks.\n#   ##\n#   # address = \"127.0.0.1:7634\"\n#   # devices = [\"sda\", \"*\"]\n\n\n# # Read formatted metrics from one or more HTTP endpoints\n# [[inputs.http]]\n#   ## One or more URLs from which to read formatted metrics\n#   urls = [\n#     \"http://localhost/metrics\"\n#   ]\n#\n#   ## HTTP method\n#   # method = \"GET\"\n#\n#   ## Optional HTTP headers\n#   # headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## Optional file with Bearer token\n#   ## file content is added as an Authorization header\n#   # bearer_token = \"/path/to/file\"\n#\n#   ## Optional HTTP Basic Auth Credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## HTTP entity-body to send with POST/PUT requests.\n#   # body = \"\"\n#\n#   ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## HTTP Proxy support\n#   # http_proxy_url = \"\"\n#\n#   ## OAuth2 Client Credentials Grant\n#   # client_id = \"clientid\"\n#   # client_secret = \"secret\"\n#   # token_url = \"https://indentityprovider/oauth2/v1/token\"\n#   # scopes = [\"urn:opc:idm:__myscopes__\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional Cookie authentication\n#   # cookie_auth_url = \"https://localhost/authMe\"\n#   # cookie_auth_method = \"POST\"\n#   # cookie_auth_username = \"username\"\n#   # cookie_auth_password = \"pa$$word\"\n#   # cookie_auth_headers = '{\"Content-Type\": \"application/json\", \"X-MY-HEADER\":\"hello\"}'\n#   # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n#   ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n#   # cookie_auth_renewal = \"5m\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n#\n#   ## List of success status codes\n#   # success_status_codes = [200]\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   # data_format = \"influx\"\n\n\n# # HTTP/HTTPS request given an address a method and a timeout\n# [[inputs.http_response]]\n#   ## List of urls to query.\n#   # urls = [\"http://localhost\"]\n#\n#   ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n#   # http_proxy = \"http://localhost:8888\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## HTTP Request Method\n#   # method = \"GET\"\n#\n#   ## Whether to follow redirects from the server (defaults to false)\n#   # follow_redirects = false\n#\n#   ## Optional file with Bearer token\n#   ## file content is added as an Authorization header\n#   # bearer_token = \"/path/to/file\"\n#\n#   ## Optional HTTP Basic Auth Credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional HTTP Request Body\n#   # body = '''\n#   # {'fake':'data'}\n#   # '''\n#\n#   ## Optional name of the field that will contain the body of the response.\n#   ## By default it is set to an empty String indicating that the body's content won't be added\n#   # response_body_field = ''\n#\n#   ## Maximum allowed HTTP response body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   ## If the response body size exceeds this limit a \"body_read_error\" will be raised\n#   # response_body_max_size = \"32MiB\"\n#\n#   ## Optional substring or regex match in body of the response (case sensitive)\n#   # response_string_match = \"\\\"service_status\\\": \\\"up\\\"\"\n#   # response_string_match = \"ok\"\n#   # response_string_match = \"\\\".*_status\\\".?:.?\\\"up\\\"\"\n#\n#   ## Expected response status code.\n#   ## The status code of the response is compared to this value. If they match, the field\n#   ## \"response_status_code_match\" will be 1, otherwise it will be 0. If the\n#   ## expected status code is 0, the check is disabled and the field won't be added.\n#   # response_status_code = 0\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## HTTP Request Headers (all values must be strings)\n#   # [inputs.http_response.headers]\n#   #   Host = \"github.com\"\n#\n#   ## Optional setting to map response http headers into tags\n#   ## If the http header is not present on the request, no corresponding tag will be added\n#   ## If multiple instances of the http header are present, only the first value will be used\n#   # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n#\n#   ## Interface to use when dialing an address\n#   # interface = \"eth0\"\n\n\n# # Read flattened metrics from one or more JSON HTTP endpoints\n# [[inputs.httpjson]]\n#   ## DEPRECATED: The 'httpjson' plugin is deprecated in version 1.6.0, use 'inputs.http' instead.\n#   ## NOTE This plugin only reads numerical measurements, strings and booleans\n#   ## will be ignored.\n#\n#   ## URL of each server in the service's cluster\n#   servers = [\n#     \"http://localhost:9999/stats/\",\n#     \"http://localhost:9998/stats/\",\n#   ]\n#   ## Set response_timeout (default 5 seconds)\n#   response_timeout = \"5s\"\n#\n#   ## HTTP method to use: GET or POST (case-sensitive)\n#   method = \"GET\"\n#\n#   ## List of tag names to extract from top-level of JSON server response\n#   # tag_keys = [\n#   #   \"my_tag_1\",\n#   #   \"my_tag_2\"\n#   # ]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## HTTP parameters (all values must be strings).  For \"GET\" requests, data\n#   ## will be included in the query.  For \"POST\" requests, data will be included\n#   ## in the request body as \"x-www-form-urlencoded\".\n#   # [inputs.httpjson.parameters]\n#   #   event_type = \"cpu_spike\"\n#   #   threshold = \"0.75\"\n#\n#   ## HTTP Headers (all values must be strings)\n#   # [inputs.httpjson.headers]\n#   #   X-Auth-Token = \"my-xauth-token\"\n#   #   apiVersion = \"v1\"\n\n\n# # Gather Icinga2 status\n# [[inputs.icinga2]]\n#   ## Required Icinga2 server address\n#   # server = \"https://localhost:5665\"\n#\n#   ## Required Icinga2 object type (\"services\" or \"hosts\")\n#   # object_type = \"services\"\n#\n#   ## Credentials for basic HTTP authentication\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Gets counters from all InfiniBand cards and ports installed\n# [[inputs.infiniband]]\n#   # no configuration\n\n\n# # Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints\n# [[inputs.influxdb]]\n#   ## Works with InfluxDB debug endpoints out of the box,\n#   ## but other services can use this format too.\n#   ## See the influxdb plugin's README for more details.\n#\n#   ## Multiple URLs from which to read InfluxDB-formatted JSON\n#   ## Default is \"http://localhost:8086/debug/vars\".\n#   urls = [\n#     \"http://localhost:8086/debug/vars\"\n#   ]\n#\n#   ## Username and password to send using HTTP Basic Authentication.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## http request \u0026 header timeout\n#   timeout = \"5s\"\n\n\n# # Intel PowerStat plugin enables monitoring of platform metrics (power, TDP) and Core metrics like temperature, power and utilization.\n# [[inputs.intel_powerstat]]\n#   ## All global metrics are always collected by Intel PowerStat plugin.\n#   ## User can choose which per-CPU metrics are monitored by the plugin in cpu_metrics array.\n#   ## Empty array means no per-CPU specific metrics will be collected by the plugin - in this case only platform level\n#   ## telemetry will be exposed by Intel PowerStat plugin.\n#   ## Supported options:\n#   ## \"cpu_frequency\", \"cpu_busy_frequency\", \"cpu_temperature\", \"cpu_c1_state_residency\", \"cpu_c6_state_residency\", \"cpu_busy_cycles\"\n#   # cpu_metrics = []\n\n\n# # Collect statistics about itself\n# [[inputs.internal]]\n#   ## If true, collect telegraf memory stats.\n#   # collect_memstats = true\n\n\n# # Monitors internet speed using speedtest.net service\n# [[inputs.internet_speed]]\n#   ## Sets if runs file download test\n#   # enable_file_download = false\n#\n#   ## Caches the closest server location\n#   # cache = false\n\n\n# # This plugin gathers interrupts data from /proc/interrupts and /proc/softirqs.\n# [[inputs.interrupts]]\n#   ## When set to true, cpu metrics are tagged with the cpu.  Otherwise cpu is\n#   ## stored as a field.\n#   ##\n#   ## The default is false for backwards compatibility, and will be changed to\n#   ## true in a future version.  It is recommended to set to true on new\n#   ## deployments.\n#   # cpu_as_tag = false\n#\n#   ## To filter which IRQs to collect, make use of tagpass / tagdrop, i.e.\n#   # [inputs.interrupts.tagdrop]\n#   #   irq = [ \"NET_RX\", \"TASKLET\" ]\n\n\n# # Read metrics from the bare metal servers via IPMI\n# [[inputs.ipmi_sensor]]\n#   ## optionally specify the path to the ipmitool executable\n#   # path = \"/usr/bin/ipmitool\"\n#   ##\n#   ## Setting 'use_sudo' to true will make use of sudo to run ipmitool.\n#   ## Sudo must be configured to allow the telegraf user to run ipmitool\n#   ## without a password.\n#   # use_sudo = false\n#   ##\n#   ## optionally force session privilege level. Can be CALLBACK, USER, OPERATOR, ADMINISTRATOR\n#   # privilege = \"ADMINISTRATOR\"\n#   ##\n#   ## optionally specify one or more servers via a url matching\n#   ##  [username[:password]@][protocol[(address)]]\n#   ##  e.g.\n#   ##    root:passwd@lan(127.0.0.1)\n#   ##\n#   ## if no servers are specified, local machine sensor stats will be queried\n#   ##\n#   # servers = [\"USERID:PASSW0RD@lan(192.168.1.1)\"]\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'timeout' to avoid\n#   ## gaps or overlap in pulled data\n#   interval = \"30s\"\n#\n#   ## Timeout for the ipmitool command to complete\n#   timeout = \"20s\"\n#\n#   ## Schema Version: (Optional, defaults to version 1)\n#   metric_version = 2\n#\n#   ## Optionally provide the hex key for the IMPI connection.\n#   # hex_key = \"\"\n#\n#   ## If ipmitool should use a cache\n#   ## for me ipmitool runs about 2 to 10 times faster with cache enabled on HP G10 servers (when using ubuntu20.04)\n#   ## the cache file may not work well for you if some sensors come up late\n#   # use_cache = false\n#\n#   ## Path to the ipmitools cache file (defaults to OS temp dir)\n#   ## The provided path must exist and must be writable\n#   # cache_path = \"\"\n\n\n# # Gather packets and bytes counters from Linux ipsets\n# [[inputs.ipset]]\n#   ## By default, we only show sets which have already matched at least 1 packet.\n#   ## set include_unmatched_sets = true to gather them all.\n#   include_unmatched_sets = false\n#   ## Adjust your sudo settings appropriately if using this option (\"sudo ipset save\")\n#   use_sudo = false\n#   ## The default timeout of 1s for ipset execution can be overridden here:\n#   # timeout = \"1s\"\n\n\n# # Gather packets and bytes throughput from iptables\n# [[inputs.iptables]]\n#   ## iptables require root access on most systems.\n#   ## Setting 'use_sudo' to true will make use of sudo to run iptables.\n#   ## Users must configure sudo to allow telegraf user to run iptables with no password.\n#   ## iptables can be restricted to only list command \"iptables -nvL\".\n#   use_sudo = false\n#   ## Setting 'use_lock' to true runs iptables with the \"-w\" option.\n#   ## Adjust your sudo settings appropriately if using this option (\"iptables -w 5 -nvl\")\n#   use_lock = false\n#   ## Define an alternate executable, such as \"ip6tables\". Default is \"iptables\".\n#   # binary = \"ip6tables\"\n#   ## defines the table to monitor:\n#   table = \"filter\"\n#   ## defines the chains to monitor.\n#   ## NOTE: iptables rules without a comment will not be monitored.\n#   ## Read the plugin documentation for more information.\n#   chains = [ \"INPUT\" ]\n\n\n# # Collect virtual and real server stats from Linux IPVS\n# [[inputs.ipvs]]\n#   # no configuration\n\n\n# # Read jobs and cluster metrics from Jenkins instances\n# [[inputs.jenkins]]\n#   ## The Jenkins URL in the format \"schema://host:port\"\n#   url = \"http://my-jenkins-instance:8080\"\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## Set response_timeout\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use SSL but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional Max Job Build Age filter\n#   ## Default 1 hour, ignore builds older than max_build_age\n#   # max_build_age = \"1h\"\n#\n#   ## Optional Sub Job Depth filter\n#   ## Jenkins can have unlimited layer of sub jobs\n#   ## This config will limit the layers of pulling, default value 0 means\n#   ## unlimited pulling until no more sub jobs\n#   # max_subjob_depth = 0\n#\n#   ## Optional Sub Job Per Layer\n#   ## In workflow-multibranch-plugin, each branch will be created as a sub job.\n#   ## This config will limit to call only the lasted branches in each layer,\n#   ## empty will use default value 10\n#   # max_subjob_per_layer = 10\n#\n#   ## Jobs to include or exclude from gathering\n#   ## When using both lists, job_exclude has priority.\n#   ## Wildcards are supported: [ \"jobA/*\", \"jobB/subjob1/*\"]\n#   # job_include = [ \"*\" ]\n#   # job_exclude = [ ]\n#\n#   ## Nodes to include or exclude from gathering\n#   ## When using both lists, node_exclude has priority.\n#   # node_include = [ \"*\" ]\n#   # node_exclude = [ ]\n#\n#   ## Worker pool for jenkins plugin only\n#   ## Empty this field will use default value 5\n#   # max_connections = 5\n\n\n# # Read JMX metrics through Jolokia\n# [[inputs.jolokia]]\n#   ## DEPRECATED: The 'jolokia' plugin is deprecated in version 1.5.0, use 'inputs.jolokia2' instead.\n#   # DEPRECATED: the jolokia plugin has been deprecated in favor of the\n#   # jolokia2 plugin\n#   # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n#\n#   ## This is the context root used to compose the jolokia url\n#   ## NOTE that Jolokia requires a trailing slash at the end of the context root\n#   ## NOTE that your jolokia security policy must allow for POST requests.\n#   context = \"/jolokia/\"\n#\n#   ## This specifies the mode used\n#   # mode = \"proxy\"\n#   #\n#   ## When in proxy mode this section is used to specify further\n#   ## proxy address configurations.\n#   ## Remember to change host address to fit your environment.\n#   # [inputs.jolokia.proxy]\n#   #   host = \"127.0.0.1\"\n#   #   port = \"8080\"\n#\n#   ## Optional http timeouts\n#   ##\n#   ## response_header_timeout, if non-zero, specifies the amount of time to wait\n#   ## for a server's response headers after fully writing the request.\n#   # response_header_timeout = \"3s\"\n#   ##\n#   ## client_timeout specifies a time limit for requests made by this client.\n#   ## Includes connection time, any redirects, and reading the response body.\n#   # client_timeout = \"4s\"\n#\n#   ## Attribute delimiter\n#   ##\n#   ## When multiple attributes are returned for a single\n#   ## [inputs.jolokia.metrics], the field name is a concatenation of the metric\n#   ## name, and the attribute name, separated by the given delimiter.\n#   # delimiter = \"_\"\n#\n#   ## List of servers exposing jolokia read service\n#   [[inputs.jolokia.servers]]\n#     name = \"as-server-01\"\n#     host = \"127.0.0.1\"\n#     port = \"8080\"\n#     # username = \"myuser\"\n#     # password = \"mypassword\"\n#\n#   ## List of metrics collected on above servers\n#   ## Each metric consists in a name, a jmx path and either\n#   ## a pass or drop slice attribute.\n#   ## This collect all heap memory usage metrics.\n#   [[inputs.jolokia.metrics]]\n#     name = \"heap_memory_usage\"\n#     mbean  = \"java.lang:type=Memory\"\n#     attribute = \"HeapMemoryUsage\"\n#\n#   ## This collect thread counts metrics.\n#   [[inputs.jolokia.metrics]]\n#     name = \"thread_count\"\n#     mbean  = \"java.lang:type=Threading\"\n#     attribute = \"TotalStartedThreadCount,ThreadCount,DaemonThreadCount,PeakThreadCount\"\n#\n#   ## This collect number of class loaded/unloaded counts metrics.\n#   [[inputs.jolokia.metrics]]\n#     name = \"class_count\"\n#     mbean  = \"java.lang:type=ClassLoading\"\n#     attribute = \"LoadedClassCount,UnloadedClassCount,TotalLoadedClassCount\"\n\n\n# # Read JMX metrics from a Jolokia REST agent endpoint\n# [[inputs.jolokia2_agent]]\n#   # default_tag_prefix      = \"\"\n#   # default_field_prefix    = \"\"\n#   # default_field_separator = \".\"\n#\n#   # Add agents URLs to query\n#   urls = [\"http://localhost:8080/jolokia\"]\n#   # username = \"\"\n#   # password = \"\"\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS config\n#   # tls_ca   = \"/var/private/ca.pem\"\n#   # tls_cert = \"/var/private/client.pem\"\n#   # tls_key  = \"/var/private/client-key.pem\"\n#   # insecure_skip_verify = false\n#\n#   ## Add metrics to read\n#   [[inputs.jolokia2_agent.metric]]\n#     name  = \"java_runtime\"\n#     mbean = \"java.lang:type=Runtime\"\n#     paths = [\"Uptime\"]\n\n\n# # Read JMX metrics from a Jolokia REST proxy endpoint\n# [[inputs.jolokia2_proxy]]\n#   # default_tag_prefix      = \"\"\n#   # default_field_prefix    = \"\"\n#   # default_field_separator = \".\"\n#\n#   ## Proxy agent\n#   url = \"http://localhost:8080/jolokia\"\n#   # username = \"\"\n#   # password = \"\"\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS config\n#   # tls_ca   = \"/var/private/ca.pem\"\n#   # tls_cert = \"/var/private/client.pem\"\n#   # tls_key  = \"/var/private/client-key.pem\"\n#   # insecure_skip_verify = false\n#\n#   ## Add proxy targets to query\n#   # default_target_username = \"\"\n#   # default_target_password = \"\"\n#   [[inputs.jolokia2_proxy.target]]\n#     url = \"service:jmx:rmi:///jndi/rmi://targethost:9999/jmxrmi\"\n#     # username = \"\"\n#     # password = \"\"\n#\n#   ## Add metrics to read\n#   [[inputs.jolokia2_proxy.metric]]\n#     name  = \"java_runtime\"\n#     mbean = \"java.lang:type=Runtime\"\n#     paths = [\"Uptime\"]\n\n\n# # Read Kapacitor-formatted JSON metrics from one or more HTTP endpoints\n# [[inputs.kapacitor]]\n#   ## Multiple URLs from which to read Kapacitor-formatted JSON\n#   ## Default is \"http://localhost:9092/kapacitor/v1/debug/vars\".\n#   urls = [\n#     \"http://localhost:9092/kapacitor/v1/debug/vars\"\n#   ]\n#\n#   ## Time limit for http requests\n#   timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Get kernel statistics from /proc/vmstat\n# [[inputs.kernel_vmstat]]\n#   # no configuration\n\n\n# # Read status information from one or more Kibana servers\n# [[inputs.kibana]]\n#   ## Specify a list of one or more Kibana servers\n#   servers = [\"http://localhost:5601\"]\n#\n#   ## Timeout for HTTP requests\n#   timeout = \"5s\"\n#\n#   ## HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from the Kubernetes api\n# [[inputs.kube_inventory]]\n#   ## URL for the Kubernetes API\n#   url = \"https://127.0.0.1\"\n#\n#   ## Namespace to use. Set to \"\" to use all namespaces.\n#   # namespace = \"default\"\n#\n#   ## Use bearer token for authorization. ('bearer_token' takes priority)\n#   ## If both of these are empty, we'll use the default serviceaccount:\n#   ## at: /run/secrets/kubernetes.io/serviceaccount/token\n#   # bearer_token = \"/path/to/bearer/token\"\n#   ## OR\n#   # bearer_token_string = \"abc_123\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional Resources to exclude from gathering\n#   ## Leave them with blank with try to gather everything available.\n#   ## Values can be - \"daemonsets\", deployments\", \"endpoints\", \"ingress\", \"nodes\",\n#   ## \"persistentvolumes\", \"persistentvolumeclaims\", \"pods\", \"services\", \"statefulsets\"\n#   # resource_exclude = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n#\n#   ## Optional Resources to include when gathering\n#   ## Overrides resource_exclude if both set.\n#   # resource_include = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n#\n#   ## selectors to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all selectors as tags\n#   ## selector_exclude overrides selector_include if both set.\n#   # selector_include = []\n#   # selector_exclude = [\"*\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/path/to/cafile\"\n#   # tls_cert = \"/path/to/certfile\"\n#   # tls_key = \"/path/to/keyfile\"\n#   # tls_server_name = \"kubernetes.example.com\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from the kubernetes kubelet api\n# [[inputs.kubernetes]]\n#   ## URL for the kubelet\n#   url = \"http://127.0.0.1:10255\"\n#\n#   ## Use bearer token for authorization. ('bearer_token' takes priority)\n#   ## If both of these are empty, we'll use the default serviceaccount:\n#   ## at: /run/secrets/kubernetes.io/serviceaccount/token\n#   # bearer_token = \"/path/to/bearer/token\"\n#   ## OR\n#   # bearer_token_string = \"abc_123\"\n#\n#   ## Pod labels to be added as tags.  An empty array for both include and\n#   ## exclude will include all labels.\n#   # label_include = []\n#   # label_exclude = [\"*\"]\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from a LeoFS Server via SNMP\n# [[inputs.leofs]]\n#   ## An array of URLs of the form:\n#   ##   host [ \":\" port]\n#   servers = [\"127.0.0.1:4020\"]\n\n\n# # Provides Linux sysctl fs metrics\n# [[inputs.linux_sysctl_fs]]\n#   # no configuration\n\n\n# # Read metrics exposed by Logstash\n# [[inputs.logstash]]\n#   ## The URL of the exposed Logstash API endpoint.\n#   url = \"http://127.0.0.1:9600\"\n#\n#   ## Use Logstash 5 single pipeline API, set to true when monitoring\n#   ## Logstash 5.\n#   # single_pipeline = false\n#\n#   ## Enable optional collection components.  Can contain\n#   ## \"pipelines\", \"process\", and \"jvm\".\n#   # collect = [\"pipelines\", \"process\", \"jvm\"]\n#\n#   ## Timeout for HTTP requests.\n#   # timeout = \"5s\"\n#\n#   ## Optional HTTP Basic Auth credentials.\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Use TLS but skip chain \u0026 host verification.\n#   # insecure_skip_verify = false\n#\n#   ## Optional HTTP headers.\n#   # [inputs.logstash.headers]\n#   #   \"X-Special-Header\" = \"Special-Value\"\n\n\n# # Read metrics from local Lustre service on OST, MDS\n# [[inputs.lustre2]]\n#   ## An array of /proc globs to search for Lustre stats\n#   ## If not specified, the default will work on Lustre 2.5.x\n#   ##\n#   # ost_procfiles = [\n#   #   \"/proc/fs/lustre/obdfilter/*/stats\",\n#   #   \"/proc/fs/lustre/osd-ldiskfs/*/stats\",\n#   #   \"/proc/fs/lustre/obdfilter/*/job_stats\",\n#   # ]\n#   # mds_procfiles = [\n#   #   \"/proc/fs/lustre/mdt/*/md_stats\",\n#   #   \"/proc/fs/lustre/mdt/*/job_stats\",\n#   # ]\n\n\n# # Read metrics about LVM physical volumes, volume groups, logical volumes.\n# [[inputs.lvm]]\n# ## Use sudo to run LVM commands\n# use_sudo = false\n\n\n# # Gathers metrics from the /3.0/reports MailChimp API\n# [[inputs.mailchimp]]\n#   ## MailChimp API key\n#   ## get from https://admin.mailchimp.com/account/api/\n#   api_key = \"\" # required\n#   ## Reports for campaigns sent more than days_old ago will not be collected.\n#   ## 0 means collect all.\n#   days_old = 0\n#   ## Campaign ID to get, if empty gets all campaigns, this option overrides days_old\n#   # campaign_id = \"\"\n\n\n# # Retrieves information on a specific host in a MarkLogic Cluster\n# [[inputs.marklogic]]\n#   ## Base URL of the MarkLogic HTTP Server.\n#   url = \"http://localhost:8002\"\n#\n#   ## List of specific hostnames to retrieve information. At least (1) required.\n#   # hosts = [\"hostname1\", \"hostname2\"]\n#\n#   ## Using HTTP Basic Authentication. Management API requires 'manage-user' role privileges\n#   # username = \"myuser\"\n#   # password = \"mypassword\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from one or many mcrouter servers\n# [[inputs.mcrouter]]\n#   ## An array of address to gather stats about. Specify an ip or hostname\n#   ## with port. ie tcp://localhost:11211, tcp://10.0.0.1:11211, etc.\n# \tservers = [\"tcp://localhost:11211\", \"unix:///var/run/mcrouter.sock\"]\n#\n# \t## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n#   # timeout = \"5s\"\n\n\n# # Get md array statistics from /proc/mdstat\n# [[inputs.mdstat]]\n# \t## Sets file path\n# \t## If not specified, then default is /proc/mdstat\n# \t# file_name = \"/proc/mdstat\"\n\n\n# # Read metrics from one or many memcached servers\n# [[inputs.memcached]]\n#   ## An array of address to gather stats about. Specify an ip on hostname\n#   ## with optional port. ie localhost, 10.0.0.1:11211, etc.\n#   servers = [\"localhost:11211\"]\n#   # unix_sockets = [\"/var/run/memcached.sock\"]\n#\n#   ## Optional TLS Config\n#   # enable_tls = true\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Telegraf plugin for gathering metrics from N Mesos masters\n# [[inputs.mesos]]\n#   ## Timeout, in ms.\n#   timeout = 100\n#\n#   ## A list of Mesos masters.\n#   masters = [\"http://localhost:5050\"]\n#\n#   ## Master metrics groups to be collected, by default, all enabled.\n#   master_collections = [\n#     \"resources\",\n#     \"master\",\n#     \"system\",\n#     \"agents\",\n#     \"frameworks\",\n#     \"framework_offers\",\n#     \"tasks\",\n#     \"messages\",\n#     \"evqueue\",\n#     \"registrar\",\n#     \"allocator\",\n#   ]\n#\n#   ## A list of Mesos slaves, default is []\n#   # slaves = []\n#\n#   ## Slave metrics groups to be collected, by default, all enabled.\n#   # slave_collections = [\n#   #   \"resources\",\n#   #   \"agent\",\n#   #   \"system\",\n#   #   \"executors\",\n#   #   \"tasks\",\n#   #   \"messages\",\n#   # ]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Collects scores from a Minecraft server's scoreboard using the RCON protocol\n# [[inputs.minecraft]]\n#   ## Address of the Minecraft server.\n#   # server = \"localhost\"\n#\n#   ## Server RCON Port.\n#   # port = \"25575\"\n#\n#   ## Server RCON Password.\n#   password = \"\"\n#\n#   ## Uncomment to remove deprecated metric components.\n#   # tagdrop = [\"server\"]\n\n\n# # Generate metrics for test and demonstration purposes\n# [[inputs.mock]]\n#   ## Set the metric name to use for reporting\n#   metric_name = \"mock\"\n#\n#   ## Optional string key-value pairs of tags to add to all metrics\n#   # [inputs.mock.tags]\n#   # \"key\" = \"value\"\n#\n#   ## One or more mock data fields *must* be defined.\n#   ##\n#   ## [[inputs.mock.random]]\n#   ##   name = \"rand\"\n#   ##   min = 1.0\n#   ##   max = 6.0\n#   ## [[inputs.mock.sine_wave]]\n#   ##   name = \"wave\"\n#   ##   amplitude = 1.0\n#   ##   period = 0.5\n#   ## [[inputs.mock.step]]\n#   ##   name = \"plus_one\"\n#   ##   start = 0.0\n#   ##   step = 1.0\n#   ## [[inputs.mock.stock]]\n#   ##   name = \"abc\"\n#   ##   price = 50.00\n#   ##   volatility = 0.2\n\n\n# # Retrieve data from MODBUS slave devices\n# [[inputs.modbus]]\n#   ## Connection Configuration\n#   ##\n#   ## The plugin supports connections to PLCs via MODBUS/TCP, RTU over TCP, ASCII over TCP or\n#   ## via serial line communication in binary (RTU) or readable (ASCII) encoding\n#   ##\n#   ## Device name\n#   name = \"Device\"\n#\n#   ## Slave ID - addresses a MODBUS device on the bus\n#   ## Range: 0 - 255 [0 = broadcast; 248 - 255 = reserved]\n#   slave_id = 1\n#\n#   ## Timeout for each request\n#   timeout = \"1s\"\n#\n#   ## Maximum number of retries and the time to wait between retries\n#   ## when a slave-device is busy.\n#   # busy_retries = 0\n#   # busy_retries_wait = \"100ms\"\n#\n#   # TCP - connect via Modbus/TCP\n#   controller = \"tcp://localhost:502\"\n#\n#   ## Serial (RS485; RS232)\n#   # controller = \"file:///dev/ttyUSB0\"\n#   # baud_rate = 9600\n#   # data_bits = 8\n#   # parity = \"N\"\n#   # stop_bits = 1\n#\n#   ## Trace the connection to the modbus device as debug messages\n#   ## Note: You have to enable telegraf's debug mode to see those messages!\n#   # debug_connection = false\n#\n#   ## For Modbus over TCP you can choose between \"TCP\", \"RTUoverTCP\" and \"ASCIIoverTCP\"\n#   ## default behaviour is \"TCP\" if the controller is TCP\n#   ## For Serial you can choose between \"RTU\" and \"ASCII\"\n#   # transmission_mode = \"RTU\"\n#\n# \t## Define the configuration schema\n#   ##  |---register -- define fields per register type in the original style (only supports one slave ID)\n#   ##  |---request  -- define fields on a requests base\n#   configuration_type = \"register\"\n#\n#   ## Per register definition\n#   ##\n#\n#   ## Digital Variables, Discrete Inputs and Coils\n#   ## measurement - the (optional) measurement name, defaults to \"modbus\"\n#   ## name        - the variable name\n#   ## address     - variable address\n#\n#   discrete_inputs = [\n#     { name = \"start\",          address = [0]},\n#     { name = \"stop\",           address = [1]},\n#     { name = \"reset\",          address = [2]},\n#     { name = \"emergency_stop\", address = [3]},\n#   ]\n#   coils = [\n#     { name = \"motor1_run\",     address = [0]},\n#     { name = \"motor1_jog\",     address = [1]},\n#     { name = \"motor1_stop\",    address = [2]},\n#   ]\n#\n#   ## Analog Variables, Input Registers and Holding Registers\n#   ## measurement - the (optional) measurement name, defaults to \"modbus\"\n#   ## name        - the variable name\n#   ## byte_order  - the ordering of bytes\n#   ##  |---AB, ABCD   - Big Endian\n#   ##  |---BA, DCBA   - Little Endian\n#   ##  |---BADC       - Mid-Big Endian\n#   ##  |---CDAB       - Mid-Little Endian\n#   ## data_type  - INT16, UINT16, INT32, UINT32, INT64, UINT64,\n#   ##              FLOAT32-IEEE, FLOAT64-IEEE (the IEEE 754 binary representation)\n#   ##              FLOAT32, FIXED, UFIXED (fixed-point representation on input)\n#   ## scale      - the final numeric variable representation\n#   ## address    - variable address\n#\n#   holding_registers = [\n#     { name = \"power_factor\", byte_order = \"AB\",   data_type = \"FIXED\", scale=0.01,  address = [8]},\n#     { name = \"voltage\",      byte_order = \"AB\",   data_type = \"FIXED\", scale=0.1,   address = [0]},\n#     { name = \"energy\",       byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [5,6]},\n#     { name = \"current\",      byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [1,2]},\n#     { name = \"frequency\",    byte_order = \"AB\",   data_type = \"UFIXED\", scale=0.1,  address = [7]},\n#     { name = \"power\",        byte_order = \"ABCD\", data_type = \"UFIXED\", scale=0.1,  address = [3,4]},\n#   ]\n#   input_registers = [\n#     { name = \"tank_level\",   byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [0]},\n#     { name = \"tank_ph\",      byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [1]},\n#     { name = \"pump1_speed\",  byte_order = \"ABCD\", data_type = \"INT32\",   scale=1.0,     address = [3,4]},\n#   ]\n#\n#\n#   ## Per request definition\n#   ##\n#\n#   ## Define a request sent to the device\n#   ## Multiple of those requests can be defined. Data will be collated into metrics at the end of data collection.\n#   # [[inputs.modbus.request]]\n#     ## ID of the modbus slave device to query.\n#     ## If you need to query multiple slave-devices, create several \"request\" definitions.\n#     # slave_id = 0\n#\n#     ## Byte order of the data.\n#     ##  |---ABCD or MSW-BE -- Big Endian (Motorola)\n#     ##  |---DCBA or LSW-LE -- Little Endian (Intel)\n#     ##  |---BADC or MSW-LE -- Big Endian with byte swap\n#     ##  |---CDAB or LSW-BE -- Little Endian with byte swap\n#     # byte_order = \"ABCD\"\n#\n#     ## Type of the register for the request\n#     ## Can be \"coil\", \"discrete\", \"holding\" or \"input\"\n#     # register = \"holding\"\n#\n#     ## Name of the measurement.\n#     ## Can be overriden by the individual field definitions. Defaults to \"modbus\"\n#     # measurement = \"modbus\"\n#\n#     ## Field definitions\n#     ## Analog Variables, Input Registers and Holding Registers\n#     ## address        - address of the register to query. For coil and discrete inputs this is the bit address.\n#     ## name *1        - field name\n#     ## type *1,2      - type of the modbus field, can be INT16, UINT16, INT32, UINT32, INT64, UINT64 and\n#     ##                  FLOAT32, FLOAT64 (IEEE 754 binary representation)\n#     ## scale *1,2     - (optional) factor to scale the variable with\n#     ## output *1,2    - (optional) type of resulting field, can be INT64, UINT64 or FLOAT64. Defaults to FLOAT64 if\n#     ##                  \"scale\" is provided and to the input \"type\" class otherwise (i.e. INT* -\u003e INT64, etc).\n#     ## measurement *1 - (optional) measurement name, defaults to the setting of the request\n#     ## omit           - (optional) omit this field. Useful to leave out single values when querying many registers\n#     ##                  with a single request. Defaults to \"false\".\n#     ##\n#     ## *1: Those fields are ignored if field is omitted (\"omit\"=true)\n#     ##\n#     ## *2: Thise fields are ignored for both \"coil\" and \"discrete\"-input type of registers. For those register types\n#     ##     the fields are output as zero or one in UINT64 format by default.\n#\n#     ## Coil / discrete input example\n#     # fields = [\n#     #   { address=0, name=\"motor1_run\"},\n#     #   { address=1, name=\"jog\", measurement=\"motor\"},\n#     #   { address=2, name=\"motor1_stop\", omit=true},\n#     #   { address=3, name=\"motor1_overheating\"},\n#     # ]\n#\n#     ## Per-request tags\n#     ## These tags take precedence over predefined tags.\n#     # [[inputs.modbus.request.tags]]\n#     #\t  name = \"value\"\n#\n#     ## Holding / input example\n#     ## All of those examples will result in FLOAT64 field outputs\n#     # fields = [\n#     #   { address=0, name=\"voltage\",      type=\"INT16\",   scale=0.1   },\n#     #   { address=1, name=\"current\",      type=\"INT32\",   scale=0.001 },\n#     #   { address=3, name=\"power\",        type=\"UINT32\",  omit=true   },\n#     #   { address=5, name=\"energy\",       type=\"FLOAT32\", scale=0.001, measurement=\"W\" },\n#     #   { address=7, name=\"frequency\",    type=\"UINT32\",  scale=0.1   },\n#     #   { address=8, name=\"power_factor\", type=\"INT64\",   scale=0.01  },\n#     # ]\n#\n#     ## Holding / input example with type conversions\n#     # fields = [\n#     #   { address=0, name=\"rpm\",         type=\"INT16\"                   },  # will result in INT64 field\n#     #   { address=1, name=\"temperature\", type=\"INT16\", scale=0.1        },  # will result in FLOAT64 field\n#     #   { address=2, name=\"force\",       type=\"INT32\", output=\"FLOAT64\" },  # will result in FLOAT64 field\n#     #   { address=4, name=\"hours\",       type=\"UINT32\"                  },  # will result in UIN64 field\n#     # ]\n#\n#     ## Per-request tags\n# \t\t## These tags take precedence over predefined tags.\n#     # [[inputs.modbus.request.tags]]\n#     #\t  name = \"value\"\n#\n#\n#\n#   ## Enable workarounds required by some devices to work correctly\n#   # [inputs.modbus.workarounds]\n#     ## Pause between read requests sent to the device. This might be necessary for (slow) serial devices.\n#     # pause_between_requests = \"0ms\"\n#     ## Close the connection after every gather cycle. Usually the plugin closes the connection after a certain\n#     ## idle-timeout, however, if you query a device with limited simultaneous connectivity (e.g. serial devices)\n#     ## from multiple instances you might want to only stay connected during gather and disconnect afterwards.\n#     # close_connection_after_gather = false\n\n\n# # Read metrics from one or many MongoDB servers\n# [[inputs.mongodb]]\n#   ## An array of URLs of the form:\n#   ##   \"mongodb://\" [user \":\" pass \"@\"] host [ \":\" port]\n#   ## For example:\n#   ##   mongodb://user:auth_key@10.10.3.30:27017,\n#   ##   mongodb://10.10.3.33:18832,\n#   servers = [\"mongodb://127.0.0.1:27017?connect=direct\"]\n#\n#   ## When true, collect cluster status\n#   ## Note that the query that counts jumbo chunks triggers a COLLSCAN, which\n#   ## may have an impact on performance.\n#   # gather_cluster_status = true\n#\n#   ## When true, collect per database stats\n#   # gather_perdb_stats = false\n#\n#   ## When true, collect per collection stats\n#   # gather_col_stats = false\n#\n#   ## When true, collect usage statistics for each collection\n#   ## (insert, update, queries, remove, getmore, commands etc...).\n#   # gather_top_stat = false\n#\n#   ## List of db where collections stats are collected\n#   ## If empty, all db are concerned\n#   # col_stats_dbs = [\"local\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics and status information about processes managed by Monit\n# [[inputs.monit]]\n#   ## Monit HTTPD address\n#   address = \"http://127.0.0.1:2812\"\n#\n#   ## Username and Password for Monit\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Aggregates the contents of multiple files into a single point\n# [[inputs.multifile]]\n#   ## Base directory where telegraf will look for files.\n#   ## Omit this option to use absolute paths.\n#   base_dir = \"/sys/bus/i2c/devices/1-0076/iio:device0\"\n#\n#   ## If true, Telegraf discard all data when a single file can't be read.\n#   ## Else, Telegraf omits the field generated from this file.\n#   # fail_early = true\n#\n#   ## Files to parse each interval.\n#   [[inputs.multifile.file]]\n#     file = \"in_pressure_input\"\n#     dest = \"pressure\"\n#     conversion = \"float\"\n#   [[inputs.multifile.file]]\n#     file = \"in_temp_input\"\n#     dest = \"temperature\"\n#     conversion = \"float(3)\"\n#   [[inputs.multifile.file]]\n#     file = \"in_humidityrelative_input\"\n#     dest = \"humidityrelative\"\n#     conversion = \"float(3)\"\n\n\n# # Read metrics from one or many mysql servers\n# [[inputs.mysql]]\n#   ## specify servers via a url matching:\n#   ##  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify|custom]]\n#   ##  see https://github.com/go-sql-driver/mysql#dsn-data-source-name\n#   ##  e.g.\n#   ##    servers = [\"user:passwd@tcp(127.0.0.1:3306)/?tls=false\"]\n#   ##    servers = [\"user@tcp(127.0.0.1:3306)/?tls=false\"]\n#   #\n#   ## If no servers are specified, then localhost is used as the host.\n#   servers = [\"tcp(127.0.0.1:3306)/\"]\n#\n#   ## Selects the metric output format.\n#   ##\n#   ## This option exists to maintain backwards compatibility, if you have\n#   ## existing metrics do not set or change this value until you are ready to\n#   ## migrate to the new format.\n#   ##\n#   ## If you do not have existing metrics from this plugin set to the latest\n#   ## version.\n#   ##\n#   ## Telegraf \u003e=1.6: metric_version = 2\n#   ##           \u003c1.6: metric_version = 1 (or unset)\n#   metric_version = 2\n#\n#   ## if the list is empty, then metrics are gathered from all database tables\n#   # table_schema_databases = []\n#\n#   ## gather metrics from INFORMATION_SCHEMA.TABLES for databases provided above list\n#   # gather_table_schema = false\n#\n#   ## gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST\n#   # gather_process_list = false\n#\n#   ## gather user statistics from INFORMATION_SCHEMA.USER_STATISTICS\n#   # gather_user_statistics = false\n#\n#   ## gather auto_increment columns and max values from information schema\n#   # gather_info_schema_auto_inc = false\n#\n#   ## gather metrics from INFORMATION_SCHEMA.INNODB_METRICS\n#   # gather_innodb_metrics = false\n#\n#   ## gather metrics from SHOW SLAVE STATUS command output\n#   # gather_slave_status = false\n#\n#   ## gather metrics from all channels from SHOW SLAVE STATUS command output\n#   # gather_all_slave_channels = false\n#\n#   ## use MariaDB dialect for all channels SHOW SLAVE STATUS\n#   # mariadb_dialect = false\n#\n#   ## gather metrics from SHOW BINARY LOGS command output\n#   # gather_binary_logs = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.GLOBAL_VARIABLES\n#   # gather_global_variables = true\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE\n#   # gather_table_io_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS\n#   # gather_table_lock_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE\n#   # gather_index_io_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS\n#   # gather_event_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME\n#   # gather_file_events_stats = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST\n#   # gather_perf_events_statements = false\n#\n#   ## the limits for metrics form perf_events_statements\n#   # perf_events_statements_digest_text_limit = 120\n#   # perf_events_statements_limit = 250\n#   # perf_events_statements_time_limit = 86400\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_ACCOUNT_BY_EVENT_NAME\n#   # gather_perf_sum_per_acc_per_event         = false\n#\n#   ## list of events to be gathered for gather_perf_sum_per_acc_per_event\n#   ## in case of empty list all events will be gathered\n#   # perf_summary_events                       = []\n#\n#   ## Some queries we may want to run less often (such as SHOW GLOBAL VARIABLES)\n#   ##   example: interval_slow = \"30m\"\n#   # interval_slow = \"\"\n#\n#   ## Optional TLS Config (will be used if tls=custom parameter specified in server uri)\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Provides metrics about the state of a NATS server\n# [[inputs.nats]]\n#   ## The address of the monitoring endpoint of the NATS server\n#   server = \"http://localhost:8222\"\n#\n#   ## Maximum time to receive response\n#   # response_timeout = \"5s\"\n\n\n# # Neptune Apex data collector\n# [[inputs.neptune_apex]]\n#   ## The Neptune Apex plugin reads the publicly available status.xml data from a local Apex.\n#   ## Measurements will be logged under \"apex\".\n#\n#   ## The base URL of the local Apex(es). If you specify more than one server, they will\n#   ## be differentiated by the \"source\" tag.\n#   servers = [\n#     \"http://apex.local\",\n#   ]\n#\n#   ## The response_timeout specifies how long to wait for a reply from the Apex.\n#   #response_timeout = \"5s\"\n\n\n# # Read metrics about network interface usage\n# [[inputs.net]]\n#   ## By default, telegraf gathers stats from any up interface (excluding loopback)\n#   ## Setting interfaces will tell it to gather these explicit interfaces,\n#   ## regardless of status.\n#   ##\n#   # interfaces = [\"eth0\"]\n#   ##\n#   ## On linux systems telegraf also collects protocol stats.\n#   ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.\n#   ##\n#   # ignore_protocol_stats = false\n#   ##\n\n\n# # Collect response time of a TCP or UDP connection\n# [[inputs.net_response]]\n#   ## Protocol, must be \"tcp\" or \"udp\"\n#   ## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n#   ## a send/expect string pair (see below).\n#   protocol = \"tcp\"\n#   ## Server address (default localhost)\n#   address = \"localhost:80\"\n#\n#   ## Set timeout\n#   # timeout = \"1s\"\n#\n#   ## Set read timeout (only used if expecting a response)\n#   # read_timeout = \"1s\"\n#\n#   ## The following options are required for UDP checks. For TCP, they are\n#   ## optional. The plugin will send the given string to the server and then\n#   ## expect to receive the given 'expect' string back.\n#   ## string sent to the server\n#   # send = \"ssh\"\n#   ## expected string in answer\n#   # expect = \"ssh\"\n#\n#   ## Uncomment to remove deprecated fields\n#   # fielddrop = [\"result_type\", \"string_found\"]\n\n\n# # Read TCP metrics such as established, time wait and sockets counts.\n# [[inputs.netstat]]\n#   # no configuration\n\n\n# # Read per-mount NFS client metrics from /proc/self/mountstats\n# [[inputs.nfsclient]]\n#   ## Read more low-level metrics (optional, defaults to false)\n#   # fullstat = false\n#\n#   ## List of mounts to explictly include or exclude (optional)\n#   ## The pattern (Go regexp) is matched against the mount point (not the\n#   ## device being mounted).  If include_mounts is set, all mounts are ignored\n#   ## unless present in the list. If a mount is listed in both include_mounts\n#   ## and exclude_mounts, it is excluded.  Go regexp patterns can be used.\n#   # include_mounts = []\n#   # exclude_mounts = []\n#\n#   ## List of operations to include or exclude from collecting.  This applies\n#   ## only when fullstat=true.  Symantics are similar to {include,exclude}_mounts:\n#   ## the default is to collect everything; when include_operations is set, only\n#   ## those OPs are collected; when exclude_operations is set, all are collected\n#   ## except those listed.  If include and exclude are set, the OP is excluded.\n#   ## See /proc/self/mountstats for a list of valid operations; note that\n#   ## NFSv3 and NFSv4 have different lists.  While it is not possible to\n#   ## have different include/exclude lists for NFSv3/4, unused elements\n#   ## in the list should be okay.  It is possible to have different lists\n#   ## for different mountpoints:  use mulitple [[input.nfsclient]] stanzas,\n#   ## with their own lists.  See \"include_mounts\" above, and be careful of\n#   ## duplicate metrics.\n#   # include_operations = []\n#   # exclude_operations = []\n\n\n# # Read Nginx's basic status information (ngx_http_stub_status_module)\n# [[inputs.nginx]]\n#   # An array of Nginx stub_status URI to gather stats.\n#   urls = [\"http://localhost/server_status\"]\n#\n#   ## Optional TLS Config\n#   tls_ca = \"/etc/telegraf/ca.pem\"\n#   tls_cert = \"/etc/telegraf/cert.cer\"\n#   tls_key = \"/etc/telegraf/key.key\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   insecure_skip_verify = false\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n\n\n# # Read Nginx Plus' full status information (ngx_http_status_module)\n# [[inputs.nginx_plus]]\n#   ## An array of ngx_http_status_module or status URI to gather stats.\n#   urls = [\"http://localhost/status\"]\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read Nginx Plus Api documentation\n# [[inputs.nginx_plus_api]]\n#   ## An array of API URI to gather stats.\n#   urls = [\"http://localhost/api\"]\n#\n#   # Nginx API version, default: 3\n#   # api_version = 3\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read Nginx virtual host traffic status module information (nginx-module-sts)\n# [[inputs.nginx_sts]]\n#   ## An array of ngx_http_status_module or status URI to gather stats.\n#   urls = [\"http://localhost/status\"]\n#\n#   ## HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read nginx_upstream_check module status information (https://github.com/yaoweibin/nginx_upstream_check_module)\n# [[inputs.nginx_upstream_check]]\n#   ## An URL where Nginx Upstream check module is enabled\n#   ## It should be set to return a JSON formatted response\n#   url = \"http://127.0.0.1/status?format=json\"\n#\n#   ## HTTP method\n#   # method = \"GET\"\n#\n#   ## Optional HTTP headers\n#   # headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## Override HTTP \"Host\" header\n#   # host_header = \"check.example.com\"\n#\n#   ## Timeout for HTTP requests\n#   timeout = \"5s\"\n#\n#   ## Optional HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read Nginx virtual host traffic status module information (nginx-module-vts)\n# [[inputs.nginx_vts]]\n#   ## An array of ngx_http_status_module or status URI to gather stats.\n#   urls = [\"http://localhost/status\"]\n#\n#   ## HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from the Nomad API\n# [[inputs.nomad]]\n#   ## URL for the Nomad agent\n#   # url = \"http://127.0.0.1:4646\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n\n\n# # A plugin to collect stats from the NSD authoritative DNS name server\n# [[inputs.nsd]]\n#   ## Address of server to connect to, optionally ':port'. Defaults to the\n#   ## address in the nsd config file.\n#   server = \"127.0.0.1:8953\"\n#\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   # use_sudo = false\n#\n#   ## The default location of the nsd-control binary can be overridden with:\n#   # binary = \"/usr/sbin/nsd-control\"\n#\n#   ## The default location of the nsd config file can be overridden with:\n#   # config_file = \"/etc/nsd/nsd.conf\"\n#\n#   ## The default timeout of 1s can be overridden with:\n#   # timeout = \"1s\"\n\n\n# # Read NSQ topic and channel statistics.\n# [[inputs.nsq]]\n#   ## An array of NSQD HTTP API endpoints\n#   endpoints  = [\"http://localhost:4151\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Collect kernel snmp counters and network interface statistics\n# [[inputs.nstat]]\n#   ## file paths for proc files. If empty default paths will be used:\n#   ##    /proc/net/netstat, /proc/net/snmp, /proc/net/snmp6\n#   ## These can also be overridden with env variables, see README.\n#   proc_net_netstat = \"/proc/net/netstat\"\n#   proc_net_snmp = \"/proc/net/snmp\"\n#   proc_net_snmp6 = \"/proc/net/snmp6\"\n#   ## dump metrics with 0 values too\n#   dump_zeros       = true\n\n\n# # Get standard NTP query metrics, requires ntpq executable.\n# [[inputs.ntpq]]\n#   ## If false, set the -n ntpq flag. Can reduce metric gather time.\n#   dns_lookup = true\n\n\n# # Pulls statistics from nvidia GPUs attached to the host\n# [[inputs.nvidia_smi]]\n#   ## Optional: path to nvidia-smi binary, defaults \"/usr/bin/nvidia-smi\"\n#   ## We will first try to locate the nvidia-smi binary with the explicitly specified value (or default value),\n#   ## if it is not found, we will try to locate it on PATH(exec.LookPath), if it is still not found, an error will be returned\n#   # bin_path = \"/usr/bin/nvidia-smi\"\n#\n#   ## Optional: timeout for GPU polling\n#   # timeout = \"5s\"\n\n\n# # Retrieve data from OPCUA devices\n# [[inputs.opcua]]\n#   ## Metric name\n#   # name = \"opcua\"\n#   #\n#   ## OPC UA Endpoint URL\n#   # endpoint = \"opc.tcp://localhost:4840\"\n#   #\n#   ## Maximum time allowed to establish a connect to the endpoint.\n#   # connect_timeout = \"10s\"\n#   #\n#   ## Maximum time allowed for a request over the estabilished connection.\n#   # request_timeout = \"5s\"\n#   #\n#   ## Security policy, one of \"None\", \"Basic128Rsa15\", \"Basic256\",\n#   ## \"Basic256Sha256\", or \"auto\"\n#   # security_policy = \"auto\"\n#   #\n#   ## Security mode, one of \"None\", \"Sign\", \"SignAndEncrypt\", or \"auto\"\n#   # security_mode = \"auto\"\n#   #\n#   ## Path to cert.pem. Required when security mode or policy isn't \"None\".\n#   ## If cert path is not supplied, self-signed cert and key will be generated.\n#   # certificate = \"/etc/telegraf/cert.pem\"\n#   #\n#   ## Path to private key.pem. Required when security mode or policy isn't \"None\".\n#   ## If key path is not supplied, self-signed cert and key will be generated.\n#   # private_key = \"/etc/telegraf/key.pem\"\n#   #\n#   ## Authentication Method, one of \"Certificate\", \"UserName\", or \"Anonymous\".  To\n#   ## authenticate using a specific ID, select 'Certificate' or 'UserName'\n#   # auth_method = \"Anonymous\"\n#   #\n#   ## Username. Required for auth_method = \"UserName\"\n#   # username = \"\"\n#   #\n#   ## Password. Required for auth_method = \"UserName\"\n#   # password = \"\"\n#   #\n#   ## Option to select the metric timestamp to use. Valid options are:\n#   ##     \"gather\" -- uses the time of receiving the data in telegraf\n#   ##     \"server\" -- uses the timestamp provided by the server\n#   ##     \"source\" -- uses the timestamp provided by the source\n#   # timestamp = \"gather\"\n#   #\n#   ## Node ID configuration\n#   ## name              - field name to use in the output\n#   ## namespace         - OPC UA namespace of the node (integer value 0 thru 3)\n#   ## identifier_type   - OPC UA ID type (s=string, i=numeric, g=guid, b=opaque)\n#   ## identifier        - OPC UA ID (tag as shown in opcua browser)\n#   ## Example:\n#   ## {name=\"ProductUri\", namespace=\"0\", identifier_type=\"i\", identifier=\"2262\"}\n#   # nodes = [\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #]\n#   #\n#   ## Node Group\n#   ## Sets defaults for OPC UA namespace and ID type so they aren't required in\n#   ## every node.  A group can also have a metric name that overrides the main\n#   ## plugin metric name.\n#   ##\n#   ## Multiple node groups are allowed\n#   #[[inputs.opcua.group]]\n#   ## Group Metric name. Overrides the top level name.  If unset, the\n#   ## top level name is used.\n#   # name =\n#   #\n#   ## Group default namespace. If a node in the group doesn't set its\n#   ## namespace, this is used.\n#   # namespace =\n#   #\n#   ## Group default identifier type. If a node in the group doesn't set its\n#   ## namespace, this is used.\n#   # identifier_type =\n#   #\n#   ## Node ID Configuration.  Array of nodes with the same settings as above.\n#   # nodes = [\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #]\n#\n#   ## Enable workarounds required by some devices to work correctly\n#   # [inputs.opcua.workarounds]\n#     ## Set additional valid status codes, StatusOK (0x0) is always considered valid\n#     # additional_valid_status_codes = [\"0xC0\"]\n\n\n# # OpenLDAP cn=Monitor plugin\n# [[inputs.openldap]]\n#   host = \"localhost\"\n#   port = 389\n#\n#   # ldaps, starttls, or no encryption. default is an empty string, disabling all encryption.\n#   # note that port will likely need to be changed to 636 for ldaps\n#   # valid options: \"\" | \"starttls\" | \"ldaps\"\n#   tls = \"\"\n#\n#   # skip peer certificate verification. Default is false.\n#   insecure_skip_verify = false\n#\n#   # Path to PEM-encoded Root certificate to use to verify server certificate\n#   tls_ca = \"/etc/ssl/certs.pem\"\n#\n#   # dn/password to bind with. If bind_dn is empty, an anonymous bind is performed.\n#   bind_dn = \"\"\n#   bind_password = \"\"\n#\n#   # Reverse metric names so they sort more naturally. Recommended.\n#   # This defaults to false if unset, but is set to true when generating a new config\n#   reverse_metric_names = true\n\n\n# # Get standard NTP query metrics from OpenNTPD.\n# [[inputs.openntpd]]\n#   ## Run ntpctl binary with sudo.\n#   # use_sudo = false\n#\n#   ## Location of the ntpctl binary.\n#   # binary = \"/usr/sbin/ntpctl\"\n#\n#   ## Maximum time the ntpctl binary is allowed to run.\n#   # timeout = \"5ms\"\n\n\n# # A plugin to collect stats from Opensmtpd - a validating, recursive, and caching DNS resolver \n# [[inputs.opensmtpd]]\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   #use_sudo = false\n#\n#   ## The default location of the smtpctl binary can be overridden with:\n#   binary = \"/usr/sbin/smtpctl\"\n#\n#   ## The default timeout of 1000ms can be overridden with (in milliseconds):\n#   timeout = 1000\n\n\n# # Collects performance metrics from OpenStack services\n# [[inputs.openstack]]\n#   ## The recommended interval to poll is '30m'\n#\n#   ## The identity endpoint to authenticate against and get the service catalog from.\n#   authentication_endpoint = \"https://my.openstack.cloud:5000\"\n#\n#   ## The domain to authenticate against when using a V3 identity endpoint.\n#   # domain = \"default\"\n#\n#   ## The project to authenticate as.\n#   # project = \"admin\"\n#\n#   ## User authentication credentials. Must have admin rights.\n#   username = \"admin\"\n#   password = \"password\"\n#\n#   ## Available services are:\n#   ## \"agents\", \"aggregates\", \"flavors\", \"hypervisors\", \"networks\", \"nova_services\",\n#   ## \"ports\", \"projects\", \"servers\", \"services\", \"stacks\", \"storage_pools\", \"subnets\", \"volumes\"\n#   # enabled_services = [\"services\", \"projects\", \"hypervisors\", \"flavors\", \"networks\", \"volumes\"]\n#\n#   ## Collect Server Diagnostics\n#   # server_diagnotics = false\n#\n#   ## output secrets (such as adminPass(for server) and UserID(for volume)).\n#   # output_secrets = false\n#\n#   ## Amount of time allowed to complete the HTTP(s) request.\n#   # timeout = \"5s\"\n#\n#   ## HTTP Proxy support\n#   # http_proxy_url = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Options for tags received from Openstack\n#   # tag_prefix = \"openstack_tag_\"\n#   # tag_value = \"true\"\n#\n#   ## Timestamp format for timestamp data recieved from Openstack.\n#   ## If false format is unix nanoseconds.\n#   # human_readable_timestamps = false\n#\n#   ## Measure Openstack call duration\n#   # measure_openstack_requests = false\n\n\n# # Read current weather and forecasts data from openweathermap.org\n# [[inputs.openweathermap]]\n#   ## OpenWeatherMap API key.\n#   app_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n#\n#   ## City ID's to collect weather data from.\n#   city_id = [\"5391959\"]\n#\n#   ## Language of the description field. Can be one of \"ar\", \"bg\",\n#   ## \"ca\", \"cz\", \"de\", \"el\", \"en\", \"fa\", \"fi\", \"fr\", \"gl\", \"hr\", \"hu\",\n#   ## \"it\", \"ja\", \"kr\", \"la\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\",\n#   ## \"se\", \"sk\", \"sl\", \"es\", \"tr\", \"ua\", \"vi\", \"zh_cn\", \"zh_tw\"\n#   # lang = \"en\"\n#\n#   ## APIs to fetch; can contain \"weather\" or \"forecast\".\n#   fetch = [\"weather\", \"forecast\"]\n#\n#   ## OpenWeatherMap base URL\n#   # base_url = \"https://api.openweathermap.org/\"\n#\n#   ## Timeout for HTTP response.\n#   # response_timeout = \"5s\"\n#\n#   ## Preferred unit system for temperature and wind speed. Can be one of\n#   ## \"metric\", \"imperial\", or \"standard\".\n#   # units = \"metric\"\n#\n#   ## Query interval; OpenWeatherMap updates their weather data every 10\n#   ## minutes.\n#   interval = \"10m\"\n\n\n# # Read metrics of passenger using passenger-status\n# [[inputs.passenger]]\n#   ## Path of passenger-status.\n#   ##\n#   ## Plugin gather metric via parsing XML output of passenger-status\n#   ## More information about the tool:\n#   ##   https://www.phusionpassenger.com/library/admin/apache/overall_status_report.html\n#   ##\n#   ## If no path is specified, then the plugin simply execute passenger-status\n#   ## hopefully it can be found in your PATH\n#   command = \"passenger-status -v --show=xml\"\n\n\n# # Gather counters from PF\n# [[inputs.pf]]\n#   ## PF require root access on most systems.\n#   ## Setting 'use_sudo' to true will make use of sudo to run pfctl.\n#   ## Users must configure sudo to allow telegraf user to run pfctl with no password.\n#   ## pfctl can be restricted to only list command \"pfctl -s info\".\n#   use_sudo = false\n\n\n# # Read metrics of phpfpm, via HTTP status page or socket\n# [[inputs.phpfpm]]\n#   ## An array of addresses to gather stats about. Specify an ip or hostname\n#   ## with optional port and path\n#   ##\n#   ## Plugin can be configured in three modes (either can be used):\n#   ##   - http: the URL must start with http:// or https://, ie:\n#   ##       \"http://localhost/status\"\n#   ##       \"http://192.168.130.1/status?full\"\n#   ##\n#   ##   - unixsocket: path to fpm socket, ie:\n#   ##       \"/var/run/php5-fpm.sock\"\n#   ##      or using a custom fpm status path:\n#   ##       \"/var/run/php5-fpm.sock:fpm-custom-status-path\"\n#   ##\n#   ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n#   ##       \"fcgi://10.0.0.12:9000/status\"\n#   ##       \"cgi://10.0.10.12:9001/status\"\n#   ##\n#   ## Example of multiple gathering from local socket and remote host\n#   ## urls = [\"http://192.168.1.20/status\", \"/tmp/fpm.sock\"]\n#   urls = [\"http://localhost/status\"]\n#\n#   ## Duration allowed to complete HTTP requests.\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Ping given url(s) and return statistics\n# [[inputs.ping]]\n#   ## Hosts to send ping packets to.\n#   urls = [\"example.org\"]\n#\n#   ## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n#   ## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n#   ## the plugin will send pings directly.\n#   ##\n#   ## While the default is \"exec\" for backwards compatibility, new deployments\n#   ## are encouraged to use the \"native\" method for improved compatibility and\n#   ## performance.\n#   # method = \"exec\"\n#\n#   ## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n#   ## option of the ping command.\n#   # count = 1\n#\n#   ## Time to wait between sending ping packets in seconds.  Operates like the\n#   ## \"-i\" option of the ping command.\n#   # ping_interval = 1.0\n#\n#   ## If set, the time to wait for a ping response in seconds.  Operates like\n#   ## the \"-W\" option of the ping command.\n#   # timeout = 1.0\n#\n#   ## If set, the total ping deadline, in seconds.  Operates like the -w option\n#   ## of the ping command.\n#   # deadline = 10\n#\n#   ## Interface or source address to send ping from.  Operates like the -I or -S\n#   ## option of the ping command.\n#   # interface = \"\"\n#\n#   ## Percentiles to calculate. This only works with the native method.\n#   # percentiles = [50, 95, 99]\n#\n#   ## Specify the ping executable binary.\n#   # binary = \"ping\"\n#\n#   ## Arguments for ping command. When arguments is not empty, the command from\n#   ## the binary option will be used and other options (ping_interval, timeout,\n#   ## etc) will be ignored.\n#   # arguments = [\"-c\", \"3\"]\n#\n#   ## Use only IPv6 addresses when resolving a hostname.\n#   # ipv6 = false\n#\n#   ## Number of data bytes to be sent. Corresponds to the \"-s\"\n#   ## option of the ping command. This only works with the native method.\n#   # size = 56\n\n\n# # Measure postfix queue statistics\n# [[inputs.postfix]]\n#   ## Postfix queue directory. If not provided, telegraf will try to use\n#   ## 'postconf -h queue_directory' to determine it.\n#   # queue_directory = \"/var/spool/postfix\"\n\n\n# # Read metrics from one or many PowerDNS servers\n# [[inputs.powerdns]]\n#   ## An array of sockets to gather stats about.\n#   ## Specify a path to unix socket.\n#   unix_sockets = [\"/var/run/pdns.controlsocket\"]\n\n\n# # Read metrics from one or many PowerDNS Recursor servers\n# [[inputs.powerdns_recursor]]\n#   ## Path to the Recursor control socket.\n#   unix_sockets = [\"/var/run/pdns_recursor.controlsocket\"]\n#\n#   ## Directory to create receive socket.  This default is likely not writable,\n#   ## please reference the full plugin documentation for a recommended setup.\n#   # socket_dir = \"/var/run/\"\n#   ## Socket permissions for the receive socket.\n#   # socket_mode = \"0666\"\n\n\n# # Monitor process cpu and memory usage\n# [[inputs.procstat]]\n#   ## PID file to monitor process\n#   pid_file = \"/var/run/nginx.pid\"\n#   ## executable name (ie, pgrep \u003cexe\u003e)\n#   # exe = \"nginx\"\n#   ## pattern as argument for pgrep (ie, pgrep -f \u003cpattern\u003e)\n#   # pattern = \"nginx\"\n#   ## user as argument for pgrep (ie, pgrep -u \u003cuser\u003e)\n#   # user = \"nginx\"\n#   ## Systemd unit name, supports globs when include_systemd_children is set to true\n#   # systemd_unit = \"nginx.service\"\n#   # include_systemd_children = false\n#   ## CGroup name or path, supports globs\n#   # cgroup = \"systemd/system.slice/nginx.service\"\n#\n#   ## Windows service name\n#   # win_service = \"\"\n#\n#   ## override for process_name\n#   ## This is optional; default is sourced from /proc/\u003cpid\u003e/status\n#   # process_name = \"bar\"\n#\n#   ## Field name prefix\n#   # prefix = \"\"\n#\n#   ## When true add the full cmdline as a tag.\n#   # cmdline_tag = false\n#\n#   ## Mode to use when calculating CPU usage. Can be one of 'solaris' or 'irix'.\n#   # mode = \"irix\"\n#\n#   ## Add the PID as a tag instead of as a field.  When collecting multiple\n#   ## processes with otherwise matching tags this setting should be enabled to\n#   ## ensure each process has a unique identity.\n#   ##\n#   ## Enabling this option may result in a large number of series, especially\n#   ## when processes have a short lifetime.\n#   # pid_tag = false\n#\n#   ## Method to use when finding process IDs.  Can be one of 'pgrep', or\n#   ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while\n#   ## the native finder performs the search directly in a manor dependent on the\n#   ## platform.  Default is 'pgrep'\n#   # pid_finder = \"pgrep\"\n\n\n# # Provides metrics from Proxmox nodes (Proxmox Virtual Environment \u003e 6.2).\n# [[inputs.proxmox]]\n#   ## API connection configuration. The API token was introduced in Proxmox v6.2. Required permissions for user and token: PVEAuditor role on /.\n#   base_url = \"https://localhost:8006/api2/json\"\n#   api_token = \"USER@REALM!TOKENID=UUID\"\n#   ## Node name, defaults to OS hostname\n#   # node_name = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   insecure_skip_verify = false\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n\n\n# # Reads last_run_summary.yaml file and converts to measurements\n# [[inputs.puppetagent]]\n#   ## Location of puppet last run summary file\n#   location = \"/var/lib/puppet/state/last_run_summary.yaml\"\n\n\n# # Reads metrics from RabbitMQ servers via the Management Plugin\n# [[inputs.rabbitmq]]\n#   ## Management Plugin url. (default: http://localhost:15672)\n#   # url = \"http://localhost:15672\"\n#   ## Credentials\n#   # username = \"guest\"\n#   # password = \"guest\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional request timeouts\n#   ##\n#   ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n#   ## for a server's response headers after fully writing the request.\n#   # header_timeout = \"3s\"\n#   ##\n#   ## client_timeout specifies a time limit for requests made by this client.\n#   ## Includes connection time, any redirects, and reading the response body.\n#   # client_timeout = \"4s\"\n#\n#   ## A list of nodes to gather as the rabbitmq_node measurement. If not\n#   ## specified, metrics for all nodes are gathered.\n#   # nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n#\n#   ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n#   ## specified, metrics for all exchanges are gathered.\n#   # exchanges = [\"telegraf\"]\n#\n#   ## Metrics to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all metrics\n#   ## Currently the following metrics are supported: \"exchange\", \"federation\", \"node\", \"overview\", \"queue\"\n#   # metric_include = []\n#   # metric_exclude = []\n#\n#   ## Queues to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all queues\n#   queue_name_include = []\n#   queue_name_exclude = []\n#\n#   ## Federation upstreams include and exclude when gathering the rabbitmq_federation measurement.\n#   ## If neither are specified, metrics for all federation upstreams are gathered.\n#   ## Federation link metrics will only be gathered for queues and exchanges\n#   ## whose non-federation metrics will be collected (e.g a queue excluded\n#   ## by the 'queue_name_exclude' option will also be excluded from federation).\n#   ## Globs accepted.\n#   # federation_upstream_include = [\"dataCentre-*\"]\n#   # federation_upstream_exclude = []\n\n\n# # Read raindrops stats (raindrops - real-time stats for preforking Rack servers)\n# [[inputs.raindrops]]\n#   ## An array of raindrops middleware URI to gather stats.\n#   urls = [\"http://localhost:8080/_raindrops\"]\n\n\n# # Reads metrics from RavenDB servers via the Monitoring Endpoints\n# [[inputs.ravendb]]\n#   ## Node URL and port that RavenDB is listening on. By default,\n#   ## attempts to connect securely over HTTPS, however, if the user\n#   ## is running a local unsecure development cluster users can use\n#   ## HTTP via a URL like \"http://localhost:8080\"\n#   url = \"https://localhost:4433\"\n#\n#   ## RavenDB X509 client certificate setup\n#   # tls_cert = \"/etc/telegraf/raven.crt\"\n#   # tls_key = \"/etc/telegraf/raven.key\"\n#\n#   ## Optional request timeout\n#   ##\n#   ## Timeout, specifies the amount of time to wait\n#   ## for a server's response headers after fully writing the request and\n#   ## time limit for requests made by this client\n#   # timeout = \"5s\"\n#\n#   ## List of statistics which are collected\n#   # At least one is required\n#   # Allowed values: server, databases, indexes, collections\n#   #\n#   # stats_include = [\"server\", \"databases\", \"indexes\", \"collections\"]\n#\n#   ## List of db where database stats are collected\n#   ## If empty, all db are concerned\n#   # db_stats_dbs = []\n#\n#   ## List of db where index status are collected\n#   ## If empty, all indexes from all db are concerned\n#   # index_stats_dbs = []\n#\n#   ## List of db where collection status are collected\n#   ## If empty, all collections from all db are concerned\n#   # collection_stats_dbs = []\n\n\n# # Read CPU, Fans, Powersupply and Voltage metrics of hardware server through redfish APIs\n# [[inputs.redfish]]\n#   ## Server url\n#   address = \"https://127.0.0.1:5000\"\n#\n#   ## Username, Password for hardware server\n#   username = \"root\"\n#   password = \"password123456\"\n#\n#   ## ComputerSystemId\n#   computer_system_id=\"2M220100SL\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from one or many redis servers\n# [[inputs.redis]]\n#   ## specify servers via a url matching:\n#   ##  [protocol://][:password]@address[:port]\n#   ##  e.g.\n#   ##    tcp://localhost:6379\n#   ##    tcp://:password@192.168.99.100\n#   ##    unix:///var/run/redis.sock\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no port is specified, 6379 is used\n#   servers = [\"tcp://localhost:6379\"]\n#\n#   ## Optional. Specify redis commands to retrieve values\n#   # [[inputs.redis.commands]]\n#   #   # The command to run where each argument is a separate element\n#   #   command = [\"get\", \"sample-key\"]\n#   #   # The field to store the result in\n#   #   field = \"sample-key-value\"\n#   #   # The type of the result\n#   #   # Can be \"string\", \"integer\", or \"float\"\n#   #   type = \"string\"\n#\n#   ## specify server password\n#   # password = \"s#cr@t%\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Read metrics from one or many redis-sentinel servers\n# [[inputs.redis_sentinel]]\n#   ## specify servers via a url matching:\n#   ##  [protocol://][:password]@address[:port]\n#   ##  e.g.\n#   ##    tcp://localhost:26379\n#   ##    tcp://:password@192.168.99.100\n#   ##    unix:///var/run/redis-sentinel.sock\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no port is specified, 26379 is used\n#   # servers = [\"tcp://localhost:26379\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Read metrics from one or many RethinkDB servers\n# [[inputs.rethinkdb]]\n#   ## An array of URI to gather stats about. Specify an ip or hostname\n#   ## with optional port add password. ie,\n#   ##   rethinkdb://user:auth_key@10.10.3.30:28105,\n#   ##   rethinkdb://10.10.3.33:18832,\n#   ##   10.0.0.1:10000, etc.\n#   servers = [\"127.0.0.1:28015\"]\n#   ##\n#   ## If you use actual rethinkdb of \u003e 2.3.0 with username/password authorization,\n#   ## protocol have to be named \"rethinkdb2\" - it will use 1_0 H.\n#   # servers = [\"rethinkdb2://username:password@127.0.0.1:28015\"]\n#   ##\n#   ## If you use older versions of rethinkdb (\u003c2.2) with auth_key, protocol\n#   ## have to be named \"rethinkdb\".\n#   # servers = [\"rethinkdb://username:auth_key@127.0.0.1:28015\"]\n\n\n# # Read metrics one or many Riak servers\n# [[inputs.riak]]\n#   # Specify a list of one or more riak http servers\n#   servers = [\"http://localhost:8098\"]\n\n\n# # Read API usage and limits for a Salesforce organisation\n# [[inputs.salesforce]]\n#   ## specify your credentials\n#   ##\n#   username = \"your_username\"\n#   password = \"your_password\"\n#   ##\n#   ## (optional) security token\n#   # security_token = \"your_security_token\"\n#   ##\n#   ## (optional) environment type (sandbox or production)\n#   ## default is: production\n#   ##\n#   # environment = \"production\"\n#   ##\n#   ## (optional) API version (default: \"39.0\")\n#   ##\n#   # version = \"39.0\"\n\n\n# # Monitor sensors, requires lm-sensors package\n# [[inputs.sensors]]\n#   ## Remove numbers from field names.\n#   ## If true, a field name like 'temp1_input' will be changed to 'temp_input'.\n#   # remove_numbers = true\n#\n#   ## Timeout is the maximum amount of time that the sensors command can run.\n#   # timeout = \"5s\"\n\n\n# # Read metrics from storage devices supporting S.M.A.R.T.\n# [[inputs.smart]]\n#   ## Optionally specify the path to the smartctl executable\n#   # path_smartctl = \"/usr/bin/smartctl\"\n#\n#   ## Optionally specify the path to the nvme-cli executable\n#   # path_nvme = \"/usr/bin/nvme\"\n#\n#   ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n#   ## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n#   ## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n#   # enable_extensions = [\"auto-on\"]\n#\n#   ## On most platforms used cli utilities requires root access.\n#   ## Setting 'use_sudo' to true will make use of sudo to run smartctl or nvme-cli.\n#   ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli\n#   ## without a password.\n#   # use_sudo = false\n#\n#   ## Skip checking disks in this power mode. Defaults to\n#   ## \"standby\" to not wake up disks that have stopped rotating.\n#   ## See --nocheck in the man pages for smartctl.\n#   ## smartctl version 5.41 and 5.42 have faulty detection of\n#   ## power mode and might require changing this value to\n#   ## \"never\" depending on your disks.\n#   # nocheck = \"standby\"\n#\n#   ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n#   ## information from each drive into the 'smart_attribute' measurement.\n#   # attributes = false\n#\n#   ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n#   # excludes = [ \"/dev/pass6\" ]\n#\n#   ## Optionally specify devices and device type, if unset\n#   ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n#   ## and all found will be included except for the excluded in excludes.\n#   # devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n#\n#   ## Timeout for the cli command to complete.\n#   # timeout = \"30s\"\n#\n#   ## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n#   ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n#   ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n#   ## SMART data - one individual array drive at the time. In such case please set this configuration option\n#   ## to \"sequential\" to get readings for all drives.\n#   ## valid options: concurrent, sequential\n#   # read_method = \"concurrent\"\n\n\n# # Retrieves SNMP values from remote agents\n# [[inputs.snmp]]\n#   ## Agent addresses to retrieve values from.\n#   ##   format:  agents = [\"\u003cscheme://\u003e\u003chostname\u003e:\u003cport\u003e\"]\n#   ##   scheme:  optional, either udp, udp4, udp6, tcp, tcp4, tcp6.\n#   ##            default is udp\n#   ##   port:    optional\n#   ##   example: agents = [\"udp://127.0.0.1:161\"]\n#   ##            agents = [\"tcp://127.0.0.1:161\"]\n#   ##            agents = [\"udp4://v4only-snmp-agent\"]\n#   agents = [\"udp://127.0.0.1:161\"]\n#\n#   ## Timeout for each request.\n#   # timeout = \"5s\"\n#\n#   ## SNMP version; can be 1, 2, or 3.\n#   # version = 2\n#\n#   ## Path to mib files\n#   # path = [\"/usr/share/snmp/mibs\"]\n#\n#   ## Agent host tag; the tag used to reference the source host\n#   # agent_host_tag = \"agent_host\"\n#\n#   ## SNMP community string.\n#   # community = \"public\"\n#\n#   ## Number of retries to attempt.\n#   # retries = 3\n#\n#   ## The GETBULK max-repetitions parameter.\n#   # max_repetitions = 10\n#\n#   ## SNMPv3 authentication and encryption options.\n#   ##\n#   ## Security Name.\n#   # sec_name = \"myuser\"\n#   ## Authentication protocol; one of \"MD5\", \"SHA\", \"SHA224\", \"SHA256\", \"SHA384\", \"SHA512\" or \"\".\n#   # auth_protocol = \"MD5\"\n#   ## Authentication password.\n#   # auth_password = \"pass\"\n#   ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n#   # sec_level = \"authNoPriv\"\n#   ## Context Name.\n#   # context_name = \"\"\n#   ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\" or \"\".\n#   # priv_protocol = \"\"\n#   ## Privacy password used for encrypted messages.\n#   # priv_password = \"\"\n#\n#   ## Add fields and tables defining the variables you wish to collect.  This\n#   ## example collects the system uptime and interface variables.  Reference the\n#   ## full plugin documentation for configuration details.\n\n\n# # DEPRECATED! PLEASE USE inputs.snmp INSTEAD.\n# [[inputs.snmp_legacy]]\n#   ## DEPRECATED: The 'snmp_legacy' plugin is deprecated in version 1.0.0, use 'inputs.snmp' instead.\n#   ## Use 'oids.txt' file to translate oids to names\n#   ## To generate 'oids.txt' you need to run:\n#   ##   snmptranslate -m all -Tz -On | sed -e 's/\"//g' \u003e /tmp/oids.txt\n#   ## Or if you have an other MIB folder with custom MIBs\n#   ##   snmptranslate -M /mycustommibfolder -Tz -On -m all | sed -e 's/\"//g' \u003e oids.txt\n#   snmptranslate_file = \"/tmp/oids.txt\"\n#   [[inputs.snmp.host]]\n#     address = \"192.168.2.2:161\"\n#     # SNMP community\n#     community = \"public\" # default public\n#     # SNMP version (1, 2 or 3)\n#     # Version 3 not supported yet\n#     version = 2 # default 2\n#     # SNMP response timeout\n#     timeout = 2.0 # default 2.0\n#     # SNMP request retries\n#     retries = 2 # default 2\n#     # Which get/bulk do you want to collect for this host\n#     collect = [\"mybulk\", \"sysservices\", \"sysdescr\"]\n#     # Simple list of OIDs to get, in addition to \"collect\"\n#     get_oids = []\n#\n#   [[inputs.snmp.host]]\n#     address = \"192.168.2.3:161\"\n#     community = \"public\"\n#     version = 2\n#     timeout = 2.0\n#     retries = 2\n#     collect = [\"mybulk\"]\n#     get_oids = [\n#         \"ifNumber\",\n#         \".1.3.6.1.2.1.1.3.0\",\n#     ]\n#\n#   [[inputs.snmp.get]]\n#     name = \"ifnumber\"\n#     oid = \"ifNumber\"\n#\n#   [[inputs.snmp.get]]\n#     name = \"interface_speed\"\n#     oid = \"ifSpeed\"\n#     instance = \"0\"\n#\n#   [[inputs.snmp.get]]\n#     name = \"sysuptime\"\n#     oid = \".1.3.6.1.2.1.1.3.0\"\n#     unit = \"second\"\n#\n#   [[inputs.snmp.bulk]]\n#     name = \"mybulk\"\n#     max_repetition = 127\n#     oid = \".1.3.6.1.2.1.1\"\n#\n#   [[inputs.snmp.bulk]]\n#     name = \"ifoutoctets\"\n#     max_repetition = 127\n#     oid = \"ifOutOctets\"\n#\n#   [[inputs.snmp.host]]\n#     address = \"192.168.2.13:161\"\n#     #address = \"127.0.0.1:161\"\n#     community = \"public\"\n#     version = 2\n#     timeout = 2.0\n#     retries = 2\n#     #collect = [\"mybulk\", \"sysservices\", \"sysdescr\", \"systype\"]\n#     collect = [\"sysuptime\" ]\n#     [[inputs.snmp.host.table]]\n#       name = \"iftable3\"\n#       include_instances = [\"enp5s0\", \"eth1\"]\n#\n#   # SNMP TABLEs\n#   # table without mapping neither subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable1\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#\n#   # table without mapping but with subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable2\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#     sub_tables = [\".1.3.6.1.2.1.2.2.1.13\"]\n#\n#   # table with mapping but without subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable3\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#     # if empty. get all instances\n#     mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n#     # if empty, get all subtables\n#\n#   # table with both mapping and subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable4\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#     # if empty get all instances\n#     mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n#     # if empty get all subtables\n#     # sub_tables could be not \"real subtables\"\n#     sub_tables=[\".1.3.6.1.2.1.2.2.1.13\", \"bytes_recv\", \"bytes_send\"]\n\n\n# # Gather indicators from established connections, using iproute2's `ss` command.\n# [[inputs.socketstat]]\n#   ## ss can display information about tcp, udp, raw, unix, packet, dccp and sctp sockets\n#   ## List of protocol types to collect\n#   # protocols = [ \"tcp\", \"udp\" ]\n#   ## The default timeout of 1s for ss execution can be overridden here:\n#   # timeout = \"1s\"\n\n\n# # Read stats from one or more Solr servers or cores\n# [[inputs.solr]]\n#   ## specify a list of one or more Solr servers\n#   servers = [\"http://localhost:8983\"]\n#\n#   ## specify a list of one or more Solr cores (default - all)\n#   # cores = [\"main\"]\n#\n#   ## Optional HTTP Basic Auth Credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n\n\n# # Gather timeseries from Google Cloud Platform v3 monitoring API\n# [[inputs.stackdriver]]\n#   ## GCP Project\n#   project = \"erudite-bloom-151019\"\n#\n#   ## Include timeseries that start with the given metric type.\n#   metric_type_prefix_include = [\n#     \"compute.googleapis.com/\",\n#   ]\n#\n#   ## Exclude timeseries that start with the given metric type.\n#   # metric_type_prefix_exclude = []\n#\n#   ## Many metrics are updated once per minute; it is recommended to override\n#   ## the agent level interval with a value of 1m or greater.\n#   interval = \"1m\"\n#\n#   ## Maximum number of API calls to make per second.  The quota for accounts\n#   ## varies, it can be viewed on the API dashboard:\n#   ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n#   # rate_limit = 14\n#\n#   ## The delay and window options control the number of points selected on\n#   ## each gather.  When set, metrics are gathered between:\n#   ##   start: now() - delay - window\n#   ##   end:   now() - delay\n#   #\n#   ## Collection delay; if set too low metrics may not yet be available.\n#   # delay = \"5m\"\n#   #\n#   ## If unset, the window will start at 1m and be updated dynamically to span\n#   ## the time between calls (approximately the length of the plugin interval).\n#   # window = \"1m\"\n#\n#   ## TTL for cached list of metric types.  This is the maximum amount of time\n#   ## it may take to discover new metrics.\n#   # cache_ttl = \"1h\"\n#\n#   ## If true, raw bucket counts are collected for distribution value types.\n#   ## For a more lightweight collection, you may wish to disable and use\n#   ## distribution_aggregation_aligners instead.\n#   # gather_raw_distribution_buckets = true\n#\n#   ## Aggregate functions to be used for metrics whose value type is\n#   ## distribution.  These aggregate values are recorded in in addition to raw\n#   ## bucket counts; if they are enabled.\n#   ##\n#   ## For a list of aligner strings see:\n#   ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n#   # distribution_aggregation_aligners = [\n#   # \t\"ALIGN_PERCENTILE_99\",\n#   # \t\"ALIGN_PERCENTILE_95\",\n#   # \t\"ALIGN_PERCENTILE_50\",\n#   # ]\n#\n#   ## Filters can be added to reduce the number of time series matched.  All\n#   ## functions are supported: starts_with, ends_with, has_substring, and\n#   ## one_of.  Only the '=' operator is supported.\n#   ##\n#   ## The logical operators when combining filters are defined statically using\n#   ## the following values:\n#   ##   filter ::= \u003cresource_labels\u003e {AND \u003cmetric_labels\u003e}\n#   ##   resource_labels ::= \u003cresource_labels\u003e {OR \u003cresource_label\u003e}\n#   ##   metric_labels ::= \u003cmetric_labels\u003e {OR \u003cmetric_label\u003e}\n#   ##\n#   ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n#   #\n#   ## Resource labels refine the time series selection with the following expression:\n#   ##   resource.labels.\u003ckey\u003e = \u003cvalue\u003e\n#   # [[inputs.stackdriver.filter.resource_labels]]\n#   #   key = \"instance_name\"\n#   #   value = 'starts_with(\"localhost\")'\n#   #\n#   ## Metric labels refine the time series selection with the following expression:\n#   ##   metric.labels.\u003ckey\u003e = \u003cvalue\u003e\n#   #  [[inputs.stackdriver.filter.metric_labels]]\n#   #  \t key = \"device_name\"\n#   #  \t value = 'one_of(\"sda\", \"sdb\")'\n\n\n# # Get synproxy counter statistics from procfs\n# [[inputs.synproxy]]\n#   # no configuration\n\n\n# # Sysstat metrics collector\n# [[inputs.sysstat]]\n#   ## Path to the sadc command.\n#   #\n#   ## Common Defaults:\n#   ##   Debian/Ubuntu: /usr/lib/sysstat/sadc\n#   ##   Arch:          /usr/lib/sa/sadc\n#   ##   RHEL/CentOS:   /usr/lib64/sa/sadc\n#   sadc_path = \"/usr/lib/sa/sadc\" # required\n#\n#   ## Path to the sadf command, if it is not in PATH\n#   # sadf_path = \"/usr/bin/sadf\"\n#\n#   ## Activities is a list of activities, that are passed as argument to the\n#   ## sadc collector utility (e.g: DISK, SNMP etc...)\n#   ## The more activities that are added, the more data is collected.\n#   # activities = [\"DISK\"]\n#\n#   ## Group metrics to measurements.\n#   ##\n#   ## If group is false each metric will be prefixed with a description\n#   ## and represents itself a measurement.\n#   ##\n#   ## If Group is true, corresponding metrics are grouped to a single measurement.\n#   # group = true\n#\n#   ## Options for the sadf command. The values on the left represent the sadf\n#   ## options and the values on the right their description (which are used for\n#   ## grouping and prefixing metrics).\n#   ##\n#   ## Run 'sar -h' or 'man sar' to find out the supported options for your\n#   ## sysstat version.\n#   [inputs.sysstat.options]\n#     -C = \"cpu\"\n#     -B = \"paging\"\n#     -b = \"io\"\n#     -d = \"disk\"             # requires DISK activity\n#     \"-n ALL\" = \"network\"\n#     \"-P ALL\" = \"per_cpu\"\n#     -q = \"queue\"\n#     -R = \"mem\"\n#     -r = \"mem_util\"\n#     -S = \"swap_util\"\n#     -u = \"cpu_util\"\n#     -v = \"inode\"\n#     -W = \"swap\"\n#     -w = \"task\"\n#   #  -H = \"hugepages\"        # only available for newer linux distributions\n#   #  \"-I ALL\" = \"interrupts\" # requires INT activity\n#\n#   ## Device tags can be used to add additional tags for devices.\n#   ## For example the configuration below adds a tag vg with value rootvg for\n#   ## all metrics with sda devices.\n#   # [[inputs.sysstat.device_tags.sda]]\n#   #  vg = \"rootvg\"\n\n\n# # Gather systemd units state\n# [[inputs.systemd_units]]\n#   ## Set timeout for systemctl execution\n#   # timeout = \"1s\"\n#   #\n#   ## Filter for a specific unit type, default is \"service\", other possible\n#   ## values are \"socket\", \"target\", \"device\", \"mount\", \"automount\", \"swap\",\n#   ## \"timer\", \"path\", \"slice\" and \"scope \":\n#   # unittype = \"service\"\n#   #\n#   ## Filter for a specific pattern, default is \"\" (i.e. all), other possible\n#   ## values are valid pattern for systemctl, e.g. \"a*\" for all units with\n#   ## names starting with \"a\"\n#   # pattern = \"\"\n#   ## pattern = \"telegraf* influxdb*\"\n#   ## pattern = \"a*\"\n\n\n# # Reads metrics from a Teamspeak 3 Server via ServerQuery\n# [[inputs.teamspeak]]\n#   ## Server address for Teamspeak 3 ServerQuery\n#   # server = \"127.0.0.1:10011\"\n#   ## Username for ServerQuery\n#   username = \"serverqueryuser\"\n#   ## Password for ServerQuery\n#   password = \"secret\"\n#   ## Array of virtual servers\n#   # virtual_servers = [1]\n\n\n# # Read metrics about temperature\n# [[inputs.temp]]\n#   # no configuration\n\n\n# # Read Tengine's basic status information (ngx_http_reqstat_module)\n# [[inputs.tengine]]\n#   # An array of Tengine reqstat module URI to gather stats.\n#   urls = [\"http://127.0.0.1/us\"]\n#\n#   # HTTP response timeout (default: 5s)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.cer\"\n#   # tls_key = \"/etc/telegraf/key.key\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Gather metrics from the Tomcat server status page.\n# [[inputs.tomcat]]\n#   ## URL of the Tomcat server status\n#   # url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n#\n#   ## HTTP Basic Auth Credentials\n#   # username = \"tomcat\"\n#   # password = \"s3cret\"\n#\n#   ## Request timeout\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Inserts sine and cosine waves for demonstration purposes\n# [[inputs.trig]]\n#   ## Set the amplitude\n#   amplitude = 10.0\n\n\n# # Read Twemproxy stats data\n# [[inputs.twemproxy]]\n#   ## Twemproxy stats address and port (no scheme)\n#   addr = \"localhost:22222\"\n#   ## Monitor pool name\n#   pools = [\"redis_pool\", \"mc_pool\"]\n\n\n# # A plugin to collect stats from the Unbound DNS resolver\n# [[inputs.unbound]]\n#   ## Address of server to connect to, read from unbound conf default, optionally ':port'\n#   ## Will lookup IP if given a hostname\n#   server = \"127.0.0.1:8953\"\n#\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   # use_sudo = false\n#\n#   ## The default location of the unbound-control binary can be overridden with:\n#   # binary = \"/usr/sbin/unbound-control\"\n#\n#   ## The default location of the unbound config file can be overridden with:\n#   # config_file = \"/etc/unbound/unbound.conf\"\n#\n#   ## The default timeout of 1s can be overridden with:\n#   # timeout = \"1s\"\n#\n#   ## When set to true, thread metrics are tagged with the thread id.\n#   ##\n#   ## The default is false for backwards compatibility, and will be changed to\n#   ## true in a future version.  It is recommended to set to true on new\n#   ## deployments.\n#   thread_as_tag = false\n\n\n# # Read uWSGI metrics.\n# [[inputs.uwsgi]]\n#   ## List with urls of uWSGI Stats servers. URL must match pattern:\n#   ## scheme://address[:port]\n#   ##\n#   ## For example:\n#   ## servers = [\"tcp://localhost:5050\", \"http://localhost:1717\", \"unix:///tmp/statsock\"]\n#   servers = [\"tcp://127.0.0.1:1717\"]\n#\n#   ## General connection timeout\n#   # timeout = \"5s\"\n\n\n# # A plugin to collect stats from Varnish HTTP Cache\n# [[inputs.varnish]]\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   #use_sudo = false\n#\n#   ## The default location of the varnishstat binary can be overridden with:\n#   binary = \"/usr/bin/varnishstat\"\n#\n#   ## Additional custom arguments for the varnishstat command\n#   # binary_args = [\"-f\", \"MAIN.*\"]\n#\n#   ## The default location of the varnishadm binary can be overriden with:\n#   adm_binary = \"/usr/bin/varnishadm\"\n#\n#   ## Custom arguments for the varnishadm command\n#   # adm_binary_args = [\"\"]\n#\n#   ## Metric version defaults to metric_version=1, use metric_version=2 for removal of nonactive vcls.\n#   metric_version = 1\n#\n#   ## Additional regexps to override builtin conversion of varnish metrics into telegraf metrics.\n#   ## Regexp group \"_vcl\" is used for extracting the VCL name. Metrics that contains nonactive VCL's are skipped.\n#   ## Regexp group \"_field\" overrides field name. Other named regexp groups are used as tags.\n#   # regexps = ['XCNT\\.(?P\u003c_vcl\u003e[\\w\\-]*)\\.(?P\u003cgroup\u003e[\\w\\-.+]*)\\.(?P\u003c_field\u003e[\\w\\-.+]*)\\.val']\n#\n#   ## By default, telegraf gather stats for 3 metric points.\n#   ## Setting stats will override the defaults shown below.\n#   ## Glob matching can be used, ie, stats = [\"MAIN.*\"]\n#   ## stats may also be set to [\"*\"], which will collect all stats\n#   stats = [\"MAIN.cache_hit\", \"MAIN.cache_miss\", \"MAIN.uptime\"]\n#\n#   ## Optional name for the varnish instance (or working directory) to query\n#   ## Usually append after -n in varnish cli\n#   # instance_name = instanceName\n#\n#   ## Timeout for varnishstat command\n#   # timeout = \"1s\"\n\n\n# # Read metrics from the Vault API\n# [[inputs.vault]]\n#   ## URL for the Vault agent\n#   # url = \"http://127.0.0.1:8200\"\n#\n#   ## Use Vault token for authorization.\n#   ## Vault token configuration is mandatory.\n#   ## If both are empty or both are set, an error is thrown.\n#   # token_file = \"/path/to/auth/token\"\n#   ## OR\n#   token = \"s.CDDrgg5zPv5ssI0Z2P4qxJj2\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n\n\n# # Collect Wireguard server interface and peer statistics\n# [[inputs.wireguard]]\n#   ## Optional list of Wireguard device/interface names to query.\n#   ## If omitted, all Wireguard interfaces are queried.\n#   # devices = [\"wg0\"]\n\n\n# # Monitor wifi signal strength and quality\n# [[inputs.wireless]]\n#   ## Sets 'proc' directory path\n#   ## If not specified, then default is /proc\n#   # host_proc = \"/proc\"\n\n\n# # Reads metrics from a SSL certificate\n# [[inputs.x509_cert]]\n#   ## List certificate sources\n#   ## Prefix your entry with 'file://' if you intend to use relative paths\n#   sources = [\"tcp://example.org:443\", \"https://influxdata.com:443\",\n#             \"udp://127.0.0.1:4433\", \"/etc/ssl/certs/ssl-cert-snakeoil.pem\",\n#             \"/etc/mycerts/*.mydomain.org.pem\", \"file:///path/to/*.pem\"]\n#\n#   ## Timeout for SSL connection\n#   # timeout = \"5s\"\n#\n#   ## Pass a different name into the TLS request (Server Name Indication)\n#   ##   example: server_name = \"myhost.example.org\"\n#   # server_name = \"\"\n#\n#   ## Don't include root or intermediate certificates in output\n#   # exclude_root_certs = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n\n\n# # Gathers Metrics From a Dell EMC XtremIO Storage Array's V3 API\n# [[inputs.xtremio]]\n#   ## XtremIO User Interface Endpoint\n#   url = \"https://xtremio.example.com/\" # required\n#\n#   ## Credentials\n#   username = \"user1\"\n#   password = \"pass123\"\n#\n#   ## Metrics to collect from the XtremIO\n#   # collectors = [\"bbus\",\"clusters\",\"ssds\",\"volumes\",\"xms\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use SSL but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, pools and datasets\n# [[inputs.zfs]]\n#   ## ZFS kstat path. Ignored on FreeBSD\n#   ## If not specified, then default is:\n#   # kstatPath = \"/proc/spl/kstat/zfs\"\n#\n#   ## By default, telegraf gather all zfs stats\n#   ## If not specified, then default is:\n#   # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n#   ## For Linux, the default is:\n#   # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n#   #   \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n#   ## By default, don't gather zpool stats\n#   # poolMetrics = false\n#   ## By default, don't gather zdataset stats\n#   # datasetMetrics = false\n\n\n# # Reads 'mntr' stats from one or many zookeeper servers\n# [[inputs.zookeeper]]\n#   ## An array of address to gather stats about. Specify an ip or hostname\n#   ## with port. ie localhost:2181, 10.0.0.1:2181, etc.\n#\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no port is specified, 2181 is used\n#   servers = [\":2181\"]\n#\n#   ## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # enable_tls = true\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n###############################################################################\n#                            SERVICE INPUT PLUGINS                            #\n###############################################################################\n\n\n# # Pull Metric Statistics from Aliyun CMS\n# [[inputs.aliyuncms]]\n#   ## Aliyun Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Ram RoleArn credential\n#   ## 2) AccessKey STS token credential\n#   ## 3) AccessKey credential\n#   ## 4) Ecs Ram Role credential\n#   ## 5) RSA keypair credential\n#   ## 6) Environment variables credential\n#   ## 7) Instance metadata credential\n#\n#   # access_key_id = \"\"\n#   # access_key_secret = \"\"\n#   # access_key_sts_token = \"\"\n#   # role_arn = \"\"\n#   # role_session_name = \"\"\n#   # private_key = \"\"\n#   # public_key_id = \"\"\n#   # role_name = \"\"\n#\n#   ## Specify the ali cloud region list to be queried for metrics and objects discovery\n#   ## If not set, all supported regions (see below) would be covered, it can provide a significant load on API, so the recommendation here\n#   ## is to limit the list as much as possible. Allowed values: https://www.alibabacloud.com/help/zh/doc-detail/40654.htm\n#   ## Default supported regions are:\n#   ## 21 items: cn-qingdao,cn-beijing,cn-zhangjiakou,cn-huhehaote,cn-hangzhou,cn-shanghai,cn-shenzhen,\n#   ##           cn-heyuan,cn-chengdu,cn-hongkong,ap-southeast-1,ap-southeast-2,ap-southeast-3,ap-southeast-5,\n#   ##           ap-south-1,ap-northeast-1,us-west-1,us-east-1,eu-central-1,eu-west-1,me-east-1\n#   ##\n#   ## From discovery perspective it set the scope for object discovery, the discovered info can be used to enrich\n#   ## the metrics with objects attributes/tags. Discovery is supported not for all projects (if not supported, then\n#   ## it will be reported on the start - for example for 'acs_cdn' project:\n#   ## 'E! [inputs.aliyuncms] Discovery tool is not activated: no discovery support for project \"acs_cdn\"' )\n#   ## Currently, discovery supported for the following projects:\n#   ## - acs_ecs_dashboard\n#   ## - acs_rds_dashboard\n#   ## - acs_slb_dashboard\n#   ## - acs_vpc_eip\n#   regions = [\"cn-hongkong\"]\n#\n#   # The minimum period for AliyunCMS metrics is 1 minute (60s). However not all\n#   # metrics are made available to the 1 minute period. Some are collected at\n#   # 3 minute, 5 minute, or larger intervals.\n#   # See: https://help.aliyun.com/document_detail/51936.html?spm=a2c4g.11186623.2.18.2bc1750eeOw1Pv\n#   # Note that if a period is configured that is smaller than the minimum for a\n#   # particular metric, that metric will not be returned by the Aliyun OpenAPI\n#   # and will not be collected by Telegraf.\n#   #\n#   ## Requested AliyunCMS aggregation Period (required - must be a multiple of 60s)\n#   period = \"5m\"\n#\n#   ## Collection Delay (required - must account for metrics availability via AliyunCMS API)\n#   delay = \"1m\"\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n#   ## gaps or overlap in pulled data\n#   interval = \"5m\"\n#\n#   ## Metric Statistic Project (required)\n#   project = \"acs_slb_dashboard\"\n#\n#   ## Maximum requests per second, default value is 200\n#   ratelimit = 200\n#\n#   ## How often the discovery API call executed (default 1m)\n#   #discovery_interval = \"1m\"\n#\n#   ## Metrics to Pull (Required)\n#   [[inputs.aliyuncms.metrics]]\n#   ## Metrics names to be requested,\n#   ## described here (per project): https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n#   names = [\"InstanceActiveConnection\", \"InstanceNewConnection\"]\n#\n#   ## Dimension filters for Metric (these are optional).\n#   ## This allows to get additional metric dimension. If dimension is not specified it can be returned or\n#   ## the data can be aggregated - it depends on particular metric, you can find details here: https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n#   ##\n#   ## Note, that by default dimension filter includes the list of discovered objects in scope (if discovery is enabled)\n#   ## Values specified here would be added into the list of discovered objects.\n#   ## You can specify either single dimension:\n#   #dimensions = '{\"instanceId\": \"p-example\"}'\n#\n#   ## Or you can specify several dimensions at once:\n#   #dimensions = '[{\"instanceId\": \"p-example\"},{\"instanceId\": \"q-example\"}]'\n#\n#   ## Enrichment tags, can be added from discovery (if supported)\n#   ## Notation is \u003cmeasurement_tag_name\u003e:\u003cJMES query path (https://jmespath.org/tutorial.html)\u003e\n#   ## To figure out which fields are available, consult the Describe\u003cObjectType\u003e API per project.\n#   ## For example, for SLB: https://api.aliyun.com/#/?product=Slb\u0026version=2014-05-15\u0026api=DescribeLoadBalancers\u0026params={}\u0026tab=MOCK\u0026lang=GO\n#   #tag_query_path = [\n#   #    \"address:Address\",\n#   #    \"name:LoadBalancerName\",\n#   #    \"cluster_owner:Tags.Tag[?TagKey=='cs.cluster.name'].TagValue | [0]\"\n#   #    ]\n#   ## The following tags added by default: regionId (if discovery enabled), userId, instanceId.\n#\n#   ## Allow metrics without discovery data, if discovery is enabled. If set to true, then metric without discovery\n#   ## data would be emitted, otherwise dropped. This cane be of help, in case debugging dimension filters, or partial coverage\n#   ## of discovery scope vs monitoring scope\n#   #allow_dps_without_discovery = false\n\n\n# # AMQP consumer plugin\n# [[inputs.amqp_consumer]]\n#   ## Brokers to consume from.  If multiple brokers are specified a random broker\n#   ## will be selected anytime a connection is established.  This can be\n#   ## helpful for load balancing when not using a dedicated load balancer.\n#   brokers = [\"amqp://localhost:5672/influxdb\"]\n#\n#   ## Authentication credentials for the PLAIN auth_method.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Name of the exchange to declare.  If unset, no exchange will be declared.\n#   exchange = \"telegraf\"\n#\n#   ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n#   # exchange_type = \"topic\"\n#\n#   ## If true, exchange will be passively declared.\n#   # exchange_passive = false\n#\n#   ## Exchange durability can be either \"transient\" or \"durable\".\n#   # exchange_durability = \"durable\"\n#\n#   ## Additional exchange arguments.\n#   # exchange_arguments = { }\n#   # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n#\n#   ## AMQP queue name.\n#   queue = \"telegraf\"\n#\n#   ## AMQP queue durability can be \"transient\" or \"durable\".\n#   queue_durability = \"durable\"\n#\n#   ## If true, queue will be passively declared.\n#   # queue_passive = false\n#\n#   ## A binding between the exchange and queue using this binding key is\n#   ## created.  If unset, no binding is created.\n#   binding_key = \"#\"\n#\n#   ## Maximum number of messages server should give to the worker.\n#   # prefetch_count = 50\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Auth method. PLAIN and EXTERNAL are supported\n#   ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n#   ## described here: https://www.rabbitmq.com/plugins.html\n#   # auth_method = \"PLAIN\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Content encoding for message payloads, can be set to \"gzip\" to or\n#   ## \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read Cassandra metrics through Jolokia\n# [[inputs.cassandra]]\n#   ## DEPRECATED: The 'cassandra' plugin is deprecated in version 1.7.0, use 'inputs.jolokia2' with the 'cassandra.conf' example configuration instead.\n#   ## DEPRECATED: The cassandra plugin has been deprecated.  Please use the\n#   ## jolokia2 plugin instead.\n#   ##\n#   ## see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n#\n#   context = \"/jolokia/read\"\n#   ## List of cassandra servers exposing jolokia read service\n#   servers = [\"myuser:mypassword@10.10.10.1:8778\",\"10.10.10.2:8778\",\":8778\"]\n#   ## List of metrics collected on above servers\n#   ## Each metric consists of a jmx path.\n#   ## This will collect all heap memory usage metrics from the jvm and\n#   ## ReadLatency metrics for all keyspaces and tables.\n#   ## \"type=Table\" in the query works with Cassandra3.0. Older versions might\n#   ## need to use \"type=ColumnFamily\"\n#   metrics  = [\n#     \"/java.lang:type=Memory/HeapMemoryUsage\",\n#     \"/org.apache.cassandra.metrics:type=Table,keyspace=*,scope=*,name=ReadLatency\"\n#   ]\n\n\n# # Cisco model-driven telemetry (MDT) input plugin for IOS XR, IOS XE and NX-OS platforms\n# [[inputs.cisco_telemetry_mdt]]\n#  ## Telemetry transport can be \"tcp\" or \"grpc\".  TLS is only supported when\n#  ## using the grpc transport.\n#  transport = \"grpc\"\n#\n#  ## Address and port to host telemetry listener\n#  service_address = \":57000\"\n#\n#  ## Enable TLS; grpc transport only.\n#  # tls_cert = \"/etc/telegraf/cert.pem\"\n#  # tls_key = \"/etc/telegraf/key.pem\"\n#\n#  ## Enable TLS client authentication and define allowed CA certificates; grpc\n#  ##  transport only.\n#  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#  ## Define (for certain nested telemetry measurements with embedded tags) which fields are tags\n#  # embedded_tags = [\"Cisco-IOS-XR-qos-ma-oper:qos/interface-table/interface/input/service-policy-names/service-policy-instance/statistics/class-stats/class-name\"]\n#\n#  ## Define aliases to map telemetry encoding paths to simple measurement names\n#  [inputs.cisco_telemetry_mdt.aliases]\n#    ifstats = \"ietf-interfaces:interfaces-state/interface/statistics\"\n#  ##Define Property Xformation, please refer README and https://pubhub.devnetcloud.com/media/dme-docs-9-3-3/docs/appendix/ for Model details.\n#  [inputs.cisco_telemetry_mdt.dmes]\n#    ModTs = \"ignore\"\n#    CreateTs = \"ignore\"\n\n\n# # Read metrics from one or many ClickHouse servers\n# [[inputs.clickhouse]]\n#   ## Username for authorization on ClickHouse server\n#   ## example: username = \"default\"\n#   username = \"default\"\n#\n#   ## Password for authorization on ClickHouse server\n#   ## example: password = \"super_secret\"\n#\n#   ## HTTP(s) timeout while getting metrics values\n#   ## The timeout includes connection time, any redirects, and reading the response body.\n#   ##   example: timeout = 1s\n#   # timeout = 5s\n#\n#   ## List of servers for metrics scraping\n#   ## metrics scrape via HTTP(s) clickhouse interface\n#   ## https://clickhouse.tech/docs/en/interfaces/http/\n#   ##    example: servers = [\"http://127.0.0.1:8123\",\"https://custom-server.mdb.yandexcloud.net\"]\n#   servers         = [\"http://127.0.0.1:8123\"]\n#\n#   ## If \"auto_discovery\"\" is \"true\" plugin tries to connect to all servers available in the cluster\n#   ## with using same \"user:password\" described in \"user\" and \"password\" parameters\n#   ## and get this server hostname list from \"system.clusters\" table\n#   ## see\n#   ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters\n#   ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers\n#   ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n#   ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables\n#   ##    example: auto_discovery = false\n#   # auto_discovery = true\n#\n#   ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n#   ## when this filter present then \"WHERE cluster IN (...)\" filter will apply\n#   ## please use only full cluster names here, regexp and glob filters is not allowed\n#   ## for \"/etc/clickhouse-server/config.d/remote.xml\"\n#   ## \u003cyandex\u003e\n#   ##  \u003cremote_servers\u003e\n#   ##    \u003cmy-own-cluster\u003e\n#   ##        \u003cshard\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-ru-1.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-ru-2.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##        \u003c/shard\u003e\n#   ##        \u003cshard\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-eu-1.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-eu-2.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##        \u003c/shard\u003e\n#   ##    \u003c/my-onw-cluster\u003e\n#   ##  \u003c/remote_servers\u003e\n#   ##\n#   ## \u003c/yandex\u003e\n#   ##\n#   ## example: cluster_include = [\"my-own-cluster\"]\n#   # cluster_include = []\n#\n#   ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n#   ## when this filter present then \"WHERE cluster NOT IN (...)\" filter will apply\n#   ##    example: cluster_exclude = [\"my-internal-not-discovered-cluster\"]\n#   # cluster_exclude = []\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from Google PubSub\n# [[inputs.cloud_pubsub]]\n#   ## Required. Name of Google Cloud Platform (GCP) Project that owns\n#   ## the given PubSub subscription.\n#   project = \"my-project\"\n#\n#   ## Required. Name of PubSub subscription to ingest metrics from.\n#   subscription = \"my-subscription\"\n#\n#   ## Required. Data format to consume.\n#   ## Each data format has its own unique set of configuration options.\n#   ## Read more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n#   ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n#   ## Application Default Credentials, which is preferred.\n#   # credentials_file = \"path/to/my/creds.json\"\n#\n#   ## Optional. Number of seconds to wait before attempting to restart the\n#   ## PubSub subscription receiver after an unexpected error.\n#   ## If the streaming pull for a PubSub Subscription fails (receiver),\n#   ## the agent attempts to restart receiving messages after this many seconds.\n#   # retry_delay_seconds = 5\n#\n#   ## Optional. Maximum byte length of a message to consume.\n#   ## Larger messages are dropped with an error. If less than 0 or unspecified,\n#   ## treated as no limit.\n#   # max_message_len = 1000000\n#\n#   ## Optional. Maximum messages to read from PubSub that have not been written\n#   ## to an output. Defaults to 1000.\n#   ## For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message contains 10 metrics and the output\n#   ## metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## The following are optional Subscription ReceiveSettings in PubSub.\n#   ## Read more about these values:\n#   ## https://godoc.org/cloud.google.com/go/pubsub#ReceiveSettings\n#\n#   ## Optional. Maximum number of seconds for which a PubSub subscription\n#   ## should auto-extend the PubSub ACK deadline for each message. If less than\n#   ## 0, auto-extension is disabled.\n#   # max_extension = 0\n#\n#   ## Optional. Maximum number of unprocessed messages in PubSub\n#   ## (unacknowledged but not yet expired in PubSub).\n#   ## A value of 0 is treated as the default PubSub value.\n#   ## Negative values will be treated as unlimited.\n#   # max_outstanding_messages = 0\n#\n#   ## Optional. Maximum size in bytes of unprocessed messages in PubSub\n#   ## (unacknowledged but not yet expired in PubSub).\n#   ## A value of 0 is treated as the default PubSub value.\n#   ## Negative values will be treated as unlimited.\n#   # max_outstanding_bytes = 0\n#\n#   ## Optional. Max number of goroutines a PubSub Subscription receiver can spawn\n#   ## to pull messages from PubSub concurrently. This limit applies to each\n#   ## subscription separately and is treated as the PubSub default if less than\n#   ## 1. Note this setting does not limit the number of messages that can be\n#   ## processed concurrently (use \"max_outstanding_messages\" instead).\n#   # max_receiver_go_routines = 0\n#\n#   ## Optional. If true, Telegraf will attempt to base64 decode the\n#   ## PubSub message data before parsing\n#   # base64_data = false\n\n\n# # Google Cloud Pub/Sub Push HTTP listener\n# [[inputs.cloud_pubsub_push]]\n#   ## Address and port to host HTTP listener on\n#   service_address = \":8080\"\n#\n#   ## Application secret to verify messages originate from Cloud Pub/Sub\n#   # token = \"\"\n#\n#   ## Path to listen to.\n#   # path = \"/\"\n#\n#   ## Maximum duration before timing out read of the request\n#   # read_timeout = \"10s\"\n#   ## Maximum duration before timing out write of the response. This should be set to a value\n#   ## large enough that you can send at least 'metric_batch_size' number of messages within the\n#   ## duration.\n#   # write_timeout = \"10s\"\n#\n#   ## Maximum allowed http request body size in bytes.\n#   ## 0 means to use the default of 524,288,00 bytes (500 mebibytes)\n#   # max_body_size = \"500MB\"\n#\n#   ## Whether to add the pubsub metadata, such as message attributes and subscription as a tag.\n#   # add_meta = false\n#\n#   ## Optional. Maximum messages to read from PubSub that have not been written\n#   ## to an output. Defaults to 1000.\n#   ## For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message contains 10 metrics and the output\n#   ## metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Ingests files in a directory and then moves them to a target directory.\n# [[inputs.directory_monitor]]\n#   ## The directory to monitor and read files from.\n#   directory = \"\"\n#   #\n#   ## The directory to move finished files to.\n#   finished_directory = \"\"\n#   #\n#   ## The directory to move files to upon file error.\n#   ## If not provided, erroring files will stay in the monitored directory.\n#   # error_directory = \"\"\n#   #\n#   ## The amount of time a file is allowed to sit in the directory before it is picked up.\n#   ## This time can generally be low but if you choose to have a very large file written to the directory and it's potentially slow,\n#   ## set this higher so that the plugin will wait until the file is fully copied to the directory.\n#   # directory_duration_threshold = \"50ms\"\n#   #\n#   ## A list of the only file names to monitor, if necessary. Supports regex. If left blank, all files are ingested.\n#   # files_to_monitor = [\"^.*\\.csv\"]\n#   #\n#   ## A list of files to ignore, if necessary. Supports regex.\n#   # files_to_ignore = [\".DS_Store\"]\n#   #\n#   ## Maximum lines of the file to process that have not yet be written by the\n#   ## output. For best throughput set to the size of the output's metric_buffer_limit.\n#   ## Warning: setting this number higher than the output's metric_buffer_limit can cause dropped metrics.\n#   # max_buffered_metrics = 10000\n#   #\n#   ## The maximum amount of file paths to queue up for processing at once, before waiting until files are processed to find more files.\n#   ## Lowering this value will result in *slightly* less memory use, with a potential sacrifice in speed efficiency, if absolutely necessary.\n#   #\tfile_queue_size = 100000\n#   #\n#   ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n#   ## to disable. Cautious when file name variation is high, this can increase the cardinality\n#   ## significantly. Read more about cardinality here:\n#   ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n#   # file_tag = \"\"\n#   #\n#   ## The dataformat to be read from the files.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   ## NOTE: We currently only support parsing newline-delimited JSON. See the format here: https://github.com/ndjson/ndjson-spec\n#   data_format = \"influx\"\n\n\n# # Read logging output from the Docker engine\n# [[inputs.docker_log]]\n#   ## Docker Endpoint\n#   ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n#   ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n#   # endpoint = \"unix:///var/run/docker.sock\"\n#\n#   ## When true, container logs are read from the beginning; otherwise\n#   ## reading begins at the end of the log.\n#   # from_beginning = false\n#\n#   ## Timeout for Docker API calls.\n#   # timeout = \"5s\"\n#\n#   ## Containers to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all containers\n#   # container_name_include = []\n#   # container_name_exclude = []\n#\n#   ## Container states to include and exclude. Globs accepted.\n#   ## When empty only containers in the \"running\" state will be captured.\n#   # container_state_include = []\n#   # container_state_exclude = []\n#\n#   ## docker labels to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all labels as tags\n#   # docker_label_include = []\n#   # docker_label_exclude = []\n#\n#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n#   source_tag = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Azure Event Hubs service input plugin\n# [[inputs.eventhub_consumer]]\n#   ## The default behavior is to create a new Event Hub client from environment variables.\n#   ## This requires one of the following sets of environment variables to be set:\n#   ##\n#   ## 1) Expected Environment Variables:\n#   ##    - \"EVENTHUB_CONNECTION_STRING\"\n#   ##\n#   ## 2) Expected Environment Variables:\n#   ##    - \"EVENTHUB_NAMESPACE\"\n#   ##    - \"EVENTHUB_NAME\"\n#   ##    - \"EVENTHUB_KEY_NAME\"\n#   ##    - \"EVENTHUB_KEY_VALUE\"\n#\n#   ## 3) Expected Environment Variables:\n#   ##    - \"EVENTHUB_NAMESPACE\"\n#   ##    - \"EVENTHUB_NAME\"\n#   ##    - \"AZURE_TENANT_ID\"\n#   ##    - \"AZURE_CLIENT_ID\"\n#   ##    - \"AZURE_CLIENT_SECRET\"\n#\n#   ## Uncommenting the option below will create an Event Hub client based solely on the connection string.\n#   ## This can either be the associated environment variable or hard coded directly.\n#   ## If this option is uncommented, environment variables will be ignored.\n#   ## Connection string should contain EventHubName (EntityPath)\n#   # connection_string = \"\"\n#\n#   ## Set persistence directory to a valid folder to use a file persister instead of an in-memory persister\n#   # persistence_dir = \"\"\n#\n#   ## Change the default consumer group\n#   # consumer_group = \"\"\n#\n#   ## By default the event hub receives all messages present on the broker, alternative modes can be set below.\n#   ## The timestamp should be in https://github.com/toml-lang/toml#offset-date-time format (RFC 3339).\n#   ## The 3 options below only apply if no valid persister is read from memory or file (e.g. first run).\n#   # from_timestamp =\n#   # latest = true\n#\n#   ## Set a custom prefetch count for the receiver(s)\n#   # prefetch_count = 1000\n#\n#   ## Add an epoch to the receiver(s)\n#   # epoch = 0\n#\n#   ## Change to set a custom user agent, \"telegraf\" is used by default\n#   # user_agent = \"telegraf\"\n#\n#   ## To consume from a specific partition, set the partition_ids option.\n#   ## An empty array will result in receiving from all partitions.\n#   # partition_ids = [\"0\",\"1\"]\n#\n#   ## Max undelivered messages\n#   # max_undelivered_messages = 1000\n#\n#   ## Set either option below to true to use a system property as timestamp.\n#   ## You have the choice between EnqueuedTime and IoTHubEnqueuedTime.\n#   ## It is recommended to use this setting when the data itself has no timestamp.\n#   # enqueued_time_as_ts = true\n#   # iot_hub_enqueued_time_as_ts = true\n#\n#   ## Tags or fields to create from keys present in the application property bag.\n#   ## These could for example be set by message enrichments in Azure IoT Hub.\n#   # application_property_tags = []\n#   # application_property_fields = []\n#\n#   ## Tag or field name to use for metadata\n#   ## By default all metadata is disabled\n#   # sequence_number_field = \"SequenceNumber\"\n#   # enqueued_time_field = \"EnqueuedTime\"\n#   # offset_field = \"Offset\"\n#   # partition_id_tag = \"PartitionID\"\n#   # partition_key_tag = \"PartitionKey\"\n#   # iot_hub_device_connection_id_tag = \"IoTHubDeviceConnectionID\"\n#   # iot_hub_auth_generation_id_tag = \"IoTHubAuthGenerationID\"\n#   # iot_hub_connection_auth_method_tag = \"IoTHubConnectionAuthMethod\"\n#   # iot_hub_connection_module_id_tag = \"IoTHubConnectionModuleID\"\n#   # iot_hub_enqueued_time_field = \"IoTHubEnqueuedTime\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Run executable as long-running input plugin\n# [[inputs.execd]]\n#   ## Program to run as daemon\n#   command = [\"telegraf-smartctl\", \"-d\", \"/dev/sda\"]\n#\n#   ## Define how the process is signaled on each collection interval.\n#   ## Valid values are:\n#   ##   \"none\"   : Do not signal anything.\n#   ##              The process must output metrics by itself.\n#   ##   \"STDIN\"   : Send a newline on STDIN.\n#   ##   \"SIGHUP\"  : Send a HUP signal. Not available on Windows.\n#   ##   \"SIGUSR1\" : Send a USR1 signal. Not available on Windows.\n#   ##   \"SIGUSR2\" : Send a USR2 signal. Not available on Windows.\n#   signal = \"none\"\n#\n#   ## Delay before the process is restarted after an unexpected termination\n#   restart_delay = \"10s\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # gNMI telemetry input plugin\n# [[inputs.gnmi]]\n#  ## Address and port of the gNMI GRPC server\n#  addresses = [\"10.49.234.114:57777\"]\n#\n#  ## define credentials\n#  username = \"cisco\"\n#  password = \"cisco\"\n#\n#  ## gNMI encoding requested (one of: \"proto\", \"json\", \"json_ietf\", \"bytes\")\n#  # encoding = \"proto\"\n#\n#  ## redial in case of failures after\n#  redial = \"10s\"\n#\n#  ## enable client-side TLS and define CA to authenticate the device\n#  # enable_tls = true\n#  # tls_ca = \"/etc/telegraf/ca.pem\"\n#  # insecure_skip_verify = true\n#\n#  ## define client-side TLS certificate \u0026 key to authenticate to the device\n#  # tls_cert = \"/etc/telegraf/cert.pem\"\n#  # tls_key = \"/etc/telegraf/key.pem\"\n#\n#  ## gNMI subscription prefix (optional, can usually be left empty)\n#  ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n#  # origin = \"\"\n#  # prefix = \"\"\n#  # target = \"\"\n#\n#  ## Define additional aliases to map telemetry encoding paths to simple measurement names\n#  #[inputs.gnmi.aliases]\n#  #  ifcounters = \"openconfig:/interfaces/interface/state/counters\"\n#\n#  [[inputs.gnmi.subscription]]\n#   ## Name of the measurement that will be emitted\n#   name = \"ifcounters\"\n#\n#   ## Origin and path of the subscription\n#   ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n#   ##\n#   ## origin usually refers to a (YANG) data model implemented by the device\n#   ## and path to a specific substructure inside it that should be subscribed to (similar to an XPath)\n#   ## YANG models can be found e.g. here: https://github.com/YangModels/yang/tree/master/vendor/cisco/xr\n#   origin = \"openconfig-interfaces\"\n#   path = \"/interfaces/interface/state/counters\"\n#\n#   # Subscription mode (one of: \"target_defined\", \"sample\", \"on_change\") and interval\n#   subscription_mode = \"sample\"\n#   sample_interval = \"10s\"\n#\n#   ## Suppress redundant transmissions when measured values are unchanged\n#   # suppress_redundant = false\n#\n#   ## If suppression is enabled, send updates at least every X seconds anyway\n#   # heartbeat_interval = \"60s\"\n#\n#   #[[inputs.gnmi.subscription]]\n#     # name = \"descr\"\n#     # origin = \"openconfig-interfaces\"\n#     # path = \"/interfaces/interface/state/description\"\n#     # subscription_mode = \"on_change\"\n#\n#     ## If tag_only is set, the subscription in question will be utilized to maintain a map of\n#     ## tags to apply to other measurements emitted by the plugin, by matching path keys\n#     ## All fields from the tag-only subscription will be applied as tags to other readings,\n#     ## in the format \u003cname\u003e_\u003cfieldBase\u003e.\n#     # tag_only = true\n\n\n# # Accept metrics over InfluxDB 1.x HTTP API\n# [[inputs.http_listener]]\n#   ## DEPRECATED: The 'http_listener' plugin is deprecated in version 1.9.0, has been renamed to 'influxdb_listener', use 'inputs.influxdb_listener' or 'inputs.http_listener_v2' instead.\n#   ## Address and port to host InfluxDB listener on\n#   service_address = \":8186\"\n#\n#   ## maximum duration before timing out read of the request\n#   read_timeout = \"10s\"\n#   ## maximum duration before timing out write of the response\n#   write_timeout = \"10s\"\n#\n#   ## Maximum allowed HTTP request body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   max_body_size = \"32MiB\"\n#\n#   ## Optional tag name used to store the database.\n#   ## If the write has a database in the query string then it will be kept in this tag name.\n#   ## This tag can be used in downstream outputs.\n#   ## The default value of nothing means it will be off and the database will not be recorded.\n#   # database_tag = \"\"\n#\n#   ## If set the retention policy specified in the write query will be added as\n#   ## the value of this tag name.\n#   # retention_policy_tag = \"\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   tls_cert = \"/etc/telegraf/cert.pem\"\n#   tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional username and password to accept for HTTP basic authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # basic_username = \"foobar\"\n#   # basic_password = \"barfoo\"\n#\n#   ## Influx line protocol parser\n#   ## 'internal' is the default. 'upstream' is a newer parser that is faster\n#   ## and more memory efficient.\n#   # parser_type = \"internal\"\n\n\n# # Generic HTTP write listener\n# [[inputs.http_listener_v2]]\n#   ## Address and port to host HTTP listener on\n#   service_address = \":8080\"\n#\n#   ## Paths to listen to.\n#   # paths = [\"/telegraf\"]\n#\n#   ## Save path as http_listener_v2_path tag if set to true\n#   # path_tag = false\n#\n#   ## HTTP methods to accept.\n#   # methods = [\"POST\", \"PUT\"]\n#\n#   ## maximum duration before timing out read of the request\n#   # read_timeout = \"10s\"\n#   ## maximum duration before timing out write of the response\n#   # write_timeout = \"10s\"\n#\n#   ## Maximum allowed http request body size in bytes.\n#   ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n#   # max_body_size = \"500MB\"\n#\n#   ## Part of the request to consume.  Available options are \"body\" and\n#   ## \"query\".\n#   # data_source = \"body\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional username and password to accept for HTTP basic authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # basic_username = \"foobar\"\n#   # basic_password = \"barfoo\"\n#\n#   ## Optional setting to map http headers into tags\n#   ## If the http header is not present on the request, no corresponding tag will be added\n#   ## If multiple instances of the http header are present, only the first value will be used\n#   # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Accept metrics over InfluxDB 1.x HTTP API\n# [[inputs.influxdb_listener]]\n#   ## Address and port to host InfluxDB listener on\n#   service_address = \":8186\"\n#\n#   ## maximum duration before timing out read of the request\n#   read_timeout = \"10s\"\n#   ## maximum duration before timing out write of the response\n#   write_timeout = \"10s\"\n#\n#   ## Maximum allowed HTTP request body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   max_body_size = \"32MiB\"\n#\n#   ## Optional tag name used to store the database.\n#   ## If the write has a database in the query string then it will be kept in this tag name.\n#   ## This tag can be used in downstream outputs.\n#   ## The default value of nothing means it will be off and the database will not be recorded.\n#   # database_tag = \"\"\n#\n#   ## If set the retention policy specified in the write query will be added as\n#   ## the value of this tag name.\n#   # retention_policy_tag = \"\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   tls_cert = \"/etc/telegraf/cert.pem\"\n#   tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional username and password to accept for HTTP basic authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # basic_username = \"foobar\"\n#   # basic_password = \"barfoo\"\n#\n#   ## Influx line protocol parser\n#   ## 'internal' is the default. 'upstream' is a newer parser that is faster\n#   ## and more memory efficient.\n#   # parser_type = \"internal\"\n\n\n# # Accept metrics over InfluxDB 2.x HTTP API\n# [[inputs.influxdb_v2_listener]]\n#   ## Address and port to host InfluxDB listener on\n#   ## (Double check the port. Could be 9999 if using OSS Beta)\n#   service_address = \":8086\"\n#\n#   ## Maximum allowed HTTP request body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   # max_body_size = \"32MiB\"\n#\n#   ## Optional tag to determine the bucket.\n#   ## If the write has a bucket in the query string then it will be kept in this tag name.\n#   ## This tag can be used in downstream outputs.\n#   ## The default value of nothing means it will be off and the database will not be recorded.\n#   # bucket_tag = \"\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional token to accept for HTTP authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # token = \"some-long-shared-secret-token\"\n#\n#   ## Influx line protocol parser\n#   ## 'internal' is the default. 'upstream' is a newer parser that is faster\n#   ## and more memory efficient.\n#   # parser_type = \"internal\"\n\n\n# # Intel Performance Monitoring Unit plugin exposes Intel PMU metrics available through Linux Perf subsystem\n# [[inputs.intel_pmu]]\n#   ## List of filesystem locations of JSON files that contain PMU event definitions.\n#   event_definitions = [\"/var/cache/pmu/GenuineIntel-6-55-4-core.json\", \"/var/cache/pmu/GenuineIntel-6-55-4-uncore.json\"]\n#\n#   ## List of core events measurement entities. There can be more than one core_events sections.\n#   [[inputs.intel_pmu.core_events]]\n#     ## List of events to be counted. Event names shall match names from event_definitions files.\n#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n#     ## If absent, all core events from provided event_definitions are counted skipping unresolvable ones.\n#     events = [\"INST_RETIRED.ANY\", \"CPU_CLK_UNHALTED.THREAD_ANY:config1=0x4043200000000k\"]\n#\n#     ## Limits the counting of events to core numbers specified.\n#     ## If absent, events are counted on all cores.\n#     ## Single \"0\", multiple \"0,1,2\" and range \"0-2\" notation is supported for each array element.\n#     ##   example: cores = [\"0,2\", \"4\", \"12-16\"]\n#     cores = [\"0\"]\n#\n#     ## Indicator that plugin shall attempt to run core_events.events as a single perf group.\n#     ## If absent or set to false, each event is counted individually. Defaults to false.\n#     ## This limits the number of events that can be measured to a maximum of available hardware counters per core.\n#     ## Could vary depending on type of event, use of fixed counters.\n#     # perf_group = false\n#\n#     ## Optionally set a custom tag value that will be added to every measurement within this events group.\n#     ## Can be applied to any group of events, unrelated to perf_group setting.\n#     # events_tag = \"\"\n#\n#   ## List of uncore event measurement entities. There can be more than one uncore_events sections.\n#   [[inputs.intel_pmu.uncore_events]]\n#     ## List of events to be counted. Event names shall match names from event_definitions files.\n#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n#     ## If absent, all uncore events from provided event_definitions are counted skipping unresolvable ones.\n#     events = [\"UNC_CHA_CLOCKTICKS\", \"UNC_CHA_TOR_OCCUPANCY.IA_MISS\"]\n#\n#     ## Limits the counting of events to specified sockets.\n#     ## If absent, events are counted on all sockets.\n#     ## Single \"0\", multiple \"0,1\" and range \"0-1\" notation is supported for each array element.\n#     ##   example: sockets = [\"0-2\"]\n#     sockets = [\"0\"]\n#\n#     ## Indicator that plugin shall provide an aggregated value for multiple units of same type distributed in an uncore.\n#     ## If absent or set to false, events for each unit are exposed as separate metric. Defaults to false.\n#     # aggregate_uncore_units = false\n#\n#     ## Optionally set a custom tag value that will be added to every measurement within this events group.\n#     # events_tag = \"\"\n\n\n# # Intel Resource Director Technology plugin\n# [[inputs.intel_rdt]]\n# \t## Optionally set sampling interval to Nx100ms.\n# \t## This value is propagated to pqos tool. Interval format is defined by pqos itself.\n# \t## If not provided or provided 0, will be set to 10 = 10x100ms = 1s.\n# \t# sampling_interval = \"10\"\n# \t\n# \t## Optionally specify the path to pqos executable.\n# \t## If not provided, auto discovery will be performed.\n# \t# pqos_path = \"/usr/local/bin/pqos\"\n#\n# \t## Optionally specify if IPC and LLC_Misses metrics shouldn't be propagated.\n# \t## If not provided, default value is false.\n# \t# shortened_metrics = false\n# \t\n# \t## Specify the list of groups of CPU core(s) to be provided as pqos input.\n# \t## Mandatory if processes aren't set and forbidden if processes are specified.\n# \t## e.g. [\"0-3\", \"4,5,6\"] or [\"1-3,4\"]\n# \t# cores = [\"0-3\"]\n# \t\n# \t## Specify the list of processes for which Metrics will be collected.\n# \t## Mandatory if cores aren't set and forbidden if cores are specified.\n# \t## e.g. [\"qemu\", \"pmd\"]\n# \t# processes = [\"process\"]\n#\n# \t## Specify if the pqos process should be called with sudo.\n# \t## Mandatory if the telegraf process does not run as root.\n# \t# use_sudo = false\n\n\n# # Read JTI OpenConfig Telemetry from listed sensors\n# [[inputs.jti_openconfig_telemetry]]\n#   ## List of device addresses to collect telemetry from\n#   servers = [\"localhost:1883\"]\n#\n#   ## Authentication details. Username and password are must if device expects\n#   ## authentication. Client ID must be unique when connecting from multiple instances\n#   ## of telegraf to the same device\n#   username = \"user\"\n#   password = \"pass\"\n#   client_id = \"telegraf\"\n#\n#   ## Frequency to get data\n#   sample_frequency = \"1000ms\"\n#\n#   ## Sensors to subscribe for\n#   ## A identifier for each sensor can be provided in path by separating with space\n#   ## Else sensor path will be used as identifier\n#   ## When identifier is used, we can provide a list of space separated sensors.\n#   ## A single subscription will be created with all these sensors and data will\n#   ## be saved to measurement with this identifier name\n#   sensors = [\n#    \"/interfaces/\",\n#    \"collection /components/ /lldp\",\n#   ]\n#\n#   ## We allow specifying sensor group level reporting rate. To do this, specify the\n#   ## reporting rate in Duration at the beginning of sensor paths / collection\n#   ## name. For entries without reporting rate, we use configured sample frequency\n#   sensors = [\n#    \"1000ms customReporting /interfaces /lldp\",\n#    \"2000ms collection /components\",\n#    \"/interfaces\",\n#   ]\n#\n#   ## Optional TLS Config\n#   # enable_tls = true\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Delay between retry attempts of failed RPC calls or streams. Defaults to 1000ms.\n#   ## Failed streams/calls will not be retried if 0 is provided\n#   retry_delay = \"1000ms\"\n#\n#   ## To treat all string values as tags, set this to true\n#   str_as_tags = false\n\n\n# # Read metrics from Kafka topics\n# [[inputs.kafka_consumer]]\n#   ## Kafka brokers.\n#   brokers = [\"localhost:9092\"]\n#\n#   ## Topics to consume.\n#   topics = [\"telegraf\"]\n#\n#   ## When set this tag will be added to all metrics with the topic as the value.\n#   # topic_tag = \"\"\n#\n#   ## Optional Client id\n#   # client_id = \"Telegraf\"\n#\n#   ## Set the minimal supported Kafka version.  Setting this enables the use of new\n#   ## Kafka features and APIs.  Must be 0.10.2.0 or greater.\n#   ##   ex: version = \"1.1.0\"\n#   # version = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## SASL authentication credentials.  These settings should typically be used\n#   ## with TLS encryption enabled\n#   # sasl_username = \"kafka\"\n#   # sasl_password = \"secret\"\n#\n#   ## Optional SASL:\n#   ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n#   ## (defaults to PLAIN)\n#   # sasl_mechanism = \"\"\n#\n#   ## used if sasl_mechanism is GSSAPI (experimental)\n#   # sasl_gssapi_service_name = \"\"\n#   # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n#   # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n#   # sasl_gssapi_kerberos_config_path = \"/\"\n#   # sasl_gssapi_realm = \"realm\"\n#   # sasl_gssapi_key_tab_path = \"\"\n#   # sasl_gssapi_disable_pafxfast = false\n#\n#   ## used if sasl_mechanism is OAUTHBEARER (experimental)\n#   # sasl_access_token = \"\"\n#\n#   ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n#   # sasl_version = 1\n#\n#   # Disable Kafka metadata full fetch\n#   # metadata_full = false\n#\n#   ## Name of the consumer group.\n#   # consumer_group = \"telegraf_metrics_consumers\"\n#\n#   ## Compression codec represents the various compression codecs recognized by\n#   ## Kafka in messages.\n#   ##  0 : None\n#   ##  1 : Gzip\n#   ##  2 : Snappy\n#   ##  3 : LZ4\n#   ##  4 : ZSTD\n#    # compression_codec = 0\n#\n#   ## Initial offset position; one of \"oldest\" or \"newest\".\n#   # offset = \"oldest\"\n#\n#   ## Consumer group partition assignment strategy; one of \"range\", \"roundrobin\" or \"sticky\".\n#   # balance_strategy = \"range\"\n#\n#   ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n#   ## larger messages are dropped\n#   max_message_len = 1000000\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Maximum amount of time the consumer should take to process messages. If\n#   ## the debug log prints messages from sarama about 'abandoning subscription\n#   ## to [topic] because consuming was taking too long', increase this value to\n#   ## longer than the time taken by the output plugin(s).\n#   ##\n#   ## Note that the effective timeout could be between 'max_processing_time' and\n#   ## '2 * max_processing_time'.\n#   # max_processing_time = \"100ms\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read metrics from Kafka topic(s)\n# [[inputs.kafka_consumer_legacy]]\n#   ## DEPRECATED: The 'kafka_consumer_legacy' plugin is deprecated in version 1.4.0, use 'inputs.kafka_consumer' instead, NOTE: 'kafka_consumer' only supports Kafka v0.8+.\n#   ## topic(s) to consume\n#   topics = [\"telegraf\"]\n#\n#   ## an array of Zookeeper connection strings\n#   zookeeper_peers = [\"localhost:2181\"]\n#\n#   ## Zookeeper Chroot\n#   zookeeper_chroot = \"\"\n#\n#   ## the name of the consumer group\n#   consumer_group = \"telegraf_metrics_consumers\"\n#\n#   ## Offset (must be either \"oldest\" or \"newest\")\n#   offset = \"oldest\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n#   ## larger messages are dropped\n#   max_message_len = 65536\n\n\n# # Configuration for the AWS Kinesis input.\n# [[inputs.kinesis_consumer]]\n#   ## Amazon REGION of kinesis endpoint.\n#   region = \"ap-southeast-2\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   # access_key = \"\"\n#   # secret_key = \"\"\n#   # token = \"\"\n#   # role_arn = \"\"\n#   # web_identity_token_file = \"\"\n#   # role_session_name = \"\"\n#   # profile = \"\"\n#   # shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Kinesis StreamName must exist prior to starting telegraf.\n#   streamname = \"StreamName\"\n#\n#   ## Shard iterator type (only 'TRIM_HORIZON' and 'LATEST' currently supported)\n#   # shard_iterator_type = \"TRIM_HORIZON\"\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ##\n#   ## The content encoding of the data from kinesis\n#   ## If you are processing a cloudwatch logs kinesis stream then set this to \"gzip\"\n#   ## as AWS compresses cloudwatch log data before it is sent to kinesis (aws\n#   ## also base64 encodes the zip byte data before pushing to the stream.  The base64 decoding\n#   ## is done automatically by the golang sdk, as data is read from kinesis)\n#   ##\n#   # content_encoding = \"identity\"\n#\n#   ## Optional\n#   ## Configuration for a dynamodb checkpoint\n#   [inputs.kinesis_consumer.checkpoint_dynamodb]\n# \t## unique name for this consumer\n# \tapp_name = \"default\"\n# \ttable_name = \"default\"\n\n\n# # Listener capable of handling KNX bus messages provided through a KNX-IP Interface.\n# [[inputs.knx_listener]]\n#   ## Type of KNX-IP interface.\n#   ## Can be either \"tunnel\" or \"router\".\n#   # service_type = \"tunnel\"\n#\n#   ## Address of the KNX-IP interface.\n#   service_address = \"localhost:3671\"\n#\n#   ## Measurement definition(s)\n#   # [[inputs.knx_listener.measurement]]\n#   #   ## Name of the measurement\n#   #   name = \"temperature\"\n#   #   ## Datapoint-Type (DPT) of the KNX messages\n#   #   dpt = \"9.001\"\n#   #   ## List of Group-Addresses (GAs) assigned to the measurement\n#   #   addresses = [\"5/5/1\"]\n#\n#   # [[inputs.knx_listener.measurement]]\n#   #   name = \"illumination\"\n#   #   dpt = \"9.004\"\n#   #   addresses = [\"5/5/3\"]\n\n\n# # Read metrics off Arista LANZ, via socket\n# [[inputs.lanz]]\n#   ## URL to Arista LANZ endpoint\n#   servers = [\n#     \"tcp://127.0.0.1:50001\"\n#   ]\n\n\n\n# # Read metrics from MQTT topic(s)\n# [[inputs.mqtt_consumer]]\n#   ## Broker URLs for the MQTT server or cluster.  To connect to multiple\n#   ## clusters or standalone servers, use a separate plugin instance.\n#   ##   example: servers = [\"tcp://localhost:1883\"]\n#   ##            servers = [\"ssl://localhost:1883\"]\n#   ##            servers = [\"ws://localhost:1883\"]\n#   servers = [\"tcp://127.0.0.1:1883\"]\n#   ## Topics that will be subscribed to.\n#   topics = [\n#     \"telegraf/host01/cpu\",\n#     \"telegraf/+/mem\",\n#     \"sensors/#\",\n#   ]\n#   # topic_fields = \"_/_/_/temperature\"\n#   ## The message topic will be stored in a tag specified by this value.  If set\n#   ## to the empty string no topic tag will be created.\n#   # topic_tag = \"topic\"\n#   ## QoS policy for messages\n#   ##   0 = at most once\n#   ##   1 = at least once\n#   ##   2 = exactly once\n#   ##\n#   ## When using a QoS of 1 or 2, you should enable persistent_session to allow\n#   ## resuming unacknowledged messages.\n#   # qos = 0\n#   ## Connection timeout for initial connection in seconds\n#   # connection_timeout = \"30s\"\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#   ## Persistent session disables clearing of the client session on connection.\n#   ## In order for this option to work you must also set client_id to identify\n#   ## the client.  To receive messages that arrived while the client is offline,\n#   ## also set the qos option to 1 or 2 and don't forget to also set the QoS when\n#   ## publishing.\n#   # persistent_session = false\n#   ## If unset, a random client ID will be generated.\n#   # client_id = \"\"\n#   ## Username and password to connect MQTT server.\n#   # username = \"telegraf\"\n#   # password = \"metricsmetricsmetricsmetrics\"\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#   ## Enable extracting tag values from MQTT topics\n#   ## _ denotes an ignored entry in the topic path\n#   ## [[inputs.mqtt_consumer.topic_parsing]]\n#   ##  topic = \"\"\n#   ##  measurement = \"\"\n#   ##  tags = \"\"\n#   ##  fields = \"\"\n#   ## [inputs.mqtt_consumer.topic_parsing.types]\n#   ##\n\n\n# # Read metrics from NATS subject(s)\n# [[inputs.nats_consumer]]\n#   ## urls of NATS servers\n#   servers = [\"nats://localhost:4222\"]\n#\n#   ## subject(s) to consume\n#   subjects = [\"telegraf\"]\n#\n#   ## name a queue group\n#   queue_group = \"telegraf_consumers\"\n#\n#   ## Optional credentials\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional NATS 2.0 and NATS NGS compatible user credentials\n#   # credentials = \"/etc/telegraf/nats.creds\"\n#\n#   ## Use Transport Layer Security\n#   # secure = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Sets the limits for pending msgs and bytes for each subscription\n#   ## These shouldn't need to be adjusted except in very high throughput scenarios\n#   # pending_message_limit = 65536\n#   # pending_bytes_limit = 67108864\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read NSQ topic for metrics.\n# [[inputs.nsq_consumer]]\n#   ## An array representing the NSQD TCP HTTP Endpoints\n#   nsqd = [\"localhost:4150\"]\n#\n#   ## An array representing the NSQLookupd HTTP Endpoints\n#   nsqlookupd = [\"localhost:4161\"]\n#   topic = \"telegraf\"\n#   channel = \"consumer\"\n#   max_in_flight = 100\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Receive OpenTelemetry traces, metrics, and logs over gRPC\n# [[inputs.opentelemetry]]\n#   ## Override the default (0.0.0.0:4317) destination OpenTelemetry gRPC service\n#   ## address:port\n#   # service_address = \"0.0.0.0:4317\"\n#\n#   ## Override the default (5s) new connection timeout\n#   # timeout = \"5s\"\n#\n#   ## Override the default (prometheus-v1) metrics schema.\n#   ## Supports: \"prometheus-v1\", \"prometheus-v2\"\n#   ## For more information about the alternatives, read the Prometheus input\n#   ## plugin notes.\n#   # metrics_schema = \"prometheus-v1\"\n#\n#   ## Optional TLS Config.\n#   ## For advanced options: https://github.com/influxdata/telegraf/blob/v1.18.3/docs/TLS.md\n#   ##\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#   ## Add service certificate and key.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n\n\n# # Read metrics from one or many pgbouncer servers\n# [[inputs.pgbouncer]]\n#   ## specify address via a url matching:\n#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n#   ##       ?sslmode=[disable|verify-ca|verify-full]\n#   ## or a simple string:\n#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n#   ##\n#   ## All connection parameters are optional.\n#   ##\n#   address = \"host=localhost user=pgbouncer sslmode=disable\"\n\n\n# # Read metrics from one or many postgresql servers\n# [[inputs.postgresql]]\n#   ## specify address via a url matching:\n#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n#   ##       ?sslmode=[disable|verify-ca|verify-full]\n#   ## or a simple string:\n#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n#   ##\n#   ## All connection parameters are optional.\n#   ##\n#   ## Without the dbname parameter, the driver will default to a database\n#   ## with the same name as the user. This dbname is just for instantiating a\n#   ## connection with the server and doesn't restrict the databases we are trying\n#   ## to grab metrics for.\n#   ##\n#   address = \"host=localhost user=postgres sslmode=disable\"\n#   ## A custom name for the database that will be used as the \"server\" tag in the\n#   ## measurement output. If not specified, a default one generated from\n#   ## the connection address is used.\n#   # outputaddress = \"db01\"\n#\n#   ## connection configuration.\n#   ## maxlifetime - specify the maximum lifetime of a connection.\n#   ## default is forever (0s)\n#   max_lifetime = \"0s\"\n#\n#   ## A  list of databases to explicitly ignore.  If not specified, metrics for all\n#   ## databases are gathered.  Do NOT use with the 'databases' option.\n#   # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n#\n#   ## A list of databases to pull metrics about. If not specified, metrics for all\n#   ## databases are gathered.  Do NOT use with the 'ignored_databases' option.\n#   # databases = [\"app_production\", \"testing\"]\n#\n#   ## Whether to use prepared statements when connecting to the database.\n#   ## This should be set to false when connecting through a PgBouncer instance\n#   ## with pool_mode set to transaction.\n#   # prepared_statements = true\n\n\n# # Read metrics from one or many postgresql servers\n# [[inputs.postgresql_extensible]]\n#   ## specify address via a url matching:\n#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n#   ##       ?sslmode=[disable|verify-ca|verify-full]\n#   ## or a simple string:\n#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n#   #\n#   ## All connection parameters are optional.  #\n#   ## Without the dbname parameter, the driver will default to a database\n#   ## with the same name as the user. This dbname is just for instantiating a\n#   ## connection with the server and doesn't restrict the databases we are trying\n#   ## to grab metrics for.\n#   #\n#   address = \"host=localhost user=postgres sslmode=disable\"\n#\n#   ## connection configuration.\n#   ## maxlifetime - specify the maximum lifetime of a connection.\n#   ## default is forever (0s)\n#   max_lifetime = \"0s\"\n#\n#   ## Whether to use prepared statements when connecting to the database.\n#   ## This should be set to false when connecting through a PgBouncer instance\n#   ## with pool_mode set to transaction.\n#   # prepared_statements = true\n#\n#   ## A list of databases to pull metrics about. If not specified, metrics for all\n#   ## databases are gathered.\n#   ## databases = [\"app_production\", \"testing\"]\n#   #\n#   ## A custom name for the database that will be used as the \"server\" tag in the\n#   ## measurement output. If not specified, a default one generated from\n#   ## the connection address is used.\n#   # outputaddress = \"db01\"\n#   #\n#   ## Define the toml config where the sql queries are stored\n#   ## New queries can be added, if the withdbname is set to true and there is no\n#   ## databases defined in the 'databases field', the sql query is ended by a\n#   ## 'is not null' in order to make the query succeed.\n#   ## Example :\n#   ## The sqlquery : \"SELECT * FROM pg_stat_database where datname\" become\n#   ## \"SELECT * FROM pg_stat_database where datname IN ('postgres', 'pgbench')\"\n#   ## because the databases variable was set to ['postgres', 'pgbench' ] and the\n#   ## withdbname was true. Be careful that if the withdbname is set to false you\n#   ## don't have to define the where clause (aka with the dbname) the tagvalue\n#   ## field is used to define custom tags (separated by commas)\n#   ## The optional \"measurement\" value can be used to override the default\n#   ## output measurement name (\"postgresql\").\n#   ##\n#   ## The script option can be used to specify the .sql file path.\n#   ## If script and sqlquery options specified at same time, sqlquery will be used\n#   ##\n#   ## the tagvalue field is used to define custom tags (separated by comas).\n#   ## the query is expected to return columns which match the names of the\n#   ## defined tags. The values in these columns must be of a string-type,\n#   ## a number-type or a blob-type.\n#   ##\n#   ## The timestamp field is used to override the data points timestamp value. By\n#   ## default, all rows inserted with current time. By setting a timestamp column,\n#   ## the row will be inserted with that column's value.\n#   ##\n#   ## Structure :\n#   ## [[inputs.postgresql_extensible.query]]\n#   ##   sqlquery string\n#   ##   version string\n#   ##   withdbname boolean\n#   ##   tagvalue string (comma separated)\n#   ##   measurement string\n#   ##   timestamp string\n#   [[inputs.postgresql_extensible.query]]\n#     sqlquery=\"SELECT * FROM pg_stat_database\"\n#     version=901\n#     withdbname=false\n#     tagvalue=\"\"\n#     measurement=\"\"\n#   [[inputs.postgresql_extensible.query]]\n#     sqlquery=\"SELECT * FROM pg_stat_bgwriter\"\n#     version=901\n#     withdbname=false\n#     tagvalue=\"postgresql.stats\"\n\n\n# # Read metrics from one or many prometheus clients\n# [[inputs.prometheus]]\n#   ## An array of urls to scrape metrics from.\n#   urls = [\"http://localhost:9100/metrics\"]\n#\n#   ## Metric version controls the mapping from Prometheus metrics into\n#   ## Telegraf metrics.  When using the prometheus_client output, use the same\n#   ## value in both plugins to ensure metrics are round-tripped without\n#   ## modification.\n#   ##\n#   ##   example: metric_version = 1;\n#   ##            metric_version = 2; recommended version\n#   # metric_version = 1\n#\n#   ## Url tag name (tag containing scrapped url. optional, default is \"url\")\n#   # url_tag = \"url\"\n#\n#   ## Whether the timestamp of the scraped metrics will be ignored.\n#   ## If set to true, the gather time will be used.\n#   # ignore_timestamp = false\n#\n#   ## An array of Kubernetes services to scrape metrics from.\n#   # kubernetes_services = [\"http://my-service-dns.my-namespace:9100/metrics\"]\n#\n#   ## Kubernetes config file to create client from.\n#   # kube_config = \"/path/to/kubernetes.config\"\n#\n#   ## Scrape Kubernetes pods for the following prometheus annotations:\n#   ## - prometheus.io/scrape: Enable scraping for this pod\n#   ## - prometheus.io/scheme: If the metrics endpoint is secured then you will need to\n#   ##     set this to 'https' \u0026 most likely set the tls config.\n#   ## - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.\n#   ## - prometheus.io/port: If port is not 9102 use this annotation\n#   # monitor_kubernetes_pods = true\n#   ## Get the list of pods to scrape with either the scope of\n#   ## - cluster: the kubernetes watch api (default, no need to specify)\n#   ## - node: the local cadvisor api; for scalability. Note that the config node_ip or the environment variable NODE_IP must be set to the host IP.\n#   # pod_scrape_scope = \"cluster\"\n#   ## Only for node scrape scope: node IP of the node that telegraf is running on.\n#   ## Either this config or the environment variable NODE_IP must be set.\n#   # node_ip = \"10.180.1.1\"\n# \t## Only for node scrape scope: interval in seconds for how often to get updated pod list for scraping.\n# \t## Default is 60 seconds.\n# \t# pod_scrape_interval = 60\n#   ## Restricts Kubernetes monitoring to a single namespace\n#   ##   ex: monitor_kubernetes_pods_namespace = \"default\"\n#   # monitor_kubernetes_pods_namespace = \"\"\n#   # label selector to target pods which have the label\n#   # kubernetes_label_selector = \"env=dev,app=nginx\"\n#   # field selector to target pods\n#   # eg. To scrape pods on a specific node\n#   # kubernetes_field_selector = \"spec.nodeName=$HOSTNAME\"\n#\n#   ## Scrape Services available in Consul Catalog\n#   # [inputs.prometheus.consul]\n#   #   enabled = true\n#   #   agent = \"http://localhost:8500\"\n#   #   query_interval = \"5m\"\n#\n#   #   [[inputs.prometheus.consul.query]]\n#   #     name = \"a service name\"\n#   #     tag = \"a service tag\"\n#   #     url = 'http://{{if ne .ServiceAddress \"\"}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}'\n#   #     [inputs.prometheus.consul.query.tags]\n#   #       host = \"{{.Node}}\"\n#\n#   ## Use bearer token for authorization. ('bearer_token' takes priority)\n#   # bearer_token = \"/path/to/bearer/token\"\n#   ## OR\n#   # bearer_token_string = \"abc_123\"\n#\n#   ## HTTP Basic Authentication username and password. ('bearer_token' and\n#   ## 'bearer_token_string' take priority)\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Specify timeout duration for slower prometheus clients (default is 3s)\n#   # response_timeout = \"3s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # RAS plugin exposes counter metrics for Machine Check Errors provided by RASDaemon (sqlite3 output is required).\n# [[inputs.ras]]\n#   ## Optional path to RASDaemon sqlite3 database.\n#   ## Default: /var/lib/rasdaemon/ras-mc_event.db\n#   # db_path = \"\"\n\n\n# # Riemann protobuff listener.\n# [[inputs.riemann_listener]]\n#   ## URL to listen on.\n#   ## Default is \"tcp://:5555\"\n#   # service_address = \"tcp://:8094\"\n#   # service_address = \"tcp://127.0.0.1:http\"\n#   # service_address = \"tcp4://:8094\"\n#   # service_address = \"tcp6://:8094\"\n#   # service_address = \"tcp6://[2001:db8::1]:8094\"\n#\n#   ## Maximum number of concurrent connections.\n#   ## 0 (default) is unlimited.\n#   # max_connections = 1024\n#   ## Read timeout.\n#   ## 0 (default) is unlimited.\n#   # read_timeout = \"30s\"\n#   ## Optional TLS configuration.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key  = \"/etc/telegraf/key.pem\"\n#   ## Enables client authentication if set.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#   ## Maximum socket buffer size (in bytes when no unit specified).\n#   # read_buffer_size = \"64KiB\"\n#   ## Period between keep alive probes.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n\n\n# # SFlow V5 Protocol Listener\n# [[inputs.sflow]]\n#   ## Address to listen for sFlow packets.\n#   ##   example: service_address = \"udp://:6343\"\n#   ##            service_address = \"udp4://:6343\"\n#   ##            service_address = \"udp6://:6343\"\n#   service_address = \"udp://:6343\"\n#\n#   ## Set the size of the operating system's receive buffer.\n#   ##   example: read_buffer_size = \"64KiB\"\n#   # read_buffer_size = \"\"\n\n\n# # Receive SNMP traps\n# [[inputs.snmp_trap]]\n#   ## Transport, local address, and port to listen on.  Transport must\n#   ## be \"udp://\".  Omit local address to listen on all interfaces.\n#   ##   example: \"udp://127.0.0.1:1234\"\n#   ##\n#   ## Special permissions may be required to listen on a port less than\n#   ## 1024.  See README.md for details\n#   ##\n#   # service_address = \"udp://:162\"\n#   ##\n#   ## Path to mib files\n#   # path = [\"/usr/share/snmp/mibs\"]\n#   ##\n#   ## Snmp version, defaults to 2c\n#   # version = \"2c\"\n#   ## SNMPv3 authentication and encryption options.\n#   ##\n#   ## Security Name.\n#   # sec_name = \"myuser\"\n#   ## Authentication protocol; one of \"MD5\", \"SHA\" or \"\".\n#   # auth_protocol = \"MD5\"\n#   ## Authentication password.\n#   # auth_password = \"pass\"\n#   ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n#   # sec_level = \"authNoPriv\"\n#   ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\", \"AES192\", \"AES192C\", \"AES256\", \"AES256C\" or \"\".\n#   # priv_protocol = \"\"\n#   ## Privacy password used for encrypted messages.\n#   # priv_password = \"\"\n\n\n# # Generic socket listener capable of handling multiple socket types.\n# [[inputs.socket_listener]]\n#   ## URL to listen on\n#   # service_address = \"tcp://:8094\"\n#   # service_address = \"tcp://127.0.0.1:http\"\n#   # service_address = \"tcp4://:8094\"\n#   # service_address = \"tcp6://:8094\"\n#   # service_address = \"tcp6://[2001:db8::1]:8094\"\n#   # service_address = \"udp://:8094\"\n#   # service_address = \"udp4://:8094\"\n#   # service_address = \"udp6://:8094\"\n#   # service_address = \"unix:///tmp/telegraf.sock\"\n#   # service_address = \"unixgram:///tmp/telegraf.sock\"\n#\n#   ## Change the file mode bits on unix sockets.  These permissions may not be\n#   ## respected by some platforms, to safely restrict write permissions it is best\n#   ## to place the socket into a directory that has previously been created\n#   ## with the desired permissions.\n#   ##   ex: socket_mode = \"777\"\n#   # socket_mode = \"\"\n#\n#   ## Maximum number of concurrent connections.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   ## 0 (default) is unlimited.\n#   # max_connections = 1024\n#\n#   ## Read timeout.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   ## 0 (default) is unlimited.\n#   # read_timeout = \"30s\"\n#\n#   ## Optional TLS configuration.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key  = \"/etc/telegraf/key.pem\"\n#   ## Enables client authentication if set.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Maximum socket buffer size (in bytes when no unit specified).\n#   ## For stream sockets, once the buffer fills up, the sender will start backing up.\n#   ## For datagram sockets, once the buffer fills up, metrics will start dropping.\n#   ## Defaults to the OS default.\n#   # read_buffer_size = \"64KiB\"\n#\n#   ## Period between keep alive probes.\n#   ## Only applies to TCP sockets.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   # data_format = \"influx\"\n#\n#   ## Content encoding for message payloads, can be set to \"gzip\" to or\n#   ## \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n\n\n# # Read metrics from SQL queries\n# [[inputs.sql]]\n#   ## Database Driver\n#   ## See https://github.com/influxdata/telegraf/blob/master/docs/SQL_DRIVERS_INPUT.md for\n#   ## a list of supported drivers.\n#   driver = \"mysql\"\n#\n#   ## Data source name for connecting\n#   ## The syntax and supported options depends on selected driver.\n#   dsn = \"username:password@mysqlserver:3307/dbname?param=value\"\n#\n#   ## Timeout for any operation\n#   ## Note that the timeout for queries is per query not per gather.\n#   # timeout = \"5s\"\n#\n#   ## Connection time limits\n#   ## By default the maximum idle time and maximum lifetime of a connection is unlimited, i.e. the connections\n#   ## will not be closed automatically. If you specify a positive time, the connections will be closed after\n#   ## idleing or existing for at least that amount of time, respectively.\n#   # connection_max_idle_time = \"0s\"\n#   # connection_max_life_time = \"0s\"\n#\n#   ## Connection count limits\n#   ## By default the number of open connections is not limited and the number of maximum idle connections\n#   ## will be inferred from the number of queries specified. If you specify a positive number for any of the\n#   ## two options, connections will be closed when reaching the specified limit. The number of idle connections\n#   ## will be clipped to the maximum number of connections limit if any.\n#   # connection_max_open = 0\n#   # connection_max_idle = auto\n#\n#   [[inputs.sql.query]]\n#     ## Query to perform on the server\n#     query=\"SELECT user,state,latency,score FROM Scoreboard WHERE application \u003e 0\"\n#     ## Alternatively to specifying the query directly you can select a file here containing the SQL query.\n#     ## Only one of 'query' and 'query_script' can be specified!\n#     # query_script = \"/path/to/sql/script.sql\"\n#\n#     ## Name of the measurement\n#     ## In case both measurement and 'measurement_col' are given, the latter takes precedence.\n#     # measurement = \"sql\"\n#\n#     ## Column name containing the name of the measurement\n#     ## If given, this will take precedence over the 'measurement' setting. In case a query result\n#     ## does not contain the specified column, we fall-back to the 'measurement' setting.\n#     # measurement_column = \"\"\n#\n#     ## Column name containing the time of the measurement\n#     ## If ommited, the time of the query will be used.\n#     # time_column = \"\"\n#\n#     ## Format of the time contained in 'time_col'\n#     ## The time must be 'unix', 'unix_ms', 'unix_us', 'unix_ns', or a golang time format.\n#     ## See https://golang.org/pkg/time/#Time.Format for details.\n#     # time_format = \"unix\"\n#\n#     ## Column names containing tags\n#     ## An empty include list will reject all columns and an empty exclude list will not exclude any column.\n#     ## I.e. by default no columns will be returned as tag and the tags are empty.\n#     # tag_columns_include = []\n#     # tag_columns_exclude = []\n#\n# \t\t## Column names containing fields (explicit types)\n#     ## Convert the given columns to the corresponding type. Explicit type conversions take precedence over\n# \t\t## the automatic (driver-based) conversion below.\n# \t\t## NOTE: Columns should not be specified for multiple types or the resulting type is undefined.\n#     # field_columns_float = []\n#     # field_columns_int = []\n# \t\t# field_columns_uint = []\n# \t\t# field_columns_bool = []\n# \t\t# field_columns_string = []\n#\n#     ## Column names containing fields (automatic types)\n#     ## An empty include list is equivalent to '[*]' and all returned columns will be accepted. An empty\n#     ## exclude list will not exclude any column. I.e. by default all columns will be returned as fields.\n#     ## NOTE: We rely on the database driver to perform automatic datatype conversion.\n#     # field_columns_include = []\n#     # field_columns_exclude = []\n\n\n# # Read metrics from Microsoft SQL Server\n# [[inputs.sqlserver]]\n# ## Specify instances to monitor with a list of connection strings.\n# ## All connection parameters are optional.\n# ## By default, the host is localhost, listening on default port, TCP 1433.\n# ##   for Windows, the user is the currently running AD user (SSO).\n# ##   See https://github.com/denisenkom/go-mssqldb for detailed connection\n# ##   parameters, in particular, tls connections can be created like so:\n# ##   \"encrypt=true;certificate=\u003ccert\u003e;hostNameInCertificate=\u003cSqlServer host fqdn\u003e\"\n# servers = [\n#   \"Server=192.168.1.10;Port=1433;User Id=\u003cuser\u003e;Password=\u003cpw\u003e;app name=telegraf;log=1;\",\n# ]\n#\n# ## Authentication method\n# ## valid methods: \"connection_string\", \"AAD\"\n# # auth_method = \"connection_string\"\n#\n# ## \"database_type\" enables a specific set of queries depending on the database type.\n# ## In the config file, the sql server plugin section should be repeated each with a set of servers for a specific database_type.\n# ## Possible values for database_type are - \"SQLServer\" or \"AzureSQLDB\" or \"AzureSQLManagedInstance\" or \"AzureSQLPool\"\n#\n# database_type = \"SQLServer\"\n#\n# ## A list of queries to include. If not specified, all the below listed queries are used.\n# include_query = []\n#\n# ## A list of queries to explicitly ignore.\n# exclude_query = [\"SQLServerAvailabilityReplicaStates\", \"SQLServerDatabaseReplicaStates\"]\n#\n# ## Queries enabled by default for database_type = \"SQLServer\" are -\n# ## SQLServerPerformanceCounters, SQLServerWaitStatsCategorized, SQLServerDatabaseIO, SQLServerProperties, SQLServerMemoryClerks,\n# ## SQLServerSchedulers, SQLServerRequests, SQLServerVolumeSpace, SQLServerCpu, SQLServerAvailabilityReplicaStates, SQLServerDatabaseReplicaStates\n#\n# ## Queries enabled by default for database_type = \"AzureSQLDB\" are -\n# ## AzureSQLDBResourceStats, AzureSQLDBResourceGovernance, AzureSQLDBWaitStats, AzureSQLDBDatabaseIO, AzureSQLDBServerProperties,\n# ## AzureSQLDBOsWaitstats, AzureSQLDBMemoryClerks, AzureSQLDBPerformanceCounters, AzureSQLDBRequests, AzureSQLDBSchedulers\n#\n# ## Queries enabled by default for database_type = \"AzureSQLManagedInstance\" are -\n# ## AzureSQLMIResourceStats, AzureSQLMIResourceGovernance, AzureSQLMIDatabaseIO, AzureSQLMIServerProperties, AzureSQLMIOsWaitstats,\n# ## AzureSQLMIMemoryClerks, AzureSQLMIPerformanceCounters, AzureSQLMIRequests, AzureSQLMISchedulers\n#\n# ## Queries enabled by default for database_type = \"AzureSQLPool\" are -\n# ## AzureSQLPoolResourceStats, AzureSQLPoolResourceGovernance, AzureSQLPoolDatabaseIO, AzureSQLPoolWaitStats,\n# ## AzureSQLPoolMemoryClerks, AzureSQLPoolPerformanceCounters, AzureSQLPoolSchedulers\n\n\n# # Statsd UDP/TCP Server\n# [[inputs.statsd]]\n#   ## Protocol, must be \"tcp\", \"udp\", \"udp4\" or \"udp6\" (default=udp)\n#   protocol = \"udp\"\n#\n#   ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)\n#   max_tcp_connections = 250\n#\n#   ## Enable TCP keep alive probes (default=false)\n#   tcp_keep_alive = false\n#\n#   ## Specifies the keep-alive period for an active network connection.\n#   ## Only applies to TCP sockets and will be ignored if tcp_keep_alive is false.\n#   ## Defaults to the OS configuration.\n#   # tcp_keep_alive_period = \"2h\"\n#\n#   ## Address and port to host UDP listener on\n#   service_address = \":8125\"\n#\n#   ## The following configuration options control when telegraf clears it's cache\n#   ## of previous values. If set to false, then telegraf will only clear it's\n#   ## cache when the daemon is restarted.\n#   ## Reset gauges every interval (default=true)\n#   delete_gauges = true\n#   ## Reset counters every interval (default=true)\n#   delete_counters = true\n#   ## Reset sets every interval (default=true)\n#   delete_sets = true\n#   ## Reset timings \u0026 histograms every interval (default=true)\n#   delete_timings = true\n#\n#   ## Percentiles to calculate for timing \u0026 histogram stats\n#   percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0]\n#\n#   ## separator to use between elements of a statsd metric\n#   metric_separator = \"_\"\n#\n#   ## Parses tags in the datadog statsd format\n#   ## http://docs.datadoghq.com/guides/dogstatsd/\n#   parse_data_dog_tags = false\n#\n#   ## Parses datadog extensions to the statsd format\n#   datadog_extensions = false\n#\n#   ## Parses distributions metric as specified in the datadog statsd format\n#   ## https://docs.datadoghq.com/developers/metrics/types/?tab=distribution#definition\n#   datadog_distributions = false\n#\n#   ## Statsd data translation templates, more info can be read here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md\n#   # templates = [\n#   #     \"cpu.* measurement*\"\n#   # ]\n#\n#   ## Number of UDP messages allowed to queue up, once filled,\n#   ## the statsd server will start dropping packets\n#   allowed_pending_messages = 10000\n#\n#   ## Number of timing/histogram values to track per-measurement in the\n#   ## calculation of percentiles. Raising this limit increases the accuracy\n#   ## of percentiles but also increases the memory usage and cpu time.\n#   percentile_limit = 1000\n#\n#   ## Max duration (TTL) for each metric to stay cached/reported without being updated.\n#   #max_ttl = \"1000h\"\n#\n#   ## Sanitize name method\n#   ## By default, telegraf will pass names directly as they are received.\n#   ## However, upstream statsd now does sanitization of names which can be\n#   ## enabled by using the \"upstream\" method option. This option will a) replace\n#   ## white space with '_', replace '/' with '-', and remove charachters not\n#   ## matching 'a-zA-Z_\\-0-9\\.;='.\n#   #sanitize_name_method = \"\"\n\n\n# # Suricata stats and alerts plugin\n# [[inputs.suricata]]\n#   ## Data sink for Suricata stats and alerts logs\n#   # This is expected to be a filename of a\n#   # unix socket to be created for listening.\n#   source = \"/var/run/suricata-stats.sock\"\n#\n#   # Delimiter for flattening field keys, e.g. subitem \"alert\" of \"detect\"\n#   # becomes \"detect_alert\" when delimiter is \"_\".\n#   delimiter = \"_\"\n#\n#   ## Detect alert logs\n#   # alerts = false\n\n\n# # Accepts syslog messages following RFC5424 format with transports as per RFC5426, RFC5425, or RFC6587\n# [[inputs.syslog]]\n#   ## Specify an ip or hostname with port - eg., tcp://localhost:6514, tcp://10.0.0.1:6514\n#   ## Protocol, address and port to host the syslog receiver.\n#   ## If no host is specified, then localhost is used.\n#   ## If no port is specified, 6514 is used (RFC5425#section-4.1).\n#   server = \"tcp://:6514\"\n#\n#   ## TLS Config\n#   # tls_allowed_cacerts = [\"/etc/telegraf/ca.pem\"]\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Period between keep alive probes.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   # keep_alive_period = \"5m\"\n#\n#   ## Maximum number of concurrent connections (default = 0).\n#   ## 0 means unlimited.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   # max_connections = 1024\n#\n#   ## Read timeout is the maximum time allowed for reading a single message (default = 5s).\n#   ## 0 means unlimited.\n#   # read_timeout = \"5s\"\n#\n#   ## The framing technique with which it is expected that messages are transported (default = \"octet-counting\").\n#   ## Whether the messages come using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n#   ## or the non-transparent framing technique (RFC6587#section-3.4.2).\n#   ## Must be one of \"octet-counting\", \"non-transparent\".\n#   # framing = \"octet-counting\"\n#\n#   ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n#   ## Must be one of \"LF\", or \"NUL\".\n#   # trailer = \"LF\"\n#\n#   ## Whether to parse in best effort mode or not (default = false).\n#   ## By default best effort parsing is off.\n#   # best_effort = false\n#\n#   ## The RFC standard to use for message parsing\n#   ## By default RFC5424 is used. RFC3164 only supports UDP transport (no streaming support)\n#   ## Must be one of \"RFC5424\", or \"RFC3164\".\n#   # syslog_standard = \"RFC5424\"\n#\n#   ## Character to prepend to SD-PARAMs (default = \"_\").\n#   ## A syslog message can contain multiple parameters and multiple identifiers within structured data section.\n#   ## Eg., [id1 name1=\"val1\" name2=\"val2\"][id2 name1=\"val1\" nameA=\"valA\"]\n#   ## For each combination a field is created.\n#   ## Its name is created concatenating identifier, sdparam_separator, and parameter name.\n#   # sdparam_separator = \"_\"\n\n\n# # Parse the new lines appended to a file\n# [[inputs.tail]]\n#   ## File names or a pattern to tail.\n#   ## These accept standard unix glob matching rules, but with the addition of\n#   ## ** as a \"super asterisk\". ie:\n#   ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n#   ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n#   ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n#   ##   \"/var/log/log[!1-2]*  -\u003e tail files without 1-2\n#   ##   \"/var/log/log[^1-2]*  -\u003e identical behavior as above\n#   ## See https://github.com/gobwas/glob for more examples\n#   ##\n#   files = [\"/var/mymetrics.out\"]\n#\n#   ## Read file from beginning.\n#   # from_beginning = false\n#\n#   ## Whether file is a named pipe\n#   # pipe = false\n#\n#   ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n#   # watch_method = \"inotify\"\n#\n#   ## Maximum lines of the file to process that have not yet be written by the\n#   ## output.  For best throughput set based on the number of metrics on each\n#   ## line and the size of the output's metric_batch_size.\n#   # max_undelivered_lines = 1000\n#\n#   ## Character encoding to use when interpreting the file contents.  Invalid\n#   ## characters are replaced using the unicode replacement character.  When set\n#   ## to the empty string the data is not decoded to text.\n#   ##   ex: character_encoding = \"utf-8\"\n#   ##       character_encoding = \"utf-16le\"\n#   ##       character_encoding = \"utf-16be\"\n#   ##       character_encoding = \"\"\n#   # character_encoding = \"\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Set the tag that will contain the path of the tailed file. If you don't want this tag, set it to an empty string.\n#   # path_tag = \"path\"\n#\n#   ## multiline parser/codec\n#   ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html\n#   #[inputs.tail.multiline]\n#     ## The pattern should be a regexp which matches what you believe to be an\n# \t## indicator that the field is part of an event consisting of multiple lines of log data.\n#     #pattern = \"^\\s\"\n#\n#     ## This field must be either \"previous\" or \"next\".\n# \t## If a line matches the pattern, \"previous\" indicates that it belongs to the previous line,\n# \t## whereas \"next\" indicates that the line belongs to the next one.\n#     #match_which_line = \"previous\"\n#\n#     ## The invert_match field can be true or false (defaults to false).\n#     ## If true, a message not matching the pattern will constitute a match of the multiline\n# \t## filter and the what will be applied. (vice-versa is also true)\n#     #invert_match = false\n#\n#     ## After the specified timeout, this plugin sends a multiline event even if no new pattern\n# \t## is found to start a new event. The default timeout is 5s.\n#     #timeout = 5s\n\n\n# # Generic TCP listener\n# [[inputs.tcp_listener]]\n#   ## DEPRECATED: The 'tcp_listener' plugin is deprecated in version 1.3.0, use 'inputs.socket_listener' instead.\n#   # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n#   # socket_listener plugin\n#   # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n\n# # Generic UDP listener\n# [[inputs.udp_listener]]\n#   ## DEPRECATED: The 'udp_listener' plugin is deprecated in version 1.3.0, use 'inputs.socket_listener' instead.\n#   # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n#   # socket_listener plugin\n#   # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n\n# # Read metrics from VMware vCenter\n# [[inputs.vsphere]]\n#   ## List of vCenter URLs to be monitored. These three lines must be uncommented\n#   ## and edited for the plugin to work.\n#   vcenters = [ \"https://vcenter.local/sdk\" ]\n#   username = \"user@corp.local\"\n#   password = \"secret\"\n#\n#   ## VMs\n#   ## Typical VM metrics (if omitted or empty, all metrics are collected)\n#   # vm_include = [ \"/*/vm/**\"] # Inventory path to VMs to collect (by default all are collected)\n#   # vm_exclude = [] # Inventory paths to exclude\n#   vm_metric_include = [\n#     \"cpu.demand.average\",\n#     \"cpu.idle.summation\",\n#     \"cpu.latency.average\",\n#     \"cpu.readiness.average\",\n#     \"cpu.ready.summation\",\n#     \"cpu.run.summation\",\n#     \"cpu.usagemhz.average\",\n#     \"cpu.used.summation\",\n#     \"cpu.wait.summation\",\n#     \"mem.active.average\",\n#     \"mem.granted.average\",\n#     \"mem.latency.average\",\n#     \"mem.swapin.average\",\n#     \"mem.swapinRate.average\",\n#     \"mem.swapout.average\",\n#     \"mem.swapoutRate.average\",\n#     \"mem.usage.average\",\n#     \"mem.vmmemctl.average\",\n#     \"net.bytesRx.average\",\n#     \"net.bytesTx.average\",\n#     \"net.droppedRx.summation\",\n#     \"net.droppedTx.summation\",\n#     \"net.usage.average\",\n#     \"power.power.average\",\n#     \"virtualDisk.numberReadAveraged.average\",\n#     \"virtualDisk.numberWriteAveraged.average\",\n#     \"virtualDisk.read.average\",\n#     \"virtualDisk.readOIO.latest\",\n#     \"virtualDisk.throughput.usage.average\",\n#     \"virtualDisk.totalReadLatency.average\",\n#     \"virtualDisk.totalWriteLatency.average\",\n#     \"virtualDisk.write.average\",\n#     \"virtualDisk.writeOIO.latest\",\n#     \"sys.uptime.latest\",\n#   ]\n#   # vm_metric_exclude = [] ## Nothing is excluded by default\n#   # vm_instances = true ## true by default\n#\n#   ## Hosts\n#   ## Typical host metrics (if omitted or empty, all metrics are collected)\n#   # host_include = [ \"/*/host/**\"] # Inventory path to hosts to collect (by default all are collected)\n#   # host_exclude [] # Inventory paths to exclude\n#   host_metric_include = [\n#     \"cpu.coreUtilization.average\",\n#     \"cpu.costop.summation\",\n#     \"cpu.demand.average\",\n#     \"cpu.idle.summation\",\n#     \"cpu.latency.average\",\n#     \"cpu.readiness.average\",\n#     \"cpu.ready.summation\",\n#     \"cpu.swapwait.summation\",\n#     \"cpu.usage.average\",\n#     \"cpu.usagemhz.average\",\n#     \"cpu.used.summation\",\n#     \"cpu.utilization.average\",\n#     \"cpu.wait.summation\",\n#     \"disk.deviceReadLatency.average\",\n#     \"disk.deviceWriteLatency.average\",\n#     \"disk.kernelReadLatency.average\",\n#     \"disk.kernelWriteLatency.average\",\n#     \"disk.numberReadAveraged.average\",\n#     \"disk.numberWriteAveraged.average\",\n#     \"disk.read.average\",\n#     \"disk.totalReadLatency.average\",\n#     \"disk.totalWriteLatency.average\",\n#     \"disk.write.average\",\n#     \"mem.active.average\",\n#     \"mem.latency.average\",\n#     \"mem.state.latest\",\n#     \"mem.swapin.average\",\n#     \"mem.swapinRate.average\",\n#     \"mem.swapout.average\",\n#     \"mem.swapoutRate.average\",\n#     \"mem.totalCapacity.average\",\n#     \"mem.usage.average\",\n#     \"mem.vmmemctl.average\",\n#     \"net.bytesRx.average\",\n#     \"net.bytesTx.average\",\n#     \"net.droppedRx.summation\",\n#     \"net.droppedTx.summation\",\n#     \"net.errorsRx.summation\",\n#     \"net.errorsTx.summation\",\n#     \"net.usage.average\",\n#     \"power.power.average\",\n#     \"storageAdapter.numberReadAveraged.average\",\n#     \"storageAdapter.numberWriteAveraged.average\",\n#     \"storageAdapter.read.average\",\n#     \"storageAdapter.write.average\",\n#     \"sys.uptime.latest\",\n#   ]\n#     ## Collect IP addresses? Valid values are \"ipv4\" and \"ipv6\"\n#   # ip_addresses = [\"ipv6\", \"ipv4\" ]\n#\n#   # host_metric_exclude = [] ## Nothing excluded by default\n#   # host_instances = true ## true by default\n#\n#\n#   ## Clusters\n#   # cluster_include = [ \"/*/host/**\"] # Inventory path to clusters to collect (by default all are collected)\n#   # cluster_exclude = [] # Inventory paths to exclude\n#   # cluster_metric_include = [] ## if omitted or empty, all metrics are collected\n#   # cluster_metric_exclude = [] ## Nothing excluded by default\n#   # cluster_instances = false ## false by default\n#\n#   ## Datastores\n#   # datastore_include = [ \"/*/datastore/**\"] # Inventory path to datastores to collect (by default all are collected)\n#   # datastore_exclude = [] # Inventory paths to exclude\n#   # datastore_metric_include = [] ## if omitted or empty, all metrics are collected\n#   # datastore_metric_exclude = [] ## Nothing excluded by default\n#   # datastore_instances = false ## false by default\n#\n#   ## Datacenters\n#   # datacenter_include = [ \"/*/host/**\"] # Inventory path to clusters to collect (by default all are collected)\n#   # datacenter_exclude = [] # Inventory paths to exclude\n#   datacenter_metric_include = [] ## if omitted or empty, all metrics are collected\n#   datacenter_metric_exclude = [ \"*\" ] ## Datacenters are not collected by default.\n#   # datacenter_instances = false ## false by default\n#\n#   ## Plugin Settings\n#   ## separator character to use for measurement and field names (default: \"_\")\n#   # separator = \"_\"\n#\n#   ## number of objects to retrieve per query for realtime resources (vms and hosts)\n#   ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n#   # max_query_objects = 256\n#\n#   ## number of metrics to retrieve per query for non-realtime resources (clusters and datastores)\n#   ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n#   # max_query_metrics = 256\n#\n#   ## number of go routines to use for collection and discovery of objects and metrics\n#   # collect_concurrency = 1\n#   # discover_concurrency = 1\n#\n#   ## the interval before (re)discovering objects subject to metrics collection (default: 300s)\n#   # object_discovery_interval = \"300s\"\n#\n#   ## timeout applies to any of the api request made to vcenter\n#   # timeout = \"60s\"\n#\n#   ## When set to true, all samples are sent as integers. This makes the output\n#   ## data types backwards compatible with Telegraf 1.9 or lower. Normally all\n#   ## samples from vCenter, with the exception of percentages, are integer\n#   ## values, but under some conditions, some averaging takes place internally in\n#   ## the plugin. Setting this flag to \"false\" will send values as floats to\n#   ## preserve the full precision when averaging takes place.\n#   # use_int_samples = true\n#\n#   ## Custom attributes from vCenter can be very useful for queries in order to slice the\n#   ## metrics along different dimension and for forming ad-hoc relationships. They are disabled\n#   ## by default, since they can add a considerable amount of tags to the resulting metrics. To\n#   ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include\n#   ## to select the attributes you want to include.\n#   ## By default, since they can add a considerable amount of tags to the resulting metrics. To\n#   ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include\n#   ## to select the attributes you want to include.\n#   # custom_attribute_include = []\n#   # custom_attribute_exclude = [\"*\"]\n#\n#   ## The number of vSphere 5 minute metric collection cycles to look back for non-realtime metrics. In\n#   ## some versions (6.7, 7.0 and possible more), certain metrics, such as cluster metrics, may be reported\n#   ## with a significant delay (\u003e30min). If this happens, try increasing this number. Please note that increasing\n#   ## it too much may cause performance issues.\n#   # metric_lookback = 3\n#\n#   ## Optional SSL Config\n#   # ssl_ca = \"/path/to/cafile\"\n#   # ssl_cert = \"/path/to/certfile\"\n#   # ssl_key = \"/path/to/keyfile\"\n#   ## Use SSL but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## The Historical Interval value must match EXACTLY the interval in the daily\n#   # \"Interval Duration\" found on the VCenter server under Configure \u003e General \u003e Statistics \u003e Statistic intervals\n#   # historical_interval = \"5m\"\n\n\n# # A Webhooks Event collector\n# [[inputs.webhooks]]\n#   ## Address and port to host Webhook listener on\n#   service_address = \":1619\"\n#\n#   [inputs.webhooks.filestack]\n#     path = \"/filestack\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.github]\n#     path = \"/github\"\n#     # secret = \"\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.mandrill]\n#     path = \"/mandrill\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.rollbar]\n#     path = \"/rollbar\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.papertrail]\n#     path = \"/papertrail\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.particle]\n#     path = \"/particle\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n\n\n# # This plugin implements the Zipkin http server to gather trace and timing data needed to troubleshoot latency problems in microservice architectures.\n# [[inputs.zipkin]]\n#   # path = \"/api/v1/spans\" # URL path for span data\n#   # port = 9411            # Port on which Telegraf listens\n",
            "template": "\n[global_tags]\n  # dc = \"us-east-1\" # will tag all metrics with dc=us-east-1\n  # rack = \"1a\"\n  ## Environment variables can be used as tags, and throughout the config file\n  # user = \"$USER\"\n\n\n# Configuration for telegraf agent\n[agent]\n  ## Default data collection interval for all inputs\n  interval = \"10s\"\n  ## Rounds collection interval to 'interval'\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\n  round_interval = true\n\n  ## Telegraf will send metrics to outputs in batches of at most\n  ## metric_batch_size metrics.\n  ## This controls the size of writes that Telegraf sends to output plugins.\n  metric_batch_size = 1000\n\n  ## Maximum number of unwritten metrics per output.  Increasing this value\n  ## allows for longer periods of output downtime without dropping metrics at the\n  ## cost of higher maximum memory usage.\n  metric_buffer_limit = 10000\n\n  ## Collection jitter is used to jitter the collection by a random amount.\n  ## Each plugin will sleep for a random time within jitter before collecting.\n  ## This can be used to avoid many plugins querying things like sysfs at the\n  ## same time, which can have a measurable effect on the system.\n  collection_jitter = \"0s\"\n\n  ## Collection offset is used to shift the collection by the given amount.\n  ## This can be be used to avoid many plugins querying constraint devices\n  ## at the same time by manually scheduling them in time.\n  # collection_offset = \"0s\"\n\n  ## Default flushing interval for all outputs. Maximum flush_interval will be\n  ## flush_interval + flush_jitter\n  flush_interval = \"10s\"\n  ## Jitter the flush interval by a random amount. This is primarily to avoid\n  ## large write spikes for users running a large number of telegraf instances.\n  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\n  flush_jitter = \"0s\"\n\n  ## Collected metrics are rounded to the precision specified. Precision is\n  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).\n  ## Valid time units are \"ns\", \"us\" (or \"Âµs\"), \"ms\", \"s\".\n  ##\n  ## By default or when set to \"0s\", precision will be set to the same\n  ## timestamp order as the collection interval, with the maximum being 1s:\n  ##   ie, when interval = \"10s\", precision will be \"1s\"\n  ##       when interval = \"250ms\", precision will be \"1ms\"\n  ##\n  ## Precision will NOT be used for service inputs. It is up to each individual\n  ## service input to set the timestamp at the appropriate precision.\n  precision = \"\"\n\n  ## Log at debug level.\n  # debug = false\n  ## Log only error level messages.\n  # quiet = false\n\n  ## Log target controls the destination for logs and can be one of \"file\",\n  ## \"stderr\" or, on Windows, \"eventlog\".  When set to \"file\", the output file\n  ## is determined by the \"logfile\" setting.\n  # logtarget = \"file\"\n\n  ## Name of the file to be logged to when using the \"file\" logtarget.  If set to\n  ## the empty string then logs are written to stderr.\n  # logfile = \"\"\n\n  ## The logfile will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.  Logs are rotated only when\n  ## written to, if there is no log activity rotation may be delayed.\n  # logfile_rotation_interval = \"0d\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # logfile_rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # logfile_rotation_max_archives = 5\n\n  ## Pick a timezone to use when logging or type 'local' for local time.\n  ## Example: America/Chicago\n  # log_with_timezone = \"\"\n\n  ## Override default hostname, if empty use os.Hostname()\n  hostname = \"\"\n  ## If set to true, do no set the \"host\" tag in the telegraf agent.\n  omit_hostname = false\n\n###############################################################################\n#                            OUTPUT PLUGINS                                   #\n###############################################################################\n\n\n# Configuration for sending metrics to InfluxDB\n# [[outputs.influxdb]]\n  ## The full HTTP or UDP URL for your InfluxDB instance.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  # urls = [\"unix:///var/run/influxdb.sock\"]\n  # urls = [\"udp://127.0.0.1:8089\"]\n  # urls = [\"http://127.0.0.1:8086\"]\n\n  ## The target database for metrics; will be created as needed.\n  ## For UDP url endpoint database needs to be configured on server side.\n  # database = \"telegraf\"\n\n  ## The value of this tag will be used to determine the database.  If this\n  ## tag is not set the 'database' option is used as the default.\n  # database_tag = \"\"\n\n  ## If true, the 'database_tag' will not be included in the written metric.\n  # exclude_database_tag = false\n\n  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using\n  ## Telegraf with a user without permissions to create databases or when the\n  ## database already exists.\n  # skip_database_creation = false\n\n  ## Name of existing retention policy to write to.  Empty string writes to\n  ## the default retention policy.  Only takes effect when using HTTP.\n  # retention_policy = \"\"\n\n  ## The value of this tag will be used to determine the retention policy.  If this\n  ## tag is not set the 'retention_policy' option is used as the default.\n  # retention_policy_tag = \"\"\n\n  ## If true, the 'retention_policy_tag' will not be included in the written metric.\n  # exclude_retention_policy_tag = false\n\n  ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\".\n  ## Only takes effect when using HTTP.\n  # write_consistency = \"any\"\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## HTTP Basic Auth\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## UDP payload size is the maximum packet size to send.\n  # udp_payload = \"512B\"\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: \"42u\".  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  # influx_uint_support = false\n\n\n# # Configuration for Amon Server to send metrics to.\n# [[outputs.amon]]\n#   ## Amon Server Key\n#   server_key = \"my-server-key\" # required.\n#\n#   ## Amon Instance URL\n#   amon_instance = \"https://youramoninstance\" # required\n#\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n\n\n# # Publishes metrics to an AMQP broker\n# [[outputs.amqp]]\n#   ## Brokers to publish to.  If multiple brokers are specified a random broker\n#   ## will be selected anytime a connection is established.  This can be\n#   ## helpful for load balancing when not using a dedicated load balancer.\n#   brokers = [\"amqp://localhost:5672/influxdb\"]\n#\n#   ## Maximum messages to send over a connection.  Once this is reached, the\n#   ## connection is closed and a new connection is made.  This can be helpful for\n#   ## load balancing when not using a dedicated load balancer.\n#   # max_messages = 0\n#\n#   ## Exchange to declare and publish to.\n#   exchange = \"telegraf\"\n#\n#   ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n#   # exchange_type = \"topic\"\n#\n#   ## If true, exchange will be passively declared.\n#   # exchange_passive = false\n#\n#   ## Exchange durability can be either \"transient\" or \"durable\".\n#   # exchange_durability = \"durable\"\n#\n#   ## Additional exchange arguments.\n#   # exchange_arguments = { }\n#   # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n#\n#   ## Authentication credentials for the PLAIN auth_method.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Auth method. PLAIN and EXTERNAL are supported\n#   ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n#   ## described here: https://www.rabbitmq.com/plugins.html\n#   # auth_method = \"PLAIN\"\n#\n#   ## Metric tag to use as a routing key.\n#   ##   ie, if this tag exists, its value will be used as the routing key\n#   # routing_tag = \"host\"\n#\n#   ## Static routing key.  Used when no routing_tag is set or as a fallback\n#   ## when the tag specified in routing tag is not found.\n#   # routing_key = \"\"\n#   # routing_key = \"telegraf\"\n#\n#   ## Delivery Mode controls if a published message is persistent.\n#   ##   One of \"transient\" or \"persistent\".\n#   # delivery_mode = \"transient\"\n#\n#   ## Static headers added to each published message.\n#   # headers = { }\n#   # headers = {\"database\" = \"telegraf\", \"retention_policy\" = \"default\"}\n#\n#   ## Connection timeout.  If not provided, will default to 5s.  0s means no\n#   ## timeout (not recommended).\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## If true use batch serialization format instead of line based delimiting.\n#   ## Only applies to data formats which are not line based such as JSON.\n#   ## Recommended to set to true.\n#   # use_batch_format = false\n#\n#   ## Content encoding for message payloads, can be set to \"gzip\" to or\n#   ## \"identity\" to apply no encoding.\n#   ##\n#   ## Please note that when use_batch_format = false each amqp message contains only\n#   ## a single metric, it is recommended to use compression with batch format\n#   ## for best results.\n#   # content_encoding = \"identity\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n\n\n# # Send metrics to Azure Application Insights\n# [[outputs.application_insights]]\n#   ## Instrumentation key of the Application Insights resource.\n#   instrumentation_key = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"\n#\n#   ## Regions that require endpoint modification https://docs.microsoft.com/en-us/azure/azure-monitor/app/custom-endpoints\n#   # endpoint_url = \"https://dc.services.visualstudio.com/v2/track\"\n#\n#   ## Timeout for closing (default: 5s).\n#   # timeout = \"5s\"\n#\n#   ## Enable additional diagnostic logging.\n#   # enable_diagnostic_logging = false\n#\n#   ## Context Tag Sources add Application Insights context tags to a tag value.\n#   ##\n#   ## For list of allowed context tag keys see:\n#   ## https://github.com/microsoft/ApplicationInsights-Go/blob/master/appinsights/contracts/contexttagkeys.go\n#   # [outputs.application_insights.context_tag_sources]\n#   #   \"ai.cloud.role\" = \"kubernetes_container_name\"\n#   #   \"ai.cloud.roleInstance\" = \"kubernetes_pod_name\"\n\n\n# # Sends metrics to Azure Data Explorer\n# [[outputs.azure_data_explorer]]\n#   ## Azure Data Explorer cluster endpoint\n#   ## ex: endpoint_url = \"https://clustername.australiasoutheast.kusto.windows.net\"\n#   endpoint_url = \"\"\n#\n#   ## The Azure Data Explorer database that the metrics will be ingested into.\n#   ## The plugin will NOT generate this database automatically, it's expected that this database already exists before ingestion.\n#   ## ex: \"exampledatabase\"\n#   database = \"\"\n#\n#   ## Timeout for Azure Data Explorer operations\n#   # timeout = \"20s\"\n#\n#   ## Type of metrics grouping used when pushing to Azure Data Explorer.\n#   ## Default is \"TablePerMetric\" for one table per different metric.\n#   ## For more information, please check the plugin README.\n#   # metrics_grouping_type = \"TablePerMetric\"\n#\n#   ## Name of the single table to store all the metrics (Only needed if metrics_grouping_type is \"SingleTable\").\n#   # table_name = \"\"\n#\n#   ## Creates tables and relevant mapping if set to true(default).\n#   ## Skips table and mapping creation if set to false, this is useful for running Telegraf with the lowest possible permissions i.e. table ingestor role.\n#   # create_tables = true\n\n\n# # Send aggregate metrics to Azure Monitor\n# [[outputs.azure_monitor]]\n#   ## Timeout for HTTP writes.\n#   # timeout = \"20s\"\n#\n#   ## Set the namespace prefix, defaults to \"Telegraf/\u003cinput-name\u003e\".\n#   # namespace_prefix = \"Telegraf/\"\n#\n#   ## Azure Monitor doesn't have a string value type, so convert string\n#   ## fields to dimensions (a.k.a. tags) if enabled. Azure Monitor allows\n#   ## a maximum of 10 dimensions so Telegraf will only send the first 10\n#   ## alphanumeric dimensions.\n#   # strings_as_dimensions = false\n#\n#   ## Both region and resource_id must be set or be available via the\n#   ## Instance Metadata service on Azure Virtual Machines.\n#   #\n#   ## Azure Region to publish metrics against.\n#   ##   ex: region = \"southcentralus\"\n#   # region = \"\"\n#   #\n#   ## The Azure Resource ID against which metric will be logged, e.g.\n#   ##   ex: resource_id = \"/subscriptions/\u003csubscription_id\u003e/resourceGroups/\u003cresource_group\u003e/providers/Microsoft.Compute/virtualMachines/\u003cvm_name\u003e\"\n#   # resource_id = \"\"\n#\n#   ## Optionally, if in Azure US Government, China or other sovereign\n#   ## cloud environment, set appropriate REST endpoint for receiving\n#   ## metrics. (Note: region may be unused in this context)\n#   # endpoint_url = \"https://monitoring.core.usgovcloudapi.net\"\n\n\n# # Configuration for Google Cloud BigQuery to send entries\n# [[outputs.bigquery]]\n#   ## Credentials File\n#   credentials_file = \"/path/to/service/account/key.json\"\n#\n#   ## Google Cloud Platform Project\n#   project = \"my-gcp-project\"\n#\n#   ## The namespace for the metric descriptor\n#   dataset = \"telegraf\"\n#\n#   ## Timeout for BigQuery operations.\n#   # timeout = \"5s\"\n#\n#   ## Character to replace hyphens on Metric name\n#   # replace_hyphen_to = \"_\"\n\n\n# # Publish Telegraf metrics to a Google Cloud PubSub topic\n# [[outputs.cloud_pubsub]]\n#   ## Required. Name of Google Cloud Platform (GCP) Project that owns\n#   ## the given PubSub topic.\n#   project = \"my-project\"\n#\n#   ## Required. Name of PubSub topic to publish metrics to.\n#   topic = \"my-topic\"\n#\n#   ## Required. Data format to consume.\n#   ## Each data format has its own unique set of configuration options.\n#   ## Read more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n#   ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n#   ## Application Default Credentials, which is preferred.\n#   # credentials_file = \"path/to/my/creds.json\"\n#\n#   ## Optional. If true, will send all metrics per write in one PubSub message.\n#   # send_batched = true\n#\n#   ## The following publish_* parameters specifically configures batching\n#   ## requests made to the GCP Cloud PubSub API via the PubSub Golang library. Read\n#   ## more here: https://godoc.org/cloud.google.com/go/pubsub#PublishSettings\n#\n#   ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n#   ## when it has this many PubSub messages. If send_batched is true,\n#   ## this is ignored and treated as if it were 1.\n#   # publish_count_threshold = 1000\n#\n#   ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n#   ## when it has this many PubSub messages. If send_batched is true,\n#   ## this is ignored and treated as if it were 1\n#   # publish_byte_threshold = 1000000\n#\n#   ## Optional. Specifically configures requests made to the PubSub API.\n#   # publish_num_go_routines = 2\n#\n#   ## Optional. Specifies a timeout for requests to the PubSub API.\n#   # publish_timeout = \"30s\"\n#\n#   ## Optional. If true, published PubSub message data will be base64-encoded.\n#   # base64_data = false\n#\n#   ## Optional. PubSub attributes to add to metrics.\n#   # [outputs.cloud_pubsub.attributes]\n#   #   my_attr = \"tag_value\"\n\n\n# # Configuration for AWS CloudWatch output.\n# [[outputs.cloudwatch]]\n#   ## Amazon REGION\n#   region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Namespace for the CloudWatch MetricDatums\n#   namespace = \"InfluxData/Telegraf\"\n#\n#   ## If you have a large amount of metrics, you should consider to send statistic\n#   ## values instead of raw metrics which could not only improve performance but\n#   ## also save AWS API cost. If enable this flag, this plugin would parse the required\n#   ## CloudWatch statistic fields (count, min, max, and sum) and send them to CloudWatch.\n#   ## You could use basicstats aggregator to calculate those fields. If not all statistic\n#   ## fields are available, all fields would still be sent as raw metrics.\n#   # write_statistics = false\n#\n#   ## Enable high resolution metrics of 1 second (if not enabled, standard resolution are of 60 seconds precision)\n#   # high_resolution_metrics = false\n\n\n# # Configuration for AWS CloudWatchLogs output.\n# [[outputs.cloudwatch_logs]]\n# ## The region is the Amazon region that you wish to connect to.\n# ## Examples include but are not limited to:\n# ## - us-west-1\n# ## - us-west-2\n# ## - us-east-1\n# ## - ap-southeast-1\n# ## - ap-southeast-2\n# ## ...\n# region = \"us-east-1\"\n#\n# ## Amazon Credentials\n# ## Credentials are loaded in the following order\n# ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n# ## 2) Assumed credentials via STS if role_arn is specified\n# ## 3) explicit credentials from 'access_key' and 'secret_key'\n# ## 4) shared profile from 'profile'\n# ## 5) environment variables\n# ## 6) shared credentials file\n# ## 7) EC2 Instance Profile\n# #access_key = \"\"\n# #secret_key = \"\"\n# #token = \"\"\n# #role_arn = \"\"\n# #web_identity_token_file = \"\"\n# #role_session_name = \"\"\n# #profile = \"\"\n# #shared_credential_file = \"\"\n#\n# ## Endpoint to make request against, the correct endpoint is automatically\n# ## determined and this option should only be set if you wish to override the\n# ## default.\n# ##   ex: endpoint_url = \"http://localhost:8000\"\n# # endpoint_url = \"\"\n#\n# ## Cloud watch log group. Must be created in AWS cloudwatch logs upfront!\n# ## For example, you can specify the name of the k8s cluster here to group logs from all cluster in oine place\n# log_group = \"my-group-name\"\n#\n# ## Log stream in log group\n# ## Either log group name or reference to metric attribute, from which it can be parsed:\n# ## tag:\u003cTAG_NAME\u003e or field:\u003cFIELD_NAME\u003e. If log stream is not exist, it will be created.\n# ## Since AWS is not automatically delete logs streams with expired logs entries (i.e. empty log stream)\n# ## you need to put in place appropriate house-keeping (https://forums.aws.amazon.com/thread.jspa?threadID=178855)\n# log_stream = \"tag:location\"\n#\n# ## Source of log data - metric name\n# ## specify the name of the metric, from which the log data should be retrieved.\n# ## I.e., if you  are using docker_log plugin to stream logs from container, then\n# ## specify log_data_metric_name  = \"docker_log\"\n# log_data_metric_name  = \"docker_log\"\n#\n# ## Specify from which metric attribute the log data should be retrieved:\n# ## tag:\u003cTAG_NAME\u003e or field:\u003cFIELD_NAME\u003e.\n# ## I.e., if you  are using docker_log plugin to stream logs from container, then\n# ## specify log_data_source  = \"field:message\"\n# log_data_source  = \"field:message\"\n\n\n# # Configuration for CrateDB to send metrics to.\n# [[outputs.cratedb]]\n#   # A github.com/jackc/pgx/v4 connection string.\n#   # See https://pkg.go.dev/github.com/jackc/pgx/v4#ParseConfig\n#   url = \"postgres://user:password@localhost/schema?sslmode=disable\"\n#   # Timeout for all CrateDB queries.\n#   timeout = \"5s\"\n#   # Name of the table to store metrics in.\n#   table = \"metrics\"\n#   # If true, and the metrics table does not exist, create it automatically.\n#   table_create = true\n#   # The character(s) to replace any '.' in an object key with\n#   key_separator = \"_\"\n\n\n# # Configuration for DataDog API to send metrics to.\n# [[outputs.datadog]]\n#   ## Datadog API key\n#   apikey = \"my-secret-key\"\n#\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n#\n#   ## Write URL override; useful for debugging.\n#   # url = \"https://app.datadoghq.com/api/v1/series\"\n#\n#   ## Set http_proxy (telegraf uses the system wide proxy settings if it isn't set)\n#   # http_proxy_url = \"http://localhost:8888\"\n#\n#   ## Override the default (none) compression used to send data.\n#   ## Supports: \"zlib\", \"none\"\n#   # compression = \"none\"\n\n\n# # Send metrics to nowhere at all\n# [[outputs.discard]]\n#   # no configuration\n\n\n# # Send telegraf metrics to a Dynatrace environment\n# [[outputs.dynatrace]]\n#   ## For usage with the Dynatrace OneAgent you can omit any configuration,\n#   ## the only requirement is that the OneAgent is running on the same host.\n#   ## Only setup environment url and token if you want to monitor a Host without the OneAgent present.\n#   ##\n#   ## Your Dynatrace environment URL.\n#   ## For Dynatrace OneAgent you can leave this empty or set it to \"http://127.0.0.1:14499/metrics/ingest\" (default)\n#   ## For Dynatrace SaaS environments the URL scheme is \"https://{your-environment-id}.live.dynatrace.com/api/v2/metrics/ingest\"\n#   ## For Dynatrace Managed environments the URL scheme is \"https://{your-domain}/e/{your-environment-id}/api/v2/metrics/ingest\"\n#   url = \"\"\n#\n#   ## Your Dynatrace API token.\n#   ## Create an API token within your Dynatrace environment, by navigating to Settings \u003e Integration \u003e Dynatrace API\n#   ## The API token needs data ingest scope permission. When using OneAgent, no API token is required.\n#   api_token = \"\"\n#\n#   ## Optional prefix for metric names (e.g.: \"telegraf\")\n#   prefix = \"telegraf\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional flag for ignoring tls certificate check\n#   # insecure_skip_verify = false\n#\n#\n#   ## Connection timeout, defaults to \"5s\" if not set.\n#   timeout = \"5s\"\n#\n#   ## If you want metrics to be treated and reported as delta counters, add the metric names here\n#   additional_counters = [ ]\n#\n#   ## Optional dimensions to be added to every metric\n#   # [outputs.dynatrace.default_dimensions]\n#   # default_key = \"default value\"\n\n\n# # Configuration for Elasticsearch to send metrics to.\n# [[outputs.elasticsearch]]\n#   ## The full HTTP endpoint URL for your Elasticsearch instance\n#   ## Multiple urls can be specified as part of the same cluster,\n#   ## this means that only ONE of the urls will be written to each interval.\n#   urls = [ \"http://node1.es.example.com:9200\" ] # required.\n#   ## Elasticsearch client timeout, defaults to \"5s\" if not set.\n#   timeout = \"5s\"\n#   ## Set to true to ask Elasticsearch a list of all cluster nodes,\n#   ## thus it is not necessary to list all nodes in the urls config option.\n#   enable_sniffer = false\n#   ## Set to true to enable gzip compression\n#   enable_gzip = false\n#   ## Set the interval to check if the Elasticsearch nodes are available\n#   ## Setting to \"0s\" will disable the health check (not recommended in production)\n#   health_check_interval = \"10s\"\n#   ## HTTP basic authentication details\n#   # username = \"telegraf\"\n#   # password = \"mypassword\"\n#   ## HTTP bearer token authentication details\n#   # auth_bearer_token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9\"\n#\n#   ## Index Config\n#   ## The target index for metrics (Elasticsearch will create if it not exists).\n#   ## You can use the date specifiers below to create indexes per time frame.\n#   ## The metric timestamp will be used to decide the destination index name\n#   # %Y - year (2016)\n#   # %y - last two digits of year (00..99)\n#   # %m - month (01..12)\n#   # %d - day of month (e.g., 01)\n#   # %H - hour (00..23)\n#   # %V - week of the year (ISO week) (01..53)\n#   ## Additionally, you can specify a tag name using the notation {{tag_name}}\n#   ## which will be used as part of the index name. If the tag does not exist,\n#   ## the default tag value will be used.\n#   # index_name = \"telegraf-{{host}}-%Y.%m.%d\"\n#   # default_tag_value = \"none\"\n#   index_name = \"telegraf-%Y.%m.%d\" # required.\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Template Config\n#   ## Set to true if you want telegraf to manage its index template.\n#   ## If enabled it will create a recommended index template for telegraf indexes\n#   manage_template = true\n#   ## The template name used for telegraf indexes\n#   template_name = \"telegraf\"\n#   ## Set to true if you want telegraf to overwrite an existing template\n#   overwrite_template = false\n#   ## If set to true a unique ID hash will be sent as sha256(concat(timestamp,measurement,series-hash)) string\n#   ## it will enable data resend and update metric points avoiding duplicated metrics with diferent id's\n#   force_document_id = false\n#\n#   ## Specifies the handling of NaN and Inf values.\n#   ## This option can have the following values:\n#   ##    none    -- do not modify field-values (default); will produce an error if NaNs or infs are encountered\n#   ##    drop    -- drop fields containing NaNs or infs\n#   ##    replace -- replace with the value in \"float_replacement_value\" (default: 0.0)\n#   ##               NaNs and inf will be replaced with the given number, -inf with the negative of that number\n#   # float_handling = \"none\"\n#   # float_replacement_value = 0.0\n#\n#   ## Pipeline Config\n#   ## To use a ingest pipeline, set this to the name of the pipeline you want to use.\n#   # use_pipeline = \"my_pipeline\"\n#   ## Additionally, you can specify a tag name using the notation {{tag_name}}\n#   ## which will be used as part of the pipeline name. If the tag does not exist,\n#   ## the default pipeline will be used as the pipeline. If no default pipeline is set,\n#   ## no pipeline is used for the metric.\n#   # use_pipeline = \"{{es_pipeline}}\"\n#   # default_pipeline = \"my_pipeline\"\n\n\n# # Configuration for Event Hubs output plugin\n# [[outputs.event_hubs]]\n#   ## The full connection string to the Event Hub (required)\n#   ## The shared access key must have \"Send\" permissions on the target Event Hub.\n#   connection_string = \"Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName\"\n#\n#   ## Client timeout (defaults to 30s)\n#   # timeout = \"30s\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"json\"\n\n\n# # Send metrics to command as input over stdin\n# [[outputs.exec]]\n#   ## Command to ingest metrics via stdin.\n#   command = [\"tee\", \"-a\", \"/dev/null\"]\n#\n#   ## Timeout for command to complete.\n#   # timeout = \"5s\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n\n\n# # Run executable as long-running output plugin\n# [[outputs.execd]]\n#   ## Program to run as daemon\n#   command = [\"my-telegraf-output\", \"--some-flag\", \"value\"]\n#\n#   ## Delay before the process is restarted after an unexpected termination\n#   restart_delay = \"10s\"\n#\n#   ## Data format to export.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send telegraf metrics to file(s)\n# [[outputs.file]]\n#   ## Files to write to, \"stdout\" is a specially handled file.\n#   files = [\"stdout\", \"/tmp/metrics.out\"]\n#\n#   ## Use batch serialization format instead of line based delimiting.  The\n#   ## batch format allows for the production of non line based output formats and\n#   ## may more efficiently encode metric groups.\n#   # use_batch_format = false\n#\n#   ## The file will be rotated after the time interval specified.  When set\n#   ## to 0 no time based rotation is performed.\n#   # rotation_interval = \"0d\"\n#\n#   ## The logfile will be rotated when it becomes larger than the specified\n#   ## size.  When set to 0 no size based rotation is performed.\n#   # rotation_max_size = \"0MB\"\n#\n#   ## Maximum number of rotated archives to keep, any older logs are deleted.\n#   ## If set to -1, no archives are removed.\n#   # rotation_max_archives = 5\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Configuration for Graphite server to send metrics to\n# [[outputs.graphite]]\n#   ## TCP endpoint for your graphite instance.\n#   ## If multiple endpoints are configured, output will be load balanced.\n#   ## Only one of the endpoints will be written to with each iteration.\n#   servers = [\"localhost:2003\"]\n#   ## Prefix metrics name\n#   prefix = \"\"\n#   ## Graphite output template\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   template = \"host.tags.measurement.field\"\n#\n#   ## Enable Graphite tags support\n#   # graphite_tag_support = false\n#\n#   ## Define how metric names and tags are sanitized; options are \"strict\", or \"compatible\"\n#   ## strict - Default method, and backwards compatible with previous versionf of Telegraf\n#   ## compatible - More relaxed sanitizing when using tags, and compatible with the graphite spec\n#   # graphite_tag_sanitize_mode = \"strict\"\n#\n#   ## Character for separating metric name and field for Graphite tags\n#   # graphite_separator = \".\"\n#\n#   ## Graphite templates patterns\n#   ## 1. Template for cpu\n#   ## 2. Template for disk*\n#   ## 3. Default template\n#   # templates = [\n#   #  \"cpu tags.measurement.host.field\",\n#   #  \"disk* measurement.field\",\n#   #  \"host.measurement.tags.field\"\n#   #]\n#\n#   ## timeout in seconds for the write connection to graphite\n#   timeout = 2\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Send telegraf metrics to graylog\n# [[outputs.graylog]]\n#   ## Endpoints for your graylog instances.\n#   servers = [\"udp://127.0.0.1:12201\"]\n#\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n#\n#   ## The field to use as the GELF short_message, if unset the static string\n#   ## \"telegraf\" will be used.\n#   ##   example: short_message_field = \"message\"\n#   # short_message_field = \"\"\n#\n#   ## According to GELF payload specification, additional fields names must be prefixed\n#   ## with an underscore. Previous versions did not prefix custom field 'name' with underscore.\n#   ## Set to true for backward compatibility.\n#   # name_field_no_prefix = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Send telegraf metrics to GroundWork Monitor\n# [[outputs.groundwork]]\n#   ## URL of your groundwork instance.\n#   url = \"https://groundwork.example.com\"\n#\n#   ## Agent uuid for GroundWork API Server.\n#   agent_id = \"\"\n#\n#   ## Username and password to access GroundWork API.\n#   username = \"\"\n#   password = \"\"\n#\n#   ## Default display name for the host with services(metrics).\n#   # default_host = \"telegraf\"\n#\n#   ## Default service state.\n#   # default_service_state = \"SERVICE_OK\"\n#\n#   ## The name of the tag that contains the hostname.\n#   # resource_tag = \"host\"\n#\n#   ## The name of the tag that contains the host group name.\n#   # group_tag = \"group\"\n\n\n# # Configurable HTTP health check resource based on metrics\n# [[outputs.health]]\n#   ## Address and port to listen on.\n#   ##   ex: service_address = \"http://localhost:8080\"\n#   ##       service_address = \"unix:///var/run/telegraf-health.sock\"\n#   # service_address = \"http://:8080\"\n#\n#   ## The maximum duration for reading the entire request.\n#   # read_timeout = \"5s\"\n#   ## The maximum duration for writing the entire response.\n#   # write_timeout = \"5s\"\n#\n#   ## Username and password to accept for HTTP basic authentication.\n#   # basic_username = \"user1\"\n#   # basic_password = \"secret\"\n#\n#   ## Allowed CA certificates for client certificates.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## TLS server certificate and private key.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## One or more check sub-tables should be defined, it is also recommended to\n#   ## use metric filtering to limit the metrics that flow into this output.\n#   ##\n#   ## When using the default buffer sizes, this example will fail when the\n#   ## metric buffer is half full.\n#   ##\n#   ## namepass = [\"internal_write\"]\n#   ## tagpass = { output = [\"influxdb\"] }\n#   ##\n#   ## [[outputs.health.compares]]\n#   ##   field = \"buffer_size\"\n#   ##   lt = 5000.0\n#   ##\n#   ## [[outputs.health.contains]]\n#   ##   field = \"buffer_size\"\n\n\n# # A plugin that can transmit metrics over HTTP\n# [[outputs.http]]\n#   ## URL is the address to send metrics to\n#   url = \"http://127.0.0.1:8080/telegraf\"\n#\n#   ## Timeout for HTTP message\n#   # timeout = \"5s\"\n#\n#   ## HTTP method, one of: \"POST\" or \"PUT\"\n#   # method = \"POST\"\n#\n#   ## HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## OAuth2 Client Credentials Grant\n#   # client_id = \"clientid\"\n#   # client_secret = \"secret\"\n#   # token_url = \"https://indentityprovider/oauth2/v1/token\"\n#   # scopes = [\"urn:opc:idm:__myscopes__\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional Cookie authentication\n#   # cookie_auth_url = \"https://localhost/authMe\"\n#   # cookie_auth_method = \"POST\"\n#   # cookie_auth_username = \"username\"\n#   # cookie_auth_password = \"pa$$word\"\n#   # cookie_auth_headers = '{\"Content-Type\": \"application/json\", \"X-MY-HEADER\":\"hello\"}'\n#   # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n#   ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n#   # cookie_auth_renewal = \"5m\"\n#\n#   ## Data format to output.\n#   ## Each data format has it's own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n#\n#   ## Use batch serialization format (default) instead of line based format.\n#   ## Batch format is more efficient and should be used unless line based\n#   ## format is really needed.\n#   # use_batch_format = true\n#\n#   ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## Additional HTTP headers\n#   # [outputs.http.headers]\n#   #   # Should be set manually to \"application/json\" for json data_format\n#   #   Content-Type = \"text/plain; charset=utf-8\"\n#\n#   ## Idle (keep-alive) connection timeout.\n#   ## Maximum amount of time before idle connection is closed.\n#   ## Zero means no limit.\n#   # idle_conn_timeout = 0\n#\n#   ## Amazon Region\n#   #region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n\n\n# # Configuration for sending metrics to InfluxDB\n[[outputs.influxdb_v2]]\n  ## The URLs of the InfluxDB cluster nodes.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  urls = [\"http://${DOCKER_INFLUXDB_HOSTNAME}:${DOCKER_INFLUXDB_INIT_PORT}\"]\n\n  ## Token for authentication.\n  token = \"${DOCKER_INFLUXDB_INIT_ADMIN_TOKEN}\"\n\n  ## Organization is the name of the organization you wish to write to; must exist.\n  organization = \"${DOCKER_INFLUXDB_INIT_ORG}\"\n\n  ## Destination bucket to write into.\n  bucket = \"${DOCKER_INFLUXDB_INIT_BUCKET}\"\n\n#   ## The value of this tag will be used to determine the bucket.  If this\n#   ## tag is not set the 'bucket' option is used as the default.\n#   # bucket_tag = \"\"\n#\n#   ## If true, the bucket tag will not be added to the metric.\n#   # exclude_bucket_tag = false\n#\n#   ## Timeout for HTTP messages.\n#   # timeout = \"5s\"\n#\n#   ## Additional HTTP headers\n#   # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## HTTP Proxy override, if unset values the standard proxy environment\n#   ## variables are consulted to determine which proxy, if any, should be used.\n#   # http_proxy = \"http://corporate.proxy:3128\"\n#\n#   ## HTTP User-Agent\n#   # user_agent = \"telegraf\"\n#\n#   ## Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"gzip\"\n#\n#   ## Enable or disable uint support for writing uints influxdb 2.0.\n#   # influx_uint_support = false\n#\n#   ## Optional TLS Config for use on HTTP connections.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  insecure_skip_verify = false\n\n\n# # Configuration for sending metrics to an Instrumental project\n# [[outputs.instrumental]]\n#   ## Project API Token (required)\n#   api_token = \"API Token\" # required\n#   ## Prefix the metrics with a given name\n#   prefix = \"\"\n#   ## Stats output template (Graphite formatting)\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n#   template = \"host.tags.measurement.field\"\n#   ## Timeout in seconds to connect\n#   timeout = \"2s\"\n#   ## Display Communication to Instrumental\n#   debug = false\n\n\n# # Configuration for the Kafka server to send metrics to\n# [[outputs.kafka]]\n#   ## URLs of kafka brokers\n#   brokers = [\"localhost:9092\"]\n#   ## Kafka topic for producer messages\n#   topic = \"telegraf\"\n#\n#   ## The value of this tag will be used as the topic.  If not set the 'topic'\n#   ## option is used.\n#   # topic_tag = \"\"\n#\n#   ## If true, the 'topic_tag' will be removed from to the metric.\n#   # exclude_topic_tag = false\n#\n#   ## Optional Client id\n#   # client_id = \"Telegraf\"\n#\n#   ## Set the minimal supported Kafka version.  Setting this enables the use of new\n#   ## Kafka features and APIs.  Of particular interest, lz4 compression\n#   ## requires at least version 0.10.0.0.\n#   ##   ex: version = \"1.1.0\"\n#   # version = \"\"\n#\n#   ## Optional topic suffix configuration.\n#   ## If the section is omitted, no suffix is used.\n#   ## Following topic suffix methods are supported:\n#   ##   measurement - suffix equals to separator + measurement's name\n#   ##   tags        - suffix equals to separator + specified tags' values\n#   ##                 interleaved with separator\n#\n#   ## Suffix equals to \"_\" + measurement name\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"measurement\"\n#   #   separator = \"_\"\n#\n#   ## Suffix equals to \"__\" + measurement's \"foo\" tag value.\n#   ##   If there's no such a tag, suffix equals to an empty string\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"tags\"\n#   #   keys = [\"foo\"]\n#   #   separator = \"__\"\n#\n#   ## Suffix equals to \"_\" + measurement's \"foo\" and \"bar\"\n#   ##   tag values, separated by \"_\". If there is no such tags,\n#   ##   their values treated as empty strings.\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"tags\"\n#   #   keys = [\"foo\", \"bar\"]\n#   #   separator = \"_\"\n#\n#   ## The routing tag specifies a tagkey on the metric whose value is used as\n#   ## the message key.  The message key is used to determine which partition to\n#   ## send the message to.  This tag is prefered over the routing_key option.\n#   routing_tag = \"host\"\n#\n#   ## The routing key is set as the message key and used to determine which\n#   ## partition to send the message to.  This value is only used when no\n#   ## routing_tag is set or as a fallback when the tag specified in routing tag\n#   ## is not found.\n#   ##\n#   ## If set to \"random\", a random value will be generated for each message.\n#   ##\n#   ## When unset, no message key is added and each message is routed to a random\n#   ## partition.\n#   ##\n#   ##   ex: routing_key = \"random\"\n#   ##       routing_key = \"telegraf\"\n#   # routing_key = \"\"\n#\n#   ## Compression codec represents the various compression codecs recognized by\n#   ## Kafka in messages.\n#   ##  0 : None\n#   ##  1 : Gzip\n#   ##  2 : Snappy\n#   ##  3 : LZ4\n#   ##  4 : ZSTD\n#   # compression_codec = 0\n#\n#   ## Idempotent Writes\n#   ## If enabled, exactly one copy of each message is written.\n#   # idempotent_writes = false\n#\n#   ##  RequiredAcks is used in Produce Requests to tell the broker how many\n#   ##  replica acknowledgements it must see before responding\n#   ##   0 : the producer never waits for an acknowledgement from the broker.\n#   ##       This option provides the lowest latency but the weakest durability\n#   ##       guarantees (some data will be lost when a server fails).\n#   ##   1 : the producer gets an acknowledgement after the leader replica has\n#   ##       received the data. This option provides better durability as the\n#   ##       client waits until the server acknowledges the request as successful\n#   ##       (only messages that were written to the now-dead leader but not yet\n#   ##       replicated will be lost).\n#   ##   -1: the producer gets an acknowledgement after all in-sync replicas have\n#   ##       received the data. This option provides the best durability, we\n#   ##       guarantee that no messages will be lost as long as at least one in\n#   ##       sync replica remains.\n#   # required_acks = -1\n#\n#   ## The maximum number of times to retry sending a metric before failing\n#   ## until the next flush.\n#   # max_retry = 3\n#\n#   ## The maximum permitted size of a message. Should be set equal to or\n#   ## smaller than the broker's 'message.max.bytes'.\n#   # max_message_bytes = 1000000\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional SOCKS5 proxy to use when connecting to brokers\n#   # socks5_enabled = true\n#   # socks5_address = \"127.0.0.1:1080\"\n#   # socks5_username = \"alice\"\n#   # socks5_password = \"pass123\"\n#\n#   ## Optional SASL Config\n#   # sasl_username = \"kafka\"\n#   # sasl_password = \"secret\"\n#\n#   ## Optional SASL:\n#   ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n#   ## (defaults to PLAIN)\n#   # sasl_mechanism = \"\"\n#\n#   ## used if sasl_mechanism is GSSAPI (experimental)\n#   # sasl_gssapi_service_name = \"\"\n#   # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n#   # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n#   # sasl_gssapi_kerberos_config_path = \"/\"\n#   # sasl_gssapi_realm = \"realm\"\n#   # sasl_gssapi_key_tab_path = \"\"\n#   # sasl_gssapi_disable_pafxfast = false\n#\n#   ## used if sasl_mechanism is OAUTHBEARER (experimental)\n#   # sasl_access_token = \"\"\n#\n#   ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n#   # sasl_version = 1\n#\n#   # Disable Kafka metadata full fetch\n#   # metadata_full = false\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n\n\n# # Configuration for the AWS Kinesis output.\n# [[outputs.kinesis]]\n#   ## Amazon REGION of kinesis endpoint.\n#   region = \"ap-southeast-2\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Kinesis StreamName must exist prior to starting telegraf.\n#   streamname = \"StreamName\"\n#\n#   ## The partition key can be calculated using one of several methods:\n#   ##\n#   ## Use a static value for all writes:\n#   #  [outputs.kinesis.partition]\n#   #    method = \"static\"\n#   #    key = \"howdy\"\n#   #\n#   ## Use a random partition key on each write:\n#   #  [outputs.kinesis.partition]\n#   #    method = \"random\"\n#   #\n#   ## Use the measurement name as the partition key:\n#   #  [outputs.kinesis.partition]\n#   #    method = \"measurement\"\n#   #\n#   ## Use the value of a tag for all writes, if the tag is not set the empty\n#   ## default option will be used. When no default, defaults to \"telegraf\"\n#   #  [outputs.kinesis.partition]\n#   #    method = \"tag\"\n#   #    key = \"host\"\n#   #    default = \"mykey\"\n#\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n#\n#   ## debug will show upstream aws messages.\n#   debug = false\n\n\n# # Configuration for Librato API to send metrics to.\n# [[outputs.librato]]\n#   ## Librato API Docs\n#   ## http://dev.librato.com/v1/metrics-authentication\n#   ## Librato API user\n#   api_user = \"telegraf@influxdb.com\" # required.\n#   ## Librato API token\n#   api_token = \"my-secret-token\" # required.\n#   ## Debug\n#   # debug = false\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n#   ## Output source Template (same as graphite buckets)\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n#   ## This template is used in librato's source (not metric's name)\n#   template = \"host\"\n#\n\n\n# # Send aggregate metrics to Logz.io\n# [[outputs.logzio]]\n#   ## Connection timeout, defaults to \"5s\" if not set.\n#   timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Logz.io account token\n#   token = \"your logz.io token\" # required\n#\n#   ## Use your listener URL for your Logz.io account region.\n#   # url = \"https://listener.logz.io:8071\"\n\n\n# # Send logs to Loki\n# [[outputs.loki]]\n#   ## The domain of Loki\n#   domain = \"https://loki.domain.tld\"\n#\n#   ## Endpoint to write api\n#   # endpoint = \"/loki/api/v1/push\"\n#\n#   ## Connection timeout, defaults to \"5s\" if not set.\n#   # timeout = \"5s\"\n#\n#   ## Basic auth credential\n#   # username = \"loki\"\n#   # password = \"pass\"\n#\n#   ## Additional HTTP headers\n#   # http_headers = {\"X-Scope-OrgID\" = \"1\"}\n#\n#   ## If the request must be gzip encoded\n#   # gzip_request = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n\n\n# # Sends metrics to MongoDB\n# [[outputs.mongodb]]\n#   # connection string examples for mongodb\n#   dsn = \"mongodb://localhost:27017\"\n#   # dsn = \"mongodb://mongod1:27017,mongod2:27017,mongod3:27017/admin\u0026replicaSet=myReplSet\u0026w=1\"\n#\n#   # overrides serverSelectionTimeoutMS in dsn if set\n#   # timeout = \"30s\"\n#\n#   # default authentication, optional\n#   # authentication = \"NONE\"\n#\n#   # for SCRAM-SHA-256 authentication\n#   # authentication = \"SCRAM\"\n#   # username = \"root\"\n#   # password = \"***\"\n#\n#   # for x509 certificate authentication\n#   # authentication = \"X509\"\n#   # tls_ca = \"ca.pem\"\n#   # tls_key = \"client.pem\"\n#   # # tls_key_pwd = \"changeme\" # required for encrypted tls_key\n#   # insecure_skip_verify = false\n#\n#   # database to store measurements and time series collections\n#   # database = \"telegraf\"\n#\n#   # granularity can be seconds, minutes, or hours.\n#   # configuring this value will be based on your input collection frequency.\n#   # see https://docs.mongodb.com/manual/core/timeseries-collections/#create-a-time-series-collection\n#   # granularity = \"seconds\"\n#\n#   # optionally set a TTL to automatically expire documents from the measurement collections.\n#   # ttl = \"360h\"\n\n\n# # Configuration for MQTT server to send metrics to\n# [[outputs.mqtt]]\n#   ## MQTT Brokers\n#   ## The list of brokers should only include the hostname or IP address and the\n#   ## port to the broker. This should follow the format '{host}:{port}'. For\n#   ## example, \"localhost:1883\" or \"127.0.0.1:8883\".\n#   servers = [\"localhost:1883\"]\n#\n#   ## MQTT Topic for Producer Messages\n#   ## MQTT outputs send metrics to this topic format:\n#   ## \u003ctopic_prefix\u003e/\u003chostname\u003e/\u003cpluginname\u003e/ (e.g. prefix/web01.example.com/mem)\n#   topic_prefix = \"telegraf\"\n#\n#   ## QoS policy for messages\n#   ## The mqtt QoS policy for sending messages.\n#   ## See https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.0.0/com.ibm.mq.dev.doc/q029090_.htm\n#   ##   0 = at most once\n#   ##   1 = at least once\n#   ##   2 = exactly once\n#   # qos = 2\n#\n#   ## Keep Alive\n#   ## Defines the maximum length of time that the broker and client may not\n#   ## communicate. Defaults to 0 which turns the feature off.\n#   ##\n#   ## For version v2.0.12 and later mosquitto there is a bug\n#   ## (see https://github.com/eclipse/mosquitto/issues/2117), which requires\n#   ## this to be non-zero. As a reference eclipse/paho.mqtt.golang defaults to 30.\n#   # keep_alive = 0\n#\n#   ## username and password to connect MQTT server.\n#   # username = \"telegraf\"\n#   # password = \"metricsmetricsmetricsmetrics\"\n#\n#   ## client ID\n#   ## The unique client id to connect MQTT server. If this parameter is not set\n#   ## then a random ID is generated.\n#   # client_id = \"\"\n#\n#   ## Timeout for write operations. default: 5s\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## When true, metrics will be sent in one MQTT message per flush. Otherwise,\n#   ## metrics are written one metric per MQTT message.\n#   # batch = false\n#\n#   ## When true, metric will have RETAIN flag set, making broker cache entries until someone\n#   ## actually reads it\n#   # retain = false\n#\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send telegraf measurements to NATS\n# [[outputs.nats]]\n#   ## URLs of NATS servers\n#   servers = [\"nats://localhost:4222\"]\n#\n#   ## Optional client name\n#   # name = \"\"\n#\n#   ## Optional credentials\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional NATS 2.0 and NATS NGS compatible user credentials\n#   # credentials = \"/etc/telegraf/nats.creds\"\n#\n#   ## NATS subject for producer messages\n#   subject = \"telegraf\"\n#\n#   ## Use Transport Layer Security\n#   # secure = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send metrics to New Relic metrics endpoint\n# [[outputs.newrelic]]\n#   ## The 'insights_key' parameter requires a NR license key.\n#   ## New Relic recommends you create one\n#   ## with a convenient name such as TELEGRAF_INSERT_KEY.\n#   ## reference: https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#ingest-license-key\n#   # insights_key = \"New Relic License Key Here\"\n#\n#   ## Prefix to add to add to metric name for easy identification.\n#   ## This is very useful if your metric names are ambiguous.\n#   # metric_prefix = \"\"\n#\n#   ## Timeout for writes to the New Relic API.\n#   # timeout = \"15s\"\n#\n#   ## HTTP Proxy override. If unset use values from the standard\n#   ## proxy environment variables to determine proxy, if any.\n#   # http_proxy = \"http://corporate.proxy:3128\"\n#\n#   ## Metric URL override to enable geographic location endpoints.\n#   # If not set use values from the standard\n#   # metric_url = \"https://metric-api.newrelic.com/metric/v1\"\n\n\n# # Send telegraf measurements to NSQD\n# [[outputs.nsq]]\n#   ## Location of nsqd instance listening on TCP\n#   server = \"localhost:4150\"\n#   ## NSQ topic for producer messages\n#   topic = \"telegraf\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send OpenTelemetry metrics over gRPC\n# [[outputs.opentelemetry]]\n#   ## Override the default (localhost:4317) OpenTelemetry gRPC service\n#   ## address:port\n#   # service_address = \"localhost:4317\"\n#\n#   ## Override the default (5s) request timeout\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config.\n#   ##\n#   ## Root certificates for verifying server certificates encoded in PEM format.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   ## The public and private keypairs for the client encoded in PEM format.\n#   ## May contain intermediate certificates.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS, but skip TLS chain and host verification.\n#   # insecure_skip_verify = false\n#   ## Send the specified TLS server name via SNI.\n#   # tls_server_name = \"foo.example.com\"\n#\n#   ## Override the default (gzip) compression used to send data.\n#   ## Supports: \"gzip\", \"none\"\n#   # compression = \"gzip\"\n#\n#   ## Additional OpenTelemetry resource attributes\n#   # [outputs.opentelemetry.attributes]\n#   # \"service.name\" = \"demo\"\n#\n#   ## Additional gRPC request metadata\n#   # [outputs.opentelemetry.headers]\n#   # key1 = \"value1\"\n\n\n# # Configuration for OpenTSDB server to send metrics to\n# [[outputs.opentsdb]]\n#   ## prefix for metrics keys\n#   prefix = \"my.specific.prefix.\"\n#\n#   ## DNS name of the OpenTSDB server\n#   ## Using \"opentsdb.example.com\" or \"tcp://opentsdb.example.com\" will use the\n#   ## telnet API. \"http://opentsdb.example.com\" will use the Http API.\n#   host = \"opentsdb.example.com\"\n#\n#   ## Port of the OpenTSDB server\n#   port = 4242\n#\n#   ## Number of data points to send to OpenTSDB in Http requests.\n#   ## Not used with telnet API.\n#   http_batch_size = 50\n#\n#   ## URI Path for Http requests to OpenTSDB.\n#   ## Used in cases where OpenTSDB is located behind a reverse proxy.\n#   http_path = \"/api/put\"\n#\n#   ## Debug true - Prints OpenTSDB communication\n#   debug = false\n#\n#   ## Separator separates measurement name from field\n#   separator = \"_\"\n\n\n# # Configuration for the Prometheus client to spawn\n# [[outputs.prometheus_client]]\n#   ## Address to listen on\n#   listen = \":9273\"\n#\n#   ## Metric version controls the mapping from Telegraf metrics into\n#   ## Prometheus format.  When using the prometheus input, use the same value in\n#   ## both plugins to ensure metrics are round-tripped without modification.\n#   ##\n#   ##   example: metric_version = 1;\n#   ##            metric_version = 2; recommended version\n#   # metric_version = 1\n#\n#   ## Use HTTP Basic Authentication.\n#   # basic_username = \"Foo\"\n#   # basic_password = \"Bar\"\n#\n#   ## If set, the IP Ranges which are allowed to access metrics.\n#   ##   ex: ip_range = [\"192.168.0.0/24\", \"192.168.1.0/30\"]\n#   # ip_range = []\n#\n#   ## Path to publish the metrics on.\n#   # path = \"/metrics\"\n#\n#   ## Expiration interval for each metric. 0 == no expiration\n#   # expiration_interval = \"60s\"\n#\n#   ## Collectors to enable, valid entries are \"gocollector\" and \"process\".\n#   ## If unset, both are enabled.\n#   # collectors_exclude = [\"gocollector\", \"process\"]\n#\n#   ## Send string metrics as Prometheus labels.\n#   ## Unless set to false all string metrics will be sent as labels.\n#   # string_as_label = true\n#\n#   ## If set, enable TLS with the given certificate.\n#   # tls_cert = \"/etc/ssl/telegraf.crt\"\n#   # tls_key = \"/etc/ssl/telegraf.key\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Export metric collection time.\n#   # export_timestamp = false\n\n\n# # Configuration for the Riemann server to send metrics to\n# [[outputs.riemann]]\n#   ## The full TCP or UDP URL of the Riemann server\n#   url = \"tcp://localhost:5555\"\n#\n#   ## Riemann event TTL, floating-point time in seconds.\n#   ## Defines how long that an event is considered valid for in Riemann\n#   # ttl = 30.0\n#\n#   ## Separator to use between measurement and field name in Riemann service name\n#   ## This does not have any effect if 'measurement_as_attribute' is set to 'true'\n#   separator = \"/\"\n#\n#   ## Set measurement name as Riemann attribute 'measurement', instead of prepending it to the Riemann service name\n#   # measurement_as_attribute = false\n#\n#   ## Send string metrics as Riemann event states.\n#   ## Unless enabled all string metrics will be ignored\n#   # string_as_state = false\n#\n#   ## A list of tag keys whose values get sent as Riemann tags.\n#   ## If empty, all Telegraf tag values will be sent as tags\n#   # tag_keys = [\"telegraf\",\"custom_tag\"]\n#\n#   ## Additional Riemann tags to send.\n#   # tags = [\"telegraf-output\"]\n#\n#   ## Description for Riemann event\n#   # description_text = \"metrics collected from telegraf\"\n#\n#   ## Riemann client write timeout, defaults to \"5s\" if not set.\n#   # timeout = \"5s\"\n\n\n# # Configuration for the Riemann server to send metrics to\n# [[outputs.riemann_legacy]]\n#   ## DEPRECATED: The 'riemann_legacy' plugin is deprecated in version 1.3.0, use 'outputs.riemann' instead (see https://github.com/influxdata/telegraf/issues/1878).\n#   ## URL of server\n#   url = \"localhost:5555\"\n#   ## transport protocol to use either tcp or udp\n#   transport = \"tcp\"\n#   ## separator to use between input name and field name in Riemann service name\n#   separator = \" \"\n\n\n# # Send aggregate metrics to Sensu Monitor\n# [[outputs.sensu]]\n#   ## BACKEND API URL is the Sensu Backend API root URL to send metrics to\n#   ## (protocol, host, and port only). The output plugin will automatically\n#   ## append the corresponding backend API path\n#   ## /api/core/v2/namespaces/:entity_namespace/events/:entity_name/:check_name).\n#   ##\n#   ## Backend Events API reference:\n#   ## https://docs.sensu.io/sensu-go/latest/api/events/\n#   ##\n#   ## AGENT API URL is the Sensu Agent API root URL to send metrics to\n#   ## (protocol, host, and port only). The output plugin will automatically\n#   ## append the correspeonding agent API path (/events).\n#   ##\n#   ## Agent API Events API reference:\n#   ## https://docs.sensu.io/sensu-go/latest/api/events/\n#   ##\n#   ## NOTE: if backend_api_url and agent_api_url and api_key are set, the output\n#   ## plugin will use backend_api_url. If backend_api_url and agent_api_url are\n#   ## not provided, the output plugin will default to use an agent_api_url of\n#   ## http://127.0.0.1:3031\n#   ##\n#   # backend_api_url = \"http://127.0.0.1:8080\"\n#   # agent_api_url = \"http://127.0.0.1:3031\"\n#\n#   ## API KEY is the Sensu Backend API token\n#   ## Generate a new API token via:\n#   ##\n#   ## $ sensuctl cluster-role create telegraf --verb create --resource events,entities\n#   ## $ sensuctl cluster-role-binding create telegraf --cluster-role telegraf --group telegraf\n#   ## $ sensuctl user create telegraf --group telegraf --password REDACTED\n#   ## $ sensuctl api-key grant telegraf\n#   ##\n#   ## For more information on Sensu RBAC profiles \u0026 API tokens, please visit:\n#   ## - https://docs.sensu.io/sensu-go/latest/reference/rbac/\n#   ## - https://docs.sensu.io/sensu-go/latest/reference/apikeys/\n#   ##\n#   # api_key = \"SENSU_API_KEY\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Timeout for HTTP message\n#   # timeout = \"5s\"\n#\n#   ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## Sensu Event details\n#   ##\n#   ## Below are the event details to be sent to Sensu.  The main portions of the\n#   ## event are the check, entity, and metrics specifications. For more information\n#   ## on Sensu events and its components, please visit:\n#   ## - Events - https://docs.sensu.io/sensu-go/latest/reference/events\n#   ## - Checks -  https://docs.sensu.io/sensu-go/latest/reference/checks\n#   ## - Entities - https://docs.sensu.io/sensu-go/latest/reference/entities\n#   ## - Metrics - https://docs.sensu.io/sensu-go/latest/reference/events#metrics\n#   ##\n#   ## Check specification\n#   ## The check name is the name to give the Sensu check associated with the event\n#   ## created. This maps to check.metatadata.name in the event.\n#   [outputs.sensu.check]\n#   name = \"telegraf\"\n#\n#   ## Entity specification\n#   ## Configure the entity name and namespace, if necessary. This will be part of\n#   ## the entity.metadata in the event.\n#   ##\n#   ## NOTE: if the output plugin is configured to send events to a\n#   ## backend_api_url and entity_name is not set, the value returned by\n#   ## os.Hostname() will be used; if the output plugin is configured to send\n#   ## events to an agent_api_url, entity_name and entity_namespace are not used.\n#   # [outputs.sensu.entity]\n#   #   name = \"server-01\"\n#   #   namespace = \"default\"\n#\n#   ## Metrics specification\n#   ## Configure the tags for the metrics that are sent as part of the Sensu event\n#   # [outputs.sensu.tags]\n#   #   source = \"telegraf\"\n#\n#   ## Configure the handler(s) for processing the provided metrics\n#   # [outputs.sensu.metrics]\n#   #   handlers = [\"influxdb\",\"elasticsearch\"]\n\n\n# # Send metrics and events to SignalFx\n# [[outputs.signalfx]]\n#     ## SignalFx Org Access Token\n#     access_token = \"my-secret-token\"\n#\n#     ## The SignalFx realm that your organization resides in\n#     signalfx_realm = \"us9\"  # Required if ingest_url is not set\n#\n#     ## You can optionally provide a custom ingest url instead of the\n#     ## signalfx_realm option above if you are using a gateway or proxy\n#     ## instance.  This option takes precident over signalfx_realm.\n#     ingest_url = \"https://my-custom-ingest/\"\n#\n#     ## Event typed metrics are omitted by default,\n#     ## If you require an event typed metric you must specify the\n#     ## metric name in the following list.\n#     included_event_names = [\"plugin.metric_name\"]\n\n\n# # Generic socket writer capable of handling multiple socket types.\n# [[outputs.socket_writer]]\n#   ## URL to connect to\n#   # address = \"tcp://127.0.0.1:8094\"\n#   # address = \"tcp://example.com:http\"\n#   # address = \"tcp4://127.0.0.1:8094\"\n#   # address = \"tcp6://127.0.0.1:8094\"\n#   # address = \"tcp6://[2001:db8::1]:8094\"\n#   # address = \"udp://127.0.0.1:8094\"\n#   # address = \"udp4://127.0.0.1:8094\"\n#   # address = \"udp6://127.0.0.1:8094\"\n#   # address = \"unix:///tmp/telegraf.sock\"\n#   # address = \"unixgram:///tmp/telegraf.sock\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Period between keep alive probes.\n#   ## Only applies to TCP sockets.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n#\n#   ## Content encoding for packet-based connections (i.e. UDP, unixgram).\n#   ## Can be set to \"gzip\" or to \"identity\" to apply no encoding.\n#   ##\n#   # content_encoding = \"identity\"\n#\n#   ## Data format to generate.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   # data_format = \"influx\"\n\n\n# # Send metrics to SQL Database\n# [[outputs.sql]]\n#   ## Database driver\n#   ## Valid options: mssql (Microsoft SQL Server), mysql (MySQL), pgx (Postgres),\n#   ##  sqlite (SQLite3), snowflake (snowflake.com) clickhouse (ClickHouse)\n#   # driver = \"\"\n#\n#   ## Data source name\n#   ## The format of the data source name is different for each database driver.\n#   ## See the plugin readme for details.\n#   # data_source_name = \"\"\n#\n#   ## Timestamp column name\n#   # timestamp_column = \"timestamp\"\n#\n#   ## Table creation template\n#   ## Available template variables:\n#   ##  {TABLE} - table name as a quoted identifier\n#   ##  {TABLELITERAL} - table name as a quoted string literal\n#   ##  {COLUMNS} - column definitions (list of quoted identifiers and types)\n#   # table_template = \"CREATE TABLE {TABLE}({COLUMNS})\"\n#\n#   ## Table existence check template\n#   ## Available template variables:\n#   ##  {TABLE} - tablename as a quoted identifier\n#   # table_exists_template = \"SELECT 1 FROM {TABLE} LIMIT 1\"\n#\n#   ## Initialization SQL\n#   # init_sql = \"\"\n#\n#   ## Metric type to SQL type conversion\n#   ## The values on the left are the data types Telegraf has and the values on\n#   ## the right are the data types Telegraf will use when sending to a database.\n#   ##\n#   ## The database values used must be data types the destination database\n#   ## understands. It is up to the user to ensure that the selected data type is\n#   ## available in the database they are using. Refer to your database\n#   ## documentation for what data types are available and supported.\n#   #[outputs.sql.convert]\n#   #  integer              = \"INT\"\n#   #  real                 = \"DOUBLE\"\n#   #  text                 = \"TEXT\"\n#   #  timestamp            = \"TIMESTAMP\"\n#   #  defaultvalue         = \"TEXT\"\n#   #  unsigned             = \"UNSIGNED\"\n#   #  bool                 = \"BOOL\"\n#\n#   ## This setting controls the behavior of the unsigned value. By default the\n#   ## setting will take the integer value and append the unsigned value to it. The other\n#   ## option is \"literal\", which will use the actual value the user provides to\n#   ## the unsigned option. This is useful for a database like ClickHouse where\n#   ## the unsigned value should use a value like \"uint64\".\n#   # conversion_style = \"unsigned_suffix\"\n\n\n# # Configuration for Google Cloud Stackdriver to send metrics to\n# [[outputs.stackdriver]]\n#   ## GCP Project\n#   project = \"erudite-bloom-151019\"\n#\n#   ## The namespace for the metric descriptor\n#   namespace = \"telegraf\"\n#\n#   ## Custom resource type\n#   # resource_type = \"generic_node\"\n#\n#   ## Additional resource labels\n#   # [outputs.stackdriver.resource_labels]\n#   #   node_id = \"$HOSTNAME\"\n#   #   namespace = \"myapp\"\n#   #   location = \"eu-north0\"\n\n\n# # A plugin that can transmit metrics to Sumo Logic HTTP Source\n# [[outputs.sumologic]]\n#   ## Unique URL generated for your HTTP Metrics Source.\n#   ## This is the address to send metrics to.\n#   # url = \"https://events.sumologic.net/receiver/v1/http/\u003cUniqueHTTPCollectorCode\u003e\"\n#\n#   ## Data format to be used for sending metrics.\n#   ## This will set the \"Content-Type\" header accordingly.\n#   ## Currently supported formats:\n#   ## * graphite - for Content-Type of application/vnd.sumologic.graphite\n#   ## * carbon2 - for Content-Type of application/vnd.sumologic.carbon2\n#   ## * prometheus - for Content-Type of application/vnd.sumologic.prometheus\n#   ##\n#   ## More information can be found at:\n#   ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#content-type-headers-for-metrics\n#   ##\n#   ## NOTE:\n#   ## When unset, telegraf will by default use the influx serializer which is currently unsupported\n#   ## in HTTP Source.\n#   data_format = \"carbon2\"\n#\n#   ## Timeout used for HTTP request\n#   # timeout = \"5s\"\n#\n#   ## Max HTTP request body size in bytes before compression (if applied).\n#   ## By default 1MB is recommended.\n#   ## NOTE:\n#   ## Bear in mind that in some serializer a metric even though serialized to multiple\n#   ## lines cannot be split any further so setting this very low might not work\n#   ## as expected.\n#   # max_request_body_size = 1000000\n#\n#   ## Additional, Sumo specific options.\n#   ## Full list can be found here:\n#   ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#supported-http-headers\n#\n#   ## Desired source name.\n#   ## Useful if you want to override the source name configured for the source.\n#   # source_name = \"\"\n#\n#   ## Desired host name.\n#   ## Useful if you want to override the source host configured for the source.\n#   # source_host = \"\"\n#\n#   ## Desired source category.\n#   ## Useful if you want to override the source category configured for the source.\n#   # source_category = \"\"\n#\n#   ## Comma-separated key=value list of dimensions to apply to every metric.\n#   ## Custom dimensions will allow you to query your metrics at a more granular level.\n#   # dimensions = \"\"\n\n\n# # Configuration for Syslog server to send metrics to\n# [[outputs.syslog]]\n#   ## URL to connect to\n#   ## ex: address = \"tcp://127.0.0.1:8094\"\n#   ## ex: address = \"tcp4://127.0.0.1:8094\"\n#   ## ex: address = \"tcp6://127.0.0.1:8094\"\n#   ## ex: address = \"tcp6://[2001:db8::1]:8094\"\n#   ## ex: address = \"udp://127.0.0.1:8094\"\n#   ## ex: address = \"udp4://127.0.0.1:8094\"\n#   ## ex: address = \"udp6://127.0.0.1:8094\"\n#   address = \"tcp://127.0.0.1:8094\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Period between keep alive probes.\n#   ## Only applies to TCP sockets.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n#\n#   ## The framing technique with which it is expected that messages are\n#   ## transported (default = \"octet-counting\").  Whether the messages come\n#   ## using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n#   ## or the non-transparent framing technique (RFC6587#section-3.4.2).  Must\n#   ## be one of \"octet-counting\", \"non-transparent\".\n#   # framing = \"octet-counting\"\n#\n#   ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n#   ## Must be one of \"LF\", or \"NUL\".\n#   # trailer = \"LF\"\n#\n#   ## SD-PARAMs settings\n#   ## Syslog messages can contain key/value pairs within zero or more\n#   ## structured data sections.  For each unrecognized metric tag/field a\n#   ## SD-PARAMS is created.\n#   ##\n#   ## Example:\n#   ##   [[outputs.syslog]]\n#   ##     sdparam_separator = \"_\"\n#   ##     default_sdid = \"default@32473\"\n#   ##     sdids = [\"foo@123\", \"bar@456\"]\n#   ##\n#   ##   input =\u003e xyzzy,x=y foo@123_value=42,bar@456_value2=84,something_else=1\n#   ##   output (structured data only) =\u003e [foo@123 value=42][bar@456 value2=84][default@32473 something_else=1 x=y]\n#\n#   ## SD-PARAMs separator between the sdid and tag/field key (default = \"_\")\n#   # sdparam_separator = \"_\"\n#\n#   ## Default sdid used for tags/fields that don't contain a prefix defined in\n#   ## the explicit sdids setting below If no default is specified, no SD-PARAMs\n#   ## will be used for unrecognized field.\n#   # default_sdid = \"default@32473\"\n#\n#   ## List of explicit prefixes to extract from tag/field keys and use as the\n#   ## SDID, if they match (see above example for more details):\n#   # sdids = [\"foo@123\", \"bar@456\"]\n#\n#   ## Default severity value. Severity and Facility are used to calculate the\n#   ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field\n#   ## with key \"severity_code\" is defined.  If unset, 5 (notice) is the default\n#   # default_severity_code = 5\n#\n#   ## Default facility value. Facility and Severity are used to calculate the\n#   ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field with\n#   ## key \"facility_code\" is defined.  If unset, 1 (user-level) is the default\n#   # default_facility_code = 1\n#\n#   ## Default APP-NAME value (RFC5424#section-6.2.5)\n#   ## Used when no metric tag with key \"appname\" is defined.\n#   ## If unset, \"Telegraf\" is the default\n#   # default_appname = \"Telegraf\"\n\n\n# # Configuration for Amazon Timestream output.\n# [[outputs.timestream]]\n#   ## Amazon Region\n#   region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order:\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Timestream database where the metrics will be inserted.\n#   ## The database must exist prior to starting Telegraf.\n#   database_name = \"yourDatabaseNameHere\"\n#\n#   ## Specifies if the plugin should describe the Timestream database upon starting\n#   ## to validate if it has access necessary permissions, connection, etc., as a safety check.\n#   ## If the describe operation fails, the plugin will not start\n#   ## and therefore the Telegraf agent will not start.\n#   describe_database_on_start = false\n#\n#   ## The mapping mode specifies how Telegraf records are represented in Timestream.\n#   ## Valid values are: single-table, multi-table.\n#   ## For example, consider the following data in line protocol format:\n#   ## weather,location=us-midwest,season=summer temperature=82,humidity=71 1465839830100400200\n#   ## airquality,location=us-west no2=5,pm25=16 1465839830100400200\n#   ## where weather and airquality are the measurement names, location and season are tags,\n#   ## and temperature, humidity, no2, pm25 are fields.\n#   ## In multi-table mode:\n#   ##  - first line will be ingested to table named weather\n#   ##  - second line will be ingested to table named airquality\n#   ##  - the tags will be represented as dimensions\n#   ##  - first table (weather) will have two records:\n#   ##      one with measurement name equals to temperature,\n#   ##      another with measurement name equals to humidity\n#   ##  - second table (airquality) will have two records:\n#   ##      one with measurement name equals to no2,\n#   ##      another with measurement name equals to pm25\n#   ##  - the Timestream tables from the example will look like this:\n#   ##      TABLE \"weather\":\n#   ##        time | location | season | measure_name | measure_value::bigint\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | temperature | 82\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | humidity | 71\n#   ##      TABLE \"airquality\":\n#   ##        time | location | measure_name | measure_value::bigint\n#   ##        2016-06-13 17:43:50 | us-west | no2 | 5\n#   ##        2016-06-13 17:43:50 | us-west | pm25 | 16\n#   ## In single-table mode:\n#   ##  - the data will be ingested to a single table, which name will be valueOf(single_table_name)\n#   ##  - measurement name will stored in dimension named valueOf(single_table_dimension_name_for_telegraf_measurement_name)\n#   ##  - location and season will be represented as dimensions\n#   ##  - temperature, humidity, no2, pm25 will be represented as measurement name\n#   ##  - the Timestream table from the example will look like this:\n#   ##      Assuming:\n#   ##        - single_table_name = \"my_readings\"\n#   ##        - single_table_dimension_name_for_telegraf_measurement_name = \"namespace\"\n#   ##      TABLE \"my_readings\":\n#   ##        time | location | season | namespace | measure_name | measure_value::bigint\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | weather | temperature | 82\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | weather | humidity | 71\n#   ##        2016-06-13 17:43:50 | us-west | NULL | airquality | no2 | 5\n#   ##        2016-06-13 17:43:50 | us-west | NULL | airquality | pm25 | 16\n#   ## In most cases, using multi-table mapping mode is recommended.\n#   ## However, you can consider using single-table in situations when you have thousands of measurement names.\n#   mapping_mode = \"multi-table\"\n#\n#   ## Only valid and required for mapping_mode = \"single-table\"\n#   ## Specifies the Timestream table where the metrics will be uploaded.\n#   # single_table_name = \"yourTableNameHere\"\n#\n#   ## Only valid and required for mapping_mode = \"single-table\"\n#   ## Describes what will be the Timestream dimension name for the Telegraf\n#   ## measurement name.\n#   # single_table_dimension_name_for_telegraf_measurement_name = \"namespace\"\n#\n#   ## Specifies if the plugin should create the table, if the table do not exist.\n#   ## The plugin writes the data without prior checking if the table exists.\n#   ## When the table does not exist, the error returned from Timestream will cause\n#   ## the plugin to create the table, if this parameter is set to true.\n#   create_table_if_not_exists = true\n#\n#   ## Only valid and required if create_table_if_not_exists = true\n#   ## Specifies the Timestream table magnetic store retention period in days.\n#   ## Check Timestream documentation for more details.\n#   create_table_magnetic_store_retention_period_in_days = 365\n#\n#   ## Only valid and required if create_table_if_not_exists = true\n#   ## Specifies the Timestream table memory store retention period in hours.\n#   ## Check Timestream documentation for more details.\n#   create_table_memory_store_retention_period_in_hours = 24\n#\n#   ## Only valid and optional if create_table_if_not_exists = true\n#   ## Specifies the Timestream table tags.\n#   ## Check Timestream documentation for more details\n#   # create_table_tags = { \"foo\" = \"bar\", \"environment\" = \"dev\"}\n#\n#   ## Specify the maximum number of parallel go routines to ingest/write data\n#   ## If not specified, defaulted to 1 go routines\n#   max_write_go_routines = 25\n\n\n# # Write metrics to Warp 10\n# [[outputs.warp10]]\n#   # Prefix to add to the measurement.\n#   prefix = \"telegraf.\"\n#\n#   # URL of the Warp 10 server\n#   warp_url = \"http://localhost:8080\"\n#\n#   # Write token to access your app on warp 10\n#   token = \"Token\"\n#\n#   # Warp 10 query timeout\n#   # timeout = \"15s\"\n#\n#   ## Print Warp 10 error body\n#   # print_error_body = false\n#\n#   ## Max string error size\n#   # max_string_error_size = 511\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Configuration for Wavefront server to send metrics to\n# [[outputs.wavefront]]\n#   ## Url for Wavefront Direct Ingestion. For Wavefront Proxy Ingestion, see\n#   ## the 'host' and 'port' optioins below.\n#   url = \"https://metrics.wavefront.com\"\n#\n#   ## Authentication Token for Wavefront. Only required if using Direct Ingestion\n#   #token = \"DUMMY_TOKEN\"\n#\n#   ## DNS name of the wavefront proxy server. Do not use if url is specified\n#   #host = \"wavefront.example.com\"\n#\n#   ## Port that the Wavefront proxy server listens on. Do not use if url is specified\n#   #port = 2878\n#\n#   ## prefix for metrics keys\n#   #prefix = \"my.specific.prefix.\"\n#\n#   ## whether to use \"value\" for name of simple fields. default is false\n#   #simple_fields = false\n#\n#   ## character to use between metric and field name.  default is . (dot)\n#   #metric_separator = \".\"\n#\n#   ## Convert metric name paths to use metricSeparator character\n#   ## When true will convert all _ (underscore) characters in final metric name. default is true\n#   #convert_paths = true\n#\n#   ## Use Strict rules to sanitize metric and tag names from invalid characters\n#   ## When enabled forward slash (/) and comma (,) will be accepted\n#   #use_strict = false\n#\n#   ## Use Regex to sanitize metric and tag names from invalid characters\n#   ## Regex is more thorough, but significantly slower. default is false\n#   #use_regex = false\n#\n#   ## point tags to use as the source name for Wavefront (if none found, host will be used)\n#   #source_override = [\"hostname\", \"address\", \"agent_host\", \"node_host\"]\n#\n#   ## whether to convert boolean values to numeric values, with false -\u003e 0.0 and true -\u003e 1.0. default is true\n#   #convert_bool = true\n#\n#   ## Truncate metric tags to a total of 254 characters for the tag name value. Wavefront will reject any\n#   ## data point exceeding this limit if not truncated. Defaults to 'false' to provide backwards compatibility.\n#   #truncate_tags = false\n#\n#   ## Flush the internal buffers after each batch. This effectively bypasses the background sending of metrics\n#   ## normally done by the Wavefront SDK. This can be used if you are experiencing buffer overruns. The sending\n#   ## of metrics will block for a longer time, but this will be handled gracefully by the internal buffering in\n#   ## Telegraf.\n#   #immediate_flush = true\n\n\n# # Generic WebSocket output writer.\n# [[outputs.websocket]]\n#   ## URL is the address to send metrics to. Make sure ws or wss scheme is used.\n#   url = \"ws://127.0.0.1:8080/telegraf\"\n#\n#   ## Timeouts (make sure read_timeout is larger than server ping interval or set to zero).\n#   # connect_timeout = \"30s\"\n#   # write_timeout = \"30s\"\n#   # read_timeout = \"30s\"\n#\n#   ## Optionally turn on using text data frames (binary by default).\n#   # use_text_frames = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional SOCKS5 proxy to use\n#   # socks5_enabled = true\n#   # socks5_address = \"127.0.0.1:1080\"\n#   # socks5_username = \"alice\"\n#   # socks5_password = \"pass123\"\n#\n#   ## Data format to output.\n#   ## Each data format has it's own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n#\n#   ## Additional HTTP Upgrade headers\n#   # [outputs.websocket.headers]\n#   #   Authorization = \"Bearer \u003cTOKEN\u003e\"\n\n\n# # Send aggregated metrics to Yandex.Cloud Monitoring\n# [[outputs.yandex_cloud_monitoring]]\n#   ## Timeout for HTTP writes.\n#   # timeout = \"20s\"\n#\n#   ## Yandex.Cloud monitoring API endpoint. Normally should not be changed\n#   # endpoint_url = \"https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write\"\n#\n#   ## All user metrics should be sent with \"custom\" service specified. Normally should not be changed\n#   # service = \"custom\"\n\n\n###############################################################################\n#                            PROCESSOR PLUGINS                                #\n###############################################################################\n\n\n# # Attach AWS EC2 metadata to metrics\n# [[processors.aws_ec2]]\n#   ## Instance identity document tags to attach to metrics.\n#   ## For more information see:\n#   ## https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html\n#   ##\n#   ## Available tags:\n#   ## * accountId\n#   ## * architecture\n#   ## * availabilityZone\n#   ## * billingProducts\n#   ## * imageId\n#   ## * instanceId\n#   ## * instanceType\n#   ## * kernelId\n#   ## * pendingTime\n#   ## * privateIp\n#   ## * ramdiskId\n#   ## * region\n#   ## * version\n#   imds_tags = []\n#\n#   ## EC2 instance tags retrieved with DescribeTags action.\n#   ## In case tag is empty upon retrieval it's omitted when tagging metrics.\n#   ## Note that in order for this to work, role attached to EC2 instance or AWS\n#   ## credentials available from the environment must have a policy attached, that\n#   ## allows ec2:DescribeTags.\n#   ##\n#   ## For more information see:\n#   ## https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeTags.html\n#   ec2_tags = []\n#\n#   ## Timeout for http requests made by against aws ec2 metadata endpoint.\n#   timeout = \"10s\"\n#\n#   ## ordered controls whether or not the metrics need to stay in the same order\n#   ## this plugin received them in. If false, this plugin will change the order\n#   ## with requests hitting cached results moving through immediately and not\n#   ## waiting on slower lookups. This may cause issues for you if you are\n#   ## depending on the order of metrics staying the same. If so, set this to true.\n#   ## Keeping the metrics ordered may be slightly slower.\n#   ordered = false\n#\n#   ## max_parallel_calls is the maximum number of AWS API calls to be in flight\n#   ## at the same time.\n#   ## It's probably best to keep this number fairly low.\n#   max_parallel_calls = 10\n\n\n# # Clone metrics and apply modifications.\n# [[processors.clone]]\n#   ## All modifications on inputs and aggregators can be overridden:\n#   # name_override = \"new_name\"\n#   # name_prefix = \"new_name_prefix\"\n#   # name_suffix = \"new_name_suffix\"\n#\n#   ## Tags to be added (all values must be strings)\n#   # [processors.clone.tags]\n#   #   additional_tag = \"tag_value\"\n\n\n# # Convert values to another metric value type\n# [[processors.converter]]\n#   ## Tags to convert\n#   ##\n#   ## The table key determines the target type, and the array of key-values\n#   ## select the keys to convert.  The array may contain globs.\n#   ##   \u003ctarget-type\u003e = [\u003ctag-key\u003e...]\n#   [processors.converter.tags]\n#     measurement = []\n#     string = []\n#     integer = []\n#     unsigned = []\n#     boolean = []\n#     float = []\n#\n#   ## Fields to convert\n#   ##\n#   ## The table key determines the target type, and the array of key-values\n#   ## select the keys to convert.  The array may contain globs.\n#   ##   \u003ctarget-type\u003e = [\u003cfield-key\u003e...]\n#   [processors.converter.fields]\n#     measurement = []\n#     tag = []\n#     string = []\n#     integer = []\n#     unsigned = []\n#     boolean = []\n#     float = []\n\n\n# # Dates measurements, tags, and fields that pass through this filter.\n# [[processors.date]]\n# \t## New tag to create\n# \ttag_key = \"month\"\n#\n# \t## New field to create (cannot set both field_key and tag_key)\n# \t# field_key = \"month\"\n#\n# \t## Date format string, must be a representation of the Go \"reference time\"\n# \t## which is \"Mon Jan 2 15:04:05 -0700 MST 2006\".\n# \tdate_format = \"Jan\"\n#\n# \t## If destination is a field, date format can also be one of\n# \t## \"unix\", \"unix_ms\", \"unix_us\", or \"unix_ns\", which will insert an integer field.\n# \t# date_format = \"unix\"\n#\n# \t## Offset duration added to the date string when writing the new tag.\n# \t# date_offset = \"0s\"\n#\n# \t## Timezone to use when creating the tag or field using a reference time\n# \t## string.  This can be set to one of \"UTC\", \"Local\", or to a location name\n# \t## in the IANA Time Zone database.\n# \t##   example: timezone = \"America/Los_Angeles\"\n# \t# timezone = \"UTC\"\n\n\n# # Filter metrics with repeating field values\n# [[processors.dedup]]\n#   ## Maximum time to suppress output\n#   dedup_interval = \"600s\"\n\n\n# # Defaults sets default value(s) for specified fields that are not set on incoming metrics.\n# [[processors.defaults]]\n#   ## Ensures a set of fields always exists on your metric(s) with their\n#   ## respective default value.\n#   ## For any given field pair (key = default), if it's not set, a field\n#   ## is set on the metric with the specified default.\n#   ##\n#   ## A field is considered not set if it is nil on the incoming metric;\n#   ## or it is not nil but its value is an empty string or is a string\n#   ## of one or more spaces.\n#   ##   \u003ctarget-field\u003e = \u003cvalue\u003e\n#   # [processors.defaults.fields]\n#   #   field_1 = \"bar\"\n#   #   time_idle = 0\n#   #   is_error = true\n\n\n# # Map enum values according to given table.\n# [[processors.enum]]\n#   [[processors.enum.mapping]]\n#     ## Name of the field to map. Globs accepted.\n#     field = \"status\"\n#\n#     ## Name of the tag to map. Globs accepted.\n#     # tag = \"status\"\n#\n#     ## Destination tag or field to be used for the mapped value.  By default the\n#     ## source tag or field is used, overwriting the original value.\n#     dest = \"status_code\"\n#\n#     ## Default value to be used for all values not contained in the mapping\n#     ## table.  When unset, the unmodified value for the field will be used if no\n#     ## match is found.\n#     # default = 0\n#\n#     ## Table of mappings\n#     [processors.enum.mapping.value_mappings]\n#       green = 1\n#       amber = 2\n#       red = 3\n\n\n# # Run executable as long-running processor plugin\n# [[processors.execd]]\n# \t## Program to run as daemon\n# \t## eg: command = [\"/path/to/your_program\", \"arg1\", \"arg2\"]\n# \tcommand = [\"cat\"]\n#\n#   ## Delay before the process is restarted after an unexpected termination\n#   restart_delay = \"10s\"\n\n\n# # Performs file path manipulations on tags and fields\n# [[processors.filepath]]\n#   ## Treat the tag value as a path and convert it to its last element, storing the result in a new tag\n#   # [[processors.filepath.basename]]\n#   #   tag = \"path\"\n#   #   dest = \"basepath\"\n#\n#   ## Treat the field value as a path and keep all but the last element of path, typically the path's directory\n#   # [[processors.filepath.dirname]]\n#   #   field = \"path\"\n#\n#   ## Treat the tag value as a path, converting it to its the last element without its suffix\n#   # [[processors.filepath.stem]]\n#   #   tag = \"path\"\n#\n#   ## Treat the tag value as a path, converting it to the shortest path name equivalent\n#   ## to path by purely lexical processing\n#   # [[processors.filepath.clean]]\n#   #   tag = \"path\"\n#\n#   ## Treat the tag value as a path, converting it to a relative path that is lexically\n#   ## equivalent to the source path when joined to 'base_path'\n#   # [[processors.filepath.rel]]\n#   #   tag = \"path\"\n#   #   base_path = \"/var/log\"\n#\n#   ## Treat the tag value as a path, replacing each separator character in path with a '/' character. Has only\n#   ## effect on Windows\n#   # [[processors.filepath.toslash]]\n#   #   tag = \"path\"\n\n\n# # Add a tag of the network interface name looked up over SNMP by interface number\n# [[processors.ifname]]\n#   ## Name of tag holding the interface number\n#   # tag = \"ifIndex\"\n#\n#   ## Name of output tag where service name will be added\n#   # dest = \"ifName\"\n#\n#   ## Name of tag of the SNMP agent to request the interface name from\n#   # agent = \"agent\"\n#\n#   ## Timeout for each request.\n#   # timeout = \"5s\"\n#\n#   ## SNMP version; can be 1, 2, or 3.\n#   # version = 2\n#\n#   ## SNMP community string.\n#   # community = \"public\"\n#\n#   ## Number of retries to attempt.\n#   # retries = 3\n#\n#   ## The GETBULK max-repetitions parameter.\n#   # max_repetitions = 10\n#\n#   ## SNMPv3 authentication and encryption options.\n#   ##\n#   ## Security Name.\n#   # sec_name = \"myuser\"\n#   ## Authentication protocol; one of \"MD5\", \"SHA\", or \"\".\n#   # auth_protocol = \"MD5\"\n#   ## Authentication password.\n#   # auth_password = \"pass\"\n#   ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n#   # sec_level = \"authNoPriv\"\n#   ## Context Name.\n#   # context_name = \"\"\n#   ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\" or \"\".\n#   # priv_protocol = \"\"\n#   ## Privacy password used for encrypted messages.\n#   # priv_password = \"\"\n#\n#   ## max_parallel_lookups is the maximum number of SNMP requests to\n#   ## make at the same time.\n#   # max_parallel_lookups = 100\n#\n#   ## ordered controls whether or not the metrics need to stay in the\n#   ## same order this plugin received them in. If false, this plugin\n#   ## may change the order when data is cached.  If you need metrics to\n#   ## stay in order set this to true.  keeping the metrics ordered may\n#   ## be slightly slower\n#   # ordered = false\n#\n#   ## cache_ttl is the amount of time interface names are cached for a\n#   ## given agent.  After this period elapses if names are needed they\n#   ## will be retrieved again.\n#   # cache_ttl = \"8h\"\n\n\n# # Adds noise to numerical fields\n# [[processors.noise]]\n#     ## Specified the type of the random distribution.\n#     ## Can be \"laplacian\", \"gaussian\" or \"uniform\".\n#     # type = \"laplacian\n#\n#     ## Center of the distribution.\n#     ## Only used for Laplacian and Gaussian distributions.\n#     # mu = 0.0\n#\n#     ## Scale parameter for the Laplacian or Gaussian distribution\n#     # scale = 1.0\n#\n#     ## Upper and lower bound of the Uniform distribution\n#     # min = -1.0\n#     # max = 1.0\n#\n#     ## Apply the noise only to numeric fields matching the filter criteria below.\n#     ## Excludes takes precedence over includes.\n#     # include_fields = []\n#     # exclude_fields = []\n\n\n# # Apply metric modifications using override semantics.\n# [[processors.override]]\n#   ## All modifications on inputs and aggregators can be overridden:\n#   # name_override = \"new_name\"\n#   # name_prefix = \"new_name_prefix\"\n#   # name_suffix = \"new_name_suffix\"\n#\n#   ## Tags to be added (all values must be strings)\n#   # [processors.override.tags]\n#   #   additional_tag = \"tag_value\"\n\n\n# # Parse a value in a specified field/tag(s) and add the result in a new metric\n# [[processors.parser]]\n#   ## The name of the fields whose value will be parsed.\n#   parse_fields = []\n#\n#   ## If true, incoming metrics are not emitted.\n#   drop_original = false\n#\n#   ## If set to override, emitted metrics will be merged by overriding the\n#   ## original metric using the newly parsed metrics.\n#   merge = \"override\"\n#\n#   ## The dataformat to be read from files\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Rotate a single valued metric into a multi field metric\n# [[processors.pivot]]\n#   ## Tag to use for naming the new field.\n#   tag_key = \"name\"\n#   ## Field to use as the value of the new field.\n#   value_key = \"value\"\n\n\n# # Given a tag/field of a TCP or UDP port number, add a tag/field of the service name looked up in the system services file\n# [[processors.port_name]]\n# [[processors.port_name]]\n#   ## Name of tag holding the port number\n#   # tag = \"port\"\n#   ## Or name of the field holding the port number\n#   # field = \"port\"\n#\n#   ## Name of output tag or field (depending on the source) where service name will be added\n#   # dest = \"service\"\n#\n#   ## Default tcp or udp\n#   # default_protocol = \"tcp\"\n#\n#   ## Tag containing the protocol (tcp or udp, case-insensitive)\n#   # protocol_tag = \"proto\"\n#\n#   ## Field containing the protocol (tcp or udp, case-insensitive)\n#   # protocol_field = \"proto\"\n\n\n# # Print all metrics that pass through this filter.\n# [[processors.printer]]\n\n\n# # Transforms tag and field values as well as measurement, tag and field names with regex pattern\n# [[processors.regex]]\n#   ## Tag and field conversions defined in a separate sub-tables\n#   # [[processors.regex.tags]]\n#   #   ## Tag to change\n#   #   key = \"resp_code\"\n#   #   ## Regular expression to match on a tag value\n#   #   pattern = \"^(\\\\d)\\\\d\\\\d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use ${1}\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"${1}xx\"\n#\n#   # [[processors.regex.fields]]\n#   #   ## Field to change\n#   #   key = \"request\"\n#   #   ## All the power of the Go regular expressions available here\n#   #   ## For example, named subgroups\n#   #   pattern = \"^/api(?P\u003cmethod\u003e/[\\\\w/]+)\\\\S*\"\n#   #   replacement = \"method\"\n#   #   ## If result_key is present, a new field will be created\n#   #   ## instead of changing existing field\n#   #   result_key = \"method\"\n#\n#   ## Multiple conversions may be applied for one field sequentially\n#   ## Let's extract one more value\n#   # [[processors.regex.fields]]\n#   #   key = \"request\"\n#   #   pattern = \".*category=(\\\\w+).*\"\n#   #   replacement = \"${1}\"\n#   #   result_key = \"search_category\"\n#\n#   ## Rename metric fields\n#   # [[processors.regex.field_rename]]\n#   #   ## Regular expression to match on a field name\n#   #   pattern = \"^search_(\\\\w+)d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use ${1}\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"${1}\"\n#   #   ## If the new field name already exists, you can either \"overwrite\" the\n#   #   ## existing one with the value of the renamed field OR you can \"keep\"\n#   #   ## both the existing and source field.\n#   #   # result_key = \"keep\"\n#\n#   ## Rename metric tags\n#   # [[processors.regex.tag_rename]]\n#   #   ## Regular expression to match on a tag name\n#   #   pattern = \"^search_(\\\\w+)d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use ${1}\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"${1}\"\n#   #   ## If the new tag name already exists, you can either \"overwrite\" the\n#   #   ## existing one with the value of the renamed tag OR you can \"keep\"\n#   #   ## both the existing and source tag.\n#   #   # result_key = \"keep\"\n#\n#   ## Rename metrics\n#   # [[processors.regex.metric_rename]]\n#   #   ## Regular expression to match on an metric name\n#   #   pattern = \"^search_(\\\\w+)d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use ${1}\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"${1}\"\n\n\n# # Rename measurements, tags, and fields that pass through this filter.\n# [[processors.rename]]\n\n\n# # ReverseDNS does a reverse lookup on IP addresses to retrieve the DNS name\n# [[processors.reverse_dns]]\n#   ## For optimal performance, you may want to limit which metrics are passed to this\n#   ## processor. eg:\n#   ## namepass = [\"my_metric_*\"]\n#\n#   ## cache_ttl is how long the dns entries should stay cached for.\n#   ## generally longer is better, but if you expect a large number of diverse lookups\n#   ## you'll want to consider memory use.\n#   cache_ttl = \"24h\"\n#\n#   ## lookup_timeout is how long should you wait for a single dns request to repsond.\n#   ## this is also the maximum acceptable latency for a metric travelling through\n#   ## the reverse_dns processor. After lookup_timeout is exceeded, a metric will\n#   ## be passed on unaltered.\n#   ## multiple simultaneous resolution requests for the same IP will only make a\n#   ## single rDNS request, and they will all wait for the answer for this long.\n#   lookup_timeout = \"3s\"\n#\n#   ## max_parallel_lookups is the maximum number of dns requests to be in flight\n#   ## at the same time. Requesting hitting cached values do not count against this\n#   ## total, and neither do mulptiple requests for the same IP.\n#   ## It's probably best to keep this number fairly low.\n#   max_parallel_lookups = 10\n#\n#   ## ordered controls whether or not the metrics need to stay in the same order\n#   ## this plugin received them in. If false, this plugin will change the order\n#   ## with requests hitting cached results moving through immediately and not\n#   ## waiting on slower lookups. This may cause issues for you if you are\n#   ## depending on the order of metrics staying the same. If so, set this to true.\n#   ## keeping the metrics ordered may be slightly slower.\n#   ordered = false\n#\n#   [[processors.reverse_dns.lookup]]\n#     ## get the ip from the field \"source_ip\", and put the result in the field \"source_name\"\n#     field = \"source_ip\"\n#     dest = \"source_name\"\n#\n#   [[processors.reverse_dns.lookup]]\n#     ## get the ip from the tag \"destination_ip\", and put the result in the tag\n#     ## \"destination_name\".\n#     tag = \"destination_ip\"\n#     dest = \"destination_name\"\n#\n#     ## If you would prefer destination_name to be a field instead, you can use a\n#     ## processors.converter after this one, specifying the order attribute.\n\n\n# # Add the S2 Cell ID as a tag based on latitude and longitude fields\n# [[processors.s2geo]]\n#   ## The name of the lat and lon fields containing WGS-84 latitude and\n#   ## longitude in decimal degrees.\n#   # lat_field = \"lat\"\n#   # lon_field = \"lon\"\n#\n#   ## New tag to create\n#   # tag_key = \"s2_cell_id\"\n#\n#   ## Cell level (see https://s2geometry.io/resources/s2cell_statistics.html)\n#   # cell_level = 9\n\n\n# # Process metrics using a Starlark script\n# [[processors.starlark]]\n#   ## The Starlark source can be set as a string in this configuration file, or\n#   ## by referencing a file containing the script.  Only one source or script\n#   ## should be set at once.\n#   ##\n#   ## Source of the Starlark script.\n#   source = '''\n# def apply(metric):\n# \treturn metric\n# '''\n#\n#   ## File containing a Starlark script.\n#   # script = \"/usr/local/bin/myscript.star\"\n#\n#   ## The constants of the Starlark script.\n#   # [processors.starlark.constants]\n#   #   max_size = 10\n#   #   threshold = 0.75\n#   #   default_name = \"Julia\"\n#   #   debug_mode = true\n\n\n# # Perform string processing on tags, fields, and measurements\n# [[processors.strings]]\n#   ## Convert a tag value to uppercase\n#   # [[processors.strings.uppercase]]\n#   #   tag = \"method\"\n#\n#   ## Convert a field value to lowercase and store in a new field\n#   # [[processors.strings.lowercase]]\n#   #   field = \"uri_stem\"\n#   #   dest = \"uri_stem_normalised\"\n#\n#   ## Convert a field value to titlecase\n#   # [[processors.strings.titlecase]]\n#   #   field = \"status\"\n#\n#   ## Trim leading and trailing whitespace using the default cutset\n#   # [[processors.strings.trim]]\n#   #   field = \"message\"\n#\n#   ## Trim leading characters in cutset\n#   # [[processors.strings.trim_left]]\n#   #   field = \"message\"\n#   #   cutset = \"\\t\"\n#\n#   ## Trim trailing characters in cutset\n#   # [[processors.strings.trim_right]]\n#   #   field = \"message\"\n#   #   cutset = \"\\r\\n\"\n#\n#   ## Trim the given prefix from the field\n#   # [[processors.strings.trim_prefix]]\n#   #   field = \"my_value\"\n#   #   prefix = \"my_\"\n#\n#   ## Trim the given suffix from the field\n#   # [[processors.strings.trim_suffix]]\n#   #   field = \"read_count\"\n#   #   suffix = \"_count\"\n#\n#   ## Replace all non-overlapping instances of old with new\n#   # [[processors.strings.replace]]\n#   #   measurement = \"*\"\n#   #   old = \":\"\n#   #   new = \"_\"\n#\n#   ## Trims strings based on width\n#   # [[processors.strings.left]]\n#   #   field = \"message\"\n#   #   width = 10\n#\n#   ## Decode a base64 encoded utf-8 string\n#   # [[processors.strings.base64decode]]\n#   #   field = \"message\"\n#\n#   ## Sanitize a string to ensure it is a valid utf-8 string\n#   ## Each run of invalid UTF-8 byte sequences is replaced by the replacement string, which may be empty\n#   # [[processors.strings.valid_utf8]]\n#   #   field = \"message\"\n#   #   replacement = \"\"\n\n\n# # Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.\n# [[processors.tag_limit]]\n#   ## Maximum number of tags to preserve\n#   limit = 10\n#\n#   ## List of tags to preferentially preserve\n#   keep = [\"foo\", \"bar\", \"baz\"]\n\n\n# # Uses a Go template to create a new tag\n# [[processors.template]]\n#   ## Tag to set with the output of the template.\n#   tag = \"topic\"\n#\n#   ## Go template used to create the tag value.  In order to ease TOML\n#   ## escaping requirements, you may wish to use single quotes around the\n#   ## template string.\n#   template = '{{ .Tag \"hostname\" }}.{{ .Tag \"level\" }}'\n\n\n# # Print all metrics that pass through this filter.\n# [[processors.topk]]\n#   ## How many seconds between aggregations\n#   # period = 10\n#\n#   ## How many top metrics to return\n#   # k = 10\n#\n#   ## Over which tags should the aggregation be done. Globs can be specified, in\n#   ## which case any tag matching the glob will aggregated over. If set to an\n#   ## empty list is no aggregation over tags is done\n#   # group_by = ['*']\n#\n#   ## Over which fields are the top k are calculated\n#   # fields = [\"value\"]\n#\n#   ## What aggregation to use. Options: sum, mean, min, max\n#   # aggregation = \"mean\"\n#\n#   ## Instead of the top k largest metrics, return the bottom k lowest metrics\n#   # bottomk = false\n#\n#   ## The plugin assigns each metric a GroupBy tag generated from its name and\n#   ## tags. If this setting is different than \"\" the plugin will add a\n#   ## tag (which name will be the value of this setting) to each metric with\n#   ## the value of the calculated GroupBy tag. Useful for debugging\n#   # add_groupby_tag = \"\"\n#\n#   ## These settings provide a way to know the position of each metric in\n#   ## the top k. The 'add_rank_field' setting allows to specify for which\n#   ## fields the position is required. If the list is non empty, then a field\n#   ## will be added to each and every metric for each string present in this\n#   ## setting. This field will contain the ranking of the group that\n#   ## the metric belonged to when aggregated over that field.\n#   ## The name of the field will be set to the name of the aggregation field,\n#   ## suffixed with the string '_topk_rank'\n#   # add_rank_fields = []\n#\n#   ## These settings provide a way to know what values the plugin is generating\n#   ## when aggregating metrics. The 'add_aggregate_field' setting allows to\n#   ## specify for which fields the final aggregation value is required. If the\n#   ## list is non empty, then a field will be added to each every metric for\n#   ## each field present in this setting. This field will contain\n#   ## the computed aggregation for the group that the metric belonged to when\n#   ## aggregated over that field.\n#   ## The name of the field will be set to the name of the aggregation field,\n#   ## suffixed with the string '_topk_aggregate'\n#   # add_aggregate_fields = []\n\n\n# # Rotate multi field metric into several single field metrics\n# [[processors.unpivot]]\n#   ## Tag to use for the name.\n#   tag_key = \"name\"\n#   ## Field to use for the name of the value.\n#   value_key = \"value\"\n\n\n###############################################################################\n#                            AGGREGATOR PLUGINS                               #\n###############################################################################\n\n\n# # Keep the aggregate basicstats of each metric passing through.\n# [[aggregators.basicstats]]\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## Configures which basic stats to push as fields\n#   # stats = [\"count\", \"min\", \"max\", \"mean\", \"stdev\", \"s2\", \"sum\"]\n\n\n# # Calculates a derivative for every field.\n# [[aggregators.derivative]]\n# \t## The period in which to flush the aggregator.\n# \tperiod = \"30s\"\n# \t##\n# \t## If true, the original metric will be dropped by the\n# \t## aggregator and will not get sent to the output plugins.\n# \tdrop_original = false\n# \t##\n# \t## This aggregator will estimate a derivative for each field, which is\n# \t## contained in both the first and last metric of the aggregation interval.\n# \t## Without further configuration the derivative will be calculated with\n# \t## respect to the time difference between these two measurements in seconds.\n# \t## The formula applied is for every field:\n# \t##\n# \t##               value_last - value_first\n# \t## derivative = --------------------------\n# \t##              time_difference_in_seconds\n# \t##\n# \t## The resulting derivative will be named *fieldname_rate*. The suffix\n# \t## \"_rate\" can be configured by the *suffix* parameter. When using a\n# \t## derivation variable you can include its name for more clarity.\n# \t# suffix = \"_rate\"\n# \t##\n# \t## As an abstraction the derivative can be calculated not only by the time\n# \t## difference but by the difference of a field, which is contained in the\n# \t## measurement. This field is assumed to be monotonously increasing. This\n# \t## feature is used by specifying a *variable*.\n# \t## Make sure the specified variable is not filtered and exists in the metrics\n# \t## passed to this aggregator!\n# \t# variable = \"\"\n# \t##\n# \t## When using a field as the derivation parameter the name of that field will\n# \t## be used for the resulting derivative, e.g. *fieldname_by_parameter*.\n# \t##\n# \t## Note, that the calculation is based on the actual timestamp of the\n# \t## measurements. When there is only one measurement during that period, the\n# \t## measurement will be rolled over to the next period. The maximum number of\n# \t## such roll-overs can be configured with a default of 10.\n# \t# max_roll_over = 10\n# \t##\n\n\n# # Report the final metric of a series\n# [[aggregators.final]]\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## The time that a series is not updated until considering it final.\n#   series_timeout = \"5m\"\n\n\n# # Create aggregate histograms.\n# [[aggregators.histogram]]\n#   ## The period in which to flush the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## If true, the histogram will be reset on flush instead\n#   ## of accumulating the results.\n#   reset = false\n#\n#   ## Whether bucket values should be accumulated. If set to false, \"gt\" tag will be added.\n#   ## Defaults to true.\n#   cumulative = true\n#\n#   ## Expiration interval for each histogram. The histogram will be expired if\n#   ## there are no changes in any buckets for this time interval. 0 == no expiration.\n#   # expiration_interval = \"0m\"\n#\n#   ## If true, aggregated histogram are pushed to output only if it was updated since\n#   ## previous push. Defaults to false.\n#   # push_only_on_update = false\n#\n#   ## Example config that aggregates all fields of the metric.\n#   # [[aggregators.histogram.config]]\n#   #   ## Right borders of buckets (with +Inf implicitly added).\n#   #   buckets = [0.0, 15.6, 34.5, 49.1, 71.5, 80.5, 94.5, 100.0]\n#   #   ## The name of metric.\n#   #   measurement_name = \"cpu\"\n#\n#   ## Example config that aggregates only specific fields of the metric.\n#   # [[aggregators.histogram.config]]\n#   #   ## Right borders of buckets (with +Inf implicitly added).\n#   #   buckets = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n#   #   ## The name of metric.\n#   #   measurement_name = \"diskio\"\n#   #   ## The concrete fields of metric\n#   #   fields = [\"io_time\", \"read_time\", \"write_time\"]\n\n\n# # Merge metrics into multifield metrics by series key\n# [[aggregators.merge]]\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = true\n\n\n# # Keep the aggregate min/max of each metric passing through.\n# [[aggregators.minmax]]\n#   ## General Aggregator Arguments:\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n\n\n# # Keep the aggregate quantiles of each metric passing through.\n# [[aggregators.quantile]]\n#   ## General Aggregator Arguments:\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## Quantiles to output in the range [0,1]\n#   # quantiles = [0.25, 0.5, 0.75]\n#\n#   ## Type of aggregation algorithm\n#   ## Supported are:\n#   ##  \"t-digest\" -- approximation using centroids, can cope with large number of samples\n#   ##  \"exact R7\" -- exact computation also used by Excel or NumPy (Hyndman \u0026 Fan 1996 R7)\n#   ##  \"exact R8\" -- exact computation (Hyndman \u0026 Fan 1996 R8)\n#   ## NOTE: Do not use \"exact\" algorithms with large number of samples\n#   ##       to not impair performance or memory consumption!\n#   # algorithm = \"t-digest\"\n#\n#   ## Compression for approximation (t-digest). The value needs to be\n#   ## greater or equal to 1.0. Smaller values will result in more\n#   ## performance but less accuracy.\n#   # compression = 100.0\n\n\n# # Aggregate metrics using a Starlark script\n# [[aggregators.starlark]]\n#   ## The Starlark source can be set as a string in this configuration file, or\n#   ## by referencing a file containing the script.  Only one source or script\n#   ## should be set at once.\n#   ##\n#   ## Source of the Starlark script.\n#   source = '''\n# state = {}\n#\n# def add(metric):\n#   state[\"last\"] = metric\n#\n# def push():\n#   return state.get(\"last\")\n#\n# def reset():\n#   state.clear()\n# '''\n#\n#   ## File containing a Starlark script.\n#   # script = \"/usr/local/bin/myscript.star\"\n#\n#   ## The constants of the Starlark script.\n#   # [aggregators.starlark.constants]\n#   #   max_size = 10\n#   #   threshold = 0.75\n#   #   default_name = \"Julia\"\n#   #   debug_mode = true\n\n\n# # Count the occurrence of values in fields.\n# [[aggregators.valuecounter]]\n#   ## General Aggregator Arguments:\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#   ## The fields for which the values will be counted\n#   fields = []\n\n\n###############################################################################\n#                            INPUT PLUGINS                                    #\n###############################################################################\n\n\n# Read metrics about cpu usage\n[[inputs.cpu]]\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states\n  report_active = false\n\n\n# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n\n# Read metrics about disk IO by device\n[[inputs.diskio]]\n  ## By default, telegraf will gather stats for all devices including\n  ## disk partitions.\n  ## Setting devices will restrict the stats to the specified devices.\n  # devices = [\"sda\", \"sdb\", \"vd*\"]\n  ## Uncomment the following line if you need disk serial numbers.\n  # skip_serial_number = false\n  #\n  ## On systems which support it, device metadata can be added in the form of\n  ## tags.\n  ## Currently only Linux is supported via udev properties. You can view\n  ## available properties for a device by running:\n  ## 'udevadm info -q property -n /dev/sda'\n  ## Note: Most, but not all, udev properties can be accessed this way. Properties\n  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.\n  # device_tags = [\"ID_FS_TYPE\", \"ID_FS_USAGE\"]\n  #\n  ## Using the same metadata source as device_tags, you can also customize the\n  ## name of the device via templates.\n  ## The 'name_templates' parameter is a list of templates to try and apply to\n\n  ## present for the device is used as the device name tag.\n  ## The typical use case is for LVM volumes, to get the VG/LV name instead of\n  ## the near-meaningless DM-0 name.\n  # name_templates = [\"$ID_FS_LABEL\",\"$DM_VG_NAME/$DM_LV_NAME\"]\n\n\n# Get kernel statistics from /proc/stat\n[[inputs.kernel]]\n  # no configuration\n\n\n# Read metrics about memory usage\n[[inputs.mem]]\n  # no configuration\n\n\n# Get the number of processes and group them by status\n[[inputs.processes]]\n  # no configuration\n\n\n# Read metrics about swap memory usage\n[[inputs.swap]]\n  # no configuration\n\n\n# Read metrics about system load \u0026 uptime\n[[inputs.system]]\n  ## Uncomment to remove deprecated metrics.\n  # fielddrop = [\"uptime_format\"]\n\n\n# # Gather ActiveMQ metrics\n# [[inputs.activemq]]\n#   ## ActiveMQ WebConsole URL\n#   url = \"http://127.0.0.1:8161\"\n#\n#   ## Credentials for basic HTTP authentication\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## Required ActiveMQ webadmin root path\n#   # webadmin = \"admin\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read stats from aerospike server(s)\n# [[inputs.aerospike]]\n#   ## Aerospike servers to connect to (with port)\n#   ## This plugin will query all namespaces the aerospike\n#   ## server has configured and get stats for them.\n#   servers = [\"localhost:3000\"]\n#\n#   # username = \"telegraf\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # enable_tls = false\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   # tls_name = \"tlsname\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n#\n#   # Feature Options\n#   # Add namespace variable to limit the namespaces executed on\n#   # Leave blank to do all\n#   # disable_query_namespaces = true # default false\n#   # namespaces = [\"namespace1\", \"namespace2\"]\n#\n#   # Enable set level telemetry\n#   # query_sets = true # default: false\n#   # Add namespace set combinations to limit sets executed on\n#   # Leave blank to do all sets\n#   # sets = [\"namespace1/set1\", \"namespace1/set2\", \"namespace3\"]\n#\n#   # Histograms\n#   # enable_ttl_histogram = true # default: false\n#   # enable_object_size_linear_histogram = true # default: false\n#\n#   # by default, aerospike produces a 100 bucket histogram\n#   # this is not great for most graphing tools, this will allow\n#   # the ability to squash this to a smaller number of buckets\n#   # To have a balanced histogram, the number of buckets chosen\n#   # should divide evenly into 100.\n#   # num_histogram_buckets = 100 # default: 10\n\n\n# # Query statistics from AMD Graphics cards using rocm-smi binary\n# [[inputs.amd_rocm_smi]]\n# ## Optional: path to rocm-smi binary, defaults to $PATH via exec.LookPath\n# # bin_path = \"/opt/rocm/bin/rocm-smi\"\n#\n# ## Optional: timeout for GPU polling\n# # timeout = \"5s\"\n\n\n# # Read Apache status information (mod_status)\n# [[inputs.apache]]\n#   ## An array of URLs to gather from, must be directed at the machine\n#   ## readable version of the mod_status page including the auto query string.\n#   ## Default is \"http://localhost/server-status?auto\".\n#   urls = [\"http://localhost/server-status?auto\"]\n#\n#   ## Credentials for basic HTTP authentication.\n#   # username = \"myuser\"\n#   # password = \"mypassword\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Monitor APC UPSes connected to apcupsd\n# [[inputs.apcupsd]]\n#   # A list of running apcupsd server to connect to.\n#   # If not provided will default to tcp://127.0.0.1:3551\n#   servers = [\"tcp://127.0.0.1:3551\"]\n#\n#   ## Timeout for dialing server.\n#   timeout = \"5s\"\n\n\n# # Gather metrics from Apache Aurora schedulers\n# [[inputs.aurora]]\n#   ## Schedulers are the base addresses of your Aurora Schedulers\n#   schedulers = [\"http://127.0.0.1:8081\"]\n#\n#   ## Set of role types to collect metrics from.\n#   ##\n#   ## The scheduler roles are checked each interval by contacting the\n#   ## scheduler nodes; zookeeper is not contacted.\n#   # roles = [\"leader\", \"follower\"]\n#\n#   ## Timeout is the max time for total network operations.\n#   # timeout = \"5s\"\n#\n#   ## Username and password are sent using HTTP Basic Auth.\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Gather Azure Storage Queue metrics\n# [[inputs.azure_storage_queue]]\n#   ## Required Azure Storage Account name\n#   account_name = \"mystorageaccount\"\n#\n#   ## Required Azure Storage Account access key\n#   account_key = \"storageaccountaccesskey\"\n#\n#   ## Set to false to disable peeking age of oldest message (executes faster)\n#   # peek_oldest_message_age = true\n\n\n# # Read metrics of bcache from stats_total and dirty_data\n# [[inputs.bcache]]\n#   ## Bcache sets path\n#   ## If not specified, then default is:\n#   bcachePath = \"/sys/fs/bcache\"\n#\n#   ## By default, Telegraf gather stats for all bcache devices\n#   ## Setting devices will restrict the stats to the specified\n#   ## bcache devices.\n#   bcacheDevs = [\"bcache0\"]\n\n\n# # Collects Beanstalkd server and tubes stats\n# [[inputs.beanstalkd]]\n#   ## Server to collect data from\n#   server = \"localhost:11300\"\n#\n#   ## List of tubes to gather stats about.\n#   ## If no tubes specified then data gathered for each tube on server reported by list-tubes command\n#   tubes = [\"notifications\"]\n\n\n# # Read metrics exposed by Beat\n# [[inputs.beat]]\n#   ## An URL from which to read Beat-formatted JSON\n#   ## Default is \"http://127.0.0.1:5066\".\n#   url = \"http://127.0.0.1:5066\"\n#\n#   ## Enable collection of the listed stats\n#   ## An empty list means collect all. Available options are currently\n#   ## \"beat\", \"libbeat\", \"system\" and \"filebeat\".\n#   # include = [\"beat\", \"libbeat\", \"filebeat\"]\n#\n#   ## HTTP method\n#   # method = \"GET\"\n#\n#   ## Optional HTTP headers\n#   # headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## Override HTTP \"Host\" header\n#   # host_header = \"logstash.example.com\"\n#\n#   ## Timeout for HTTP requests\n#   # timeout = \"5s\"\n#\n#   ## Optional HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read BIND nameserver XML statistics\n# [[inputs.bind]]\n#   ## An array of BIND XML statistics URI to gather stats.\n#   ## Default is \"http://localhost:8053/xml/v3\".\n#   # urls = [\"http://localhost:8053/xml/v3\"]\n#   # gather_memory_contexts = false\n#   # gather_views = false\n#\n#   ## Timeout for http requests made by bind nameserver\n#   # timeout = \"4s\"\n\n\n# # Collect bond interface status, slaves statuses and failures count\n# [[inputs.bond]]\n#   ## Sets 'proc' directory path\n#   ## If not specified, then default is /proc\n#   # host_proc = \"/proc\"\n#\n#   ## Sets 'sys' directory path\n#   ## If not specified, then default is /sys\n#   # host_sys = \"/sys\"\n#\n#   ## By default, telegraf gather stats for all bond interfaces\n#   ## Setting interfaces will restrict the stats to the specified\n#   ## bond interfaces.\n#   # bond_interfaces = [\"bond0\"]\n#\n#   ## Tries to collect additional bond details from /sys/class/net/{bond}\n#   ## currently only useful for LACP (mode 4) bonds\n#   # collect_sys_details = false\n#\n\n\n# # Collect Kafka topics and consumers status from Burrow HTTP API.\n# [[inputs.burrow]]\n#   ## Burrow API endpoints in format \"schema://host:port\".\n#   ## Default is \"http://localhost:8000\".\n#   servers = [\"http://localhost:8000\"]\n#\n#   ## Override Burrow API prefix.\n#   ## Useful when Burrow is behind reverse-proxy.\n#   # api_prefix = \"/v3/kafka\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Limit per-server concurrent connections.\n#   ## Useful in case of large number of topics or consumer groups.\n#   # concurrent_connections = 20\n#\n#   ## Filter clusters, default is no filtering.\n#   ## Values can be specified as glob patterns.\n#   # clusters_include = []\n#   # clusters_exclude = []\n#\n#   ## Filter consumer groups, default is no filtering.\n#   ## Values can be specified as glob patterns.\n#   # groups_include = []\n#   # groups_exclude = []\n#\n#   ## Filter topics, default is no filtering.\n#   ## Values can be specified as glob patterns.\n#   # topics_include = []\n#   # topics_exclude = []\n#\n#   ## Credentials for basic HTTP authentication.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional SSL config\n#   # ssl_ca = \"/etc/telegraf/ca.pem\"\n#   # ssl_cert = \"/etc/telegraf/cert.pem\"\n#   # ssl_key = \"/etc/telegraf/key.pem\"\n#   # insecure_skip_verify = false\n\n\n# # Collects performance metrics from the MON, OSD, MDS and RGW nodes in a Ceph storage cluster.\n# [[inputs.ceph]]\n#   ## This is the recommended interval to poll.  Too frequent and you will lose\n#   ## data points due to timeouts during rebalancing and recovery\n#   interval = '1m'\n#\n#   ## All configuration values are optional, defaults are shown below\n#\n#   ## location of ceph binary\n#   ceph_binary = \"/usr/bin/ceph\"\n#\n#   ## directory in which to look for socket files\n#   socket_dir = \"/var/run/ceph\"\n#\n#   ## prefix of MON and OSD socket files, used to determine socket type\n#   mon_prefix = \"ceph-mon\"\n#   osd_prefix = \"ceph-osd\"\n#   mds_prefix = \"ceph-mds\"\n#   rgw_prefix = \"ceph-client\"\n#\n#   ## suffix used to identify socket files\n#   socket_suffix = \"asok\"\n#\n#   ## Ceph user to authenticate as, ceph will search for the corresponding keyring\n#   ## e.g. client.admin.keyring in /etc/ceph, or the explicit path defined in the\n#   ## client section of ceph.conf for example:\n#   ##\n#   ##     [client.telegraf]\n#   ##         keyring = /etc/ceph/client.telegraf.keyring\n#   ##\n#   ## Consult the ceph documentation for more detail on keyring generation.\n#   ceph_user = \"client.admin\"\n#\n#   ## Ceph configuration to use to locate the cluster\n#   ceph_config = \"/etc/ceph/ceph.conf\"\n#\n#   ## Whether to gather statistics via the admin socket\n#   gather_admin_socket_stats = true\n#\n#   ## Whether to gather statistics via ceph commands, requires ceph_user and ceph_config\n#   ## to be specified\n#   gather_cluster_stats = false\n\n\n# # Read specific statistics per cgroup\n# [[inputs.cgroup]]\n#   ## Directories in which to look for files, globs are supported.\n#   ## Consider restricting paths to the set of cgroups you really\n#   ## want to monitor if you have a large number of cgroups, to avoid\n#   ## any cardinality issues.\n#   # paths = [\n#   #   \"/sys/fs/cgroup/memory\",\n#   #   \"/sys/fs/cgroup/memory/child1\",\n#   #   \"/sys/fs/cgroup/memory/child2/*\",\n#   # ]\n#   ## cgroup stat fields, as file names, globs are supported.\n#   ## these file names are appended to each path from above.\n#   # files = [\"memory.*usage*\", \"memory.limit_in_bytes\"]\n\n\n# # Get standard chrony metrics, requires chronyc executable.\n# [[inputs.chrony]]\n#   ## If true, chronyc tries to perform a DNS lookup for the time server.\n#   # dns_lookup = false\n\n\n# # Pull Metric Statistics from Amazon CloudWatch\n# [[inputs.cloudwatch]]\n#   ## Amazon Region\n#   region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   # access_key = \"\"\n#   # secret_key = \"\"\n#   # token = \"\"\n#   # role_arn = \"\"\n#   # web_identity_token_file = \"\"\n#   # role_session_name = \"\"\n#   # profile = \"\"\n#   # shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n#   # http_proxy_url = \"http://localhost:8888\"\n#\n#   # The minimum period for Cloudwatch metrics is 1 minute (60s). However not all\n#   # metrics are made available to the 1 minute period. Some are collected at\n#   # 3 minute, 5 minute, or larger intervals. See https://aws.amazon.com/cloudwatch/faqs/#monitoring.\n#   # Note that if a period is configured that is smaller than the minimum for a\n#   # particular metric, that metric will not be returned by the Cloudwatch API\n#   # and will not be collected by Telegraf.\n#   #\n#   ## Requested CloudWatch aggregation Period (required - must be a multiple of 60s)\n#   period = \"5m\"\n#\n#   ## Collection Delay (required - must account for metrics availability via CloudWatch API)\n#   delay = \"5m\"\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n#   ## gaps or overlap in pulled data\n#   interval = \"5m\"\n#\n#   ## Recommended if \"delay\" and \"period\" are both within 3 hours of request time. Invalid values will be ignored.\n#   ## Recently Active feature will only poll for CloudWatch ListMetrics values that occurred within the last 3 Hours.\n#   ## If enabled, it will reduce total API usage of the CloudWatch ListMetrics API and require less memory to retain.\n#   ## Do not enable if \"period\" or \"delay\" is longer than 3 hours, as it will not return data more than 3 hours old.\n#   ## See https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_ListMetrics.html\n#   #recently_active = \"PT3H\"\n#\n#   ## Configure the TTL for the internal cache of metrics.\n#   # cache_ttl = \"1h\"\n#\n#   ## Metric Statistic Namespaces (required)\n#   namespaces = [\"AWS/ELB\"]\n#   # A single metric statistic namespace that will be appended to namespaces on startup\n#   # namespace = \"AWS/ELB\"\n#\n#   ## Maximum requests per second. Note that the global default AWS rate limit is\n#   ## 50 reqs/sec, so if you define multiple namespaces, these should add up to a\n#   ## maximum of 50.\n#   ## See http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_limits.html\n#   # ratelimit = 25\n#\n#   ## Timeout for http requests made by the cloudwatch client.\n#   # timeout = \"5s\"\n#\n#   ## Namespace-wide statistic filters. These allow fewer queries to be made to\n#   ## cloudwatch.\n#   # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n#   # statistic_exclude = []\n#\n#   ## Metrics to Pull\n#   ## Defaults to all Metrics in Namespace if nothing is provided\n#   ## Refreshes Namespace available metrics every 1h\n#   #[[inputs.cloudwatch.metrics]]\n#   #  names = [\"Latency\", \"RequestCount\"]\n#   #\n#   #  ## Statistic filters for Metric.  These allow for retrieving specific\n#   #  ## statistics for an individual metric.\n#   #  # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n#   #  # statistic_exclude = []\n#   #\n#   #  ## Dimension filters for Metric.  All dimensions defined for the metric names\n#   #  ## must be specified in order to retrieve the metric statistics.\n#   #  ## 'value' has wildcard / 'glob' matching support such as 'p-*'.\n#   #  [[inputs.cloudwatch.metrics.dimensions]]\n#   #    name = \"LoadBalancerName\"\n#   #    value = \"p-example\"\n\n\n# # Collects conntrack stats from the configured directories and files.\n# [[inputs.conntrack]]\n#    ## The following defaults would work with multiple versions of conntrack.\n#    ## Note the nf_ and ip_ filename prefixes are mutually exclusive across\n#    ## kernel versions, as are the directory locations.\n#\n#    ## Superset of filenames to look for within the conntrack dirs.\n#    ## Missing files will be ignored.\n#    files = [\"ip_conntrack_count\",\"ip_conntrack_max\",\n#             \"nf_conntrack_count\",\"nf_conntrack_max\"]\n#\n#    ## Directories to search within for the conntrack files above.\n#    ## Missing directories will be ignored.\n#    dirs = [\"/proc/sys/net/ipv4/netfilter\",\"/proc/sys/net/netfilter\"]\n\n\n# # Gather health check statuses from services registered in Consul\n# [[inputs.consul]]\n#   ## Consul server address\n#   # address = \"localhost:8500\"\n#\n#   ## URI scheme for the Consul server, one of \"http\", \"https\"\n#   # scheme = \"http\"\n#\n#   ## Metric version controls the mapping from Consul metrics into\n#   ## Telegraf metrics.\n#   ##\n#   ##   example: metric_version = 1; deprecated in 1.15\n#   ##            metric_version = 2; recommended version\n#   # metric_version = 1\n#\n#   ## ACL token used in every request\n#   # token = \"\"\n#\n#   ## HTTP Basic Authentication username and password.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Data center to query the health checks from\n#   # datacenter = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n#\n#   ## Consul checks' tag splitting\n#   # When tags are formatted like \"key:value\" with \":\" as a delimiter then\n#   # they will be splitted and reported as proper key:value in Telegraf\n#   # tag_delimiter = \":\"\n\n\n# # Read metrics from the Consul API\n# [[inputs.consul_metrics]]\n#   ## URL for the Consul agent\n#   # url = \"http://127.0.0.1:8500\"\n#\n#   ## Use auth token for authorization.\n#   ## Only one of the options can be set. Leave empty to not use any token.\n#   # token_file = \"/path/to/auth/token\"\n#   ## OR\n#   # token = \"a1234567-40c7-9048-7bae-378687048181\"\n#\n#   ## Set timeout (default 5 seconds)\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n\n\n# # Read per-node and per-bucket metrics from Couchbase\n# [[inputs.couchbase]]\n#   ## specify servers via a url matching:\n#   ##  [protocol://][:password]@address[:port]\n#   ##  e.g.\n#   ##    http://couchbase-0.example.com/\n#   ##    http://admin:secret@couchbase-0.example.com:8091/\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no protocol is specified, HTTP is used.\n#   ## If no port is specified, 8091 is used.\n#   servers = [\"http://localhost:8091\"]\n#\n#   ## Filter bucket fields to include only here.\n#   # bucket_stats_included = [\"quota_percent_used\", \"ops_per_sec\", \"disk_fetches\", \"item_count\", \"disk_used\", \"data_used\", \"mem_used\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification (defaults to false)\n#   ## If set to false, tls_cert and tls_key are required\n#   # insecure_skip_verify = false\n\n\n# # Read CouchDB Stats from one or more servers\n# [[inputs.couchdb]]\n#   ## Works with CouchDB stats endpoints out of the box\n#   ## Multiple Hosts from which to read CouchDB stats:\n#   hosts = [\"http://localhost:8086/_stats\"]\n#\n#   ## Use HTTP Basic Authentication.\n#   # basic_username = \"telegraf\"\n#   # basic_password = \"p@ssw0rd\"\n\n\n# # Fetch metrics from a CSGO SRCDS\n# [[inputs.csgo]]\n#   ## Specify servers using the following format:\n#   ##    servers = [\n#   ##      [\"ip1:port1\", \"rcon_password1\"],\n#   ##      [\"ip2:port2\", \"rcon_password2\"],\n#   ##    ]\n#   #\n#   ## If no servers are specified, no data will be collected\n#   servers = []\n\n\n# # Input plugin for DC/OS metrics\n# [[inputs.dcos]]\n#   ## The DC/OS cluster URL.\n#   cluster_url = \"https://dcos-ee-master-1\"\n#\n#   ## The ID of the service account.\n#   service_account_id = \"telegraf\"\n#   ## The private key file for the service account.\n#   service_account_private_key = \"/etc/telegraf/telegraf-sa-key.pem\"\n#\n#   ## Path containing login token.  If set, will read on every gather.\n#   # token_file = \"/home/dcos/.dcos/token\"\n#\n#   ## In all filter options if both include and exclude are empty all items\n#   ## will be collected.  Arrays may contain glob patterns.\n#   ##\n#   ## Node IDs to collect metrics from.  If a node is excluded, no metrics will\n#   ## be collected for its containers or apps.\n#   # node_include = []\n#   # node_exclude = []\n#   ## Container IDs to collect container metrics from.\n#   # container_include = []\n#   # container_exclude = []\n#   ## Container IDs to collect app metrics from.\n#   # app_include = []\n#   # app_exclude = []\n#\n#   ## Maximum concurrent connections to the cluster.\n#   # max_connections = 10\n#   ## Maximum time to receive a response from cluster.\n#   # response_timeout = \"20s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n#\n#   ## Recommended filtering to reduce series cardinality.\n#   # [inputs.dcos.tagdrop]\n#   #   path = [\"/var/lib/mesos/slave/slaves/*\"]\n\n\n# # Read metrics from one or many disque servers\n# [[inputs.disque]]\n#   ## An array of URI to gather stats about. Specify an ip or hostname\n#   ## with optional port and password.\n#   ## ie disque://localhost, disque://10.10.3.33:18832, 10.0.0.1:10000, etc.\n#   ## If no servers are specified, then localhost is used as the host.\n#   servers = [\"localhost\"]\n\n\n# # Provide a native collection for dmsetup based statistics for dm-cache\n# [[inputs.dmcache]]\n#   ## Whether to report per-device stats or not\n#   per_device = true\n\n\n# # Query given DNS server and gives statistics\n# [[inputs.dns_query]]\n#   ## servers to query\n#   servers = [\"8.8.8.8\"]\n#\n#   ## Network is the network protocol name.\n#   # network = \"udp\"\n#\n#   ## Domains or subdomains to query.\n#   # domains = [\".\"]\n#\n#   ## Query record type.\n#   ## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n#   # record_type = \"A\"\n#\n#   ## Dns server port.\n#   # port = 53\n#\n#   ## Query timeout in seconds.\n#   # timeout = 2\n\n\n# # Read metrics about docker containers\n [[inputs.docker]]\n#   ## Docker Endpoint\n#   ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n#   ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n   endpoint = \"unix:///var/run/docker.sock\"\n#\n#   ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)\n#   gather_services = false\n#\n#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n#   source_tag = false\n#\n#   ## Containers to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all containers\n#   container_name_include = []\n#   container_name_exclude = []\n#\n#   ## Container states to include and exclude. Globs accepted.\n#   ## When empty only containers in the \"running\" state will be captured.\n#   ## example: container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n#   ## example: container_state_exclude = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n   container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n#   # container_state_exclude = []\n#\n#   ## Timeout for docker list, info, and stats commands\n   timeout = \"10s\"\n#\n#   ## Specifies for which classes a per-device metric should be issued\n#   ## Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...)\n#   ## Please note that this setting has no effect if 'perdevice' is set to 'true'\n#   # perdevice_include = [\"cpu\"]\n#\n#   ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice' values.\n#   ## Possible values are 'cpu', 'blkio' and 'network'\n#   ## Total 'cpu' is reported directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin.\n#   ## Please note that this setting has no effect if 'total' is set to 'false'\n#   # total_include = [\"cpu\", \"blkio\", \"network\"]\n#\n#   ## Which environment variables should we use as a tag\n#   ##tag_env = [\"JAVA_HOME\", \"HEAP_SIZE\"]\n#\n#   ## docker labels to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all labels as tags\n#   docker_label_include = []\n#   docker_label_exclude = []\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read statistics from one or many dovecot servers\n# [[inputs.dovecot]]\n#   ## specify dovecot servers via an address:port list\n#   ##  e.g.\n#   ##    localhost:24242\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   servers = [\"localhost:24242\"]\n#\n#   ## Type is one of \"user\", \"domain\", \"ip\", or \"global\"\n#   type = \"global\"\n#\n#   ## Wildcard matches like \"*.com\". An empty string \"\" is same as \"*\"\n#   ## If type = \"ip\" filters should be \u003cIP/network\u003e\n#   filters = [\"\"]\n\n\n# # Reads metrics from DPDK applications using v2 telemetry interface.\n# [[inputs.dpdk]]\n#   ## Path to DPDK telemetry socket. This shall point to v2 version of DPDK telemetry interface.\n#   # socket_path = \"/var/run/dpdk/rte/dpdk_telemetry.v2\"\n#\n#   ## Duration that defines how long the connected socket client will wait for a response before terminating connection.\n#   ## This includes both writing to and reading from socket. Since it's local socket access\n#   ## to a fast packet processing application, the timeout should be sufficient for most users.\n#   ## Setting the value to 0 disables the timeout (not recommended)\n#   # socket_access_timeout = \"200ms\"\n#\n#   ## Enables telemetry data collection for selected device types.\n#   ## Adding \"ethdev\" enables collection of telemetry from DPDK NICs (stats, xstats, link_status).\n#   ## Adding \"rawdev\" enables collection of telemetry from DPDK Raw Devices (xstats).\n#   # device_types = [\"ethdev\"]\n#\n#   ## List of custom, application-specific telemetry commands to query\n#   ## The list of available commands depend on the application deployed. Applications can register their own commands\n#   ##   via telemetry library API http://doc.dpdk.org/guides/prog_guide/telemetry_lib.html#registering-commands\n#   ## For e.g. L3 Forwarding with Power Management Sample Application this could be:\n#   ##   additional_commands = [\"/l3fwd-power/stats\"]\n#   # additional_commands = []\n#\n#   ## Allows turning off collecting data for individual \"ethdev\" commands.\n#   ## Remove \"/ethdev/link_status\" from list to start getting link status metrics.\n#   [inputs.dpdk.ethdev]\n#     exclude_commands = [\"/ethdev/link_status\"]\n#\n#   ## When running multiple instances of the plugin it's recommended to add a unique tag to each instance to identify\n#   ## metrics exposed by an instance of DPDK application. This is useful when multiple DPDK apps run on a single host.\n#   ##  [inputs.dpdk.tags]\n#   ##    dpdk_instance = \"my-fwd-app\"\n\n\n# # Read metrics about docker containers from Fargate/ECS v2, v3 meta endpoints.\n# [[inputs.ecs]]\n#   ## ECS metadata url.\n#   ## Metadata v2 API is used if set explicitly. Otherwise,\n#   ## v3 metadata endpoint API is used if available.\n#   # endpoint_url = \"\"\n#\n#   ## Containers to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all containers\n#   # container_name_include = []\n#   # container_name_exclude = []\n#\n#   ## Container states to include and exclude. Globs accepted.\n#   ## When empty only containers in the \"RUNNING\" state will be captured.\n#   ## Possible values are \"NONE\", \"PULLED\", \"CREATED\", \"RUNNING\",\n#   ## \"RESOURCES_PROVISIONED\", \"STOPPED\".\n#   # container_status_include = []\n#   # container_status_exclude = []\n#\n#   ## ecs labels to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all labels as tags\n#   ecs_label_include = [ \"com.amazonaws.ecs.*\" ]\n#   ecs_label_exclude = []\n#\n#   ## Timeout for queries.\n#   # timeout = \"5s\"\n\n\n# # Read stats from one or more Elasticsearch servers or clusters\n# [[inputs.elasticsearch]]\n#   ## specify a list of one or more Elasticsearch servers\n#   # you can add username and password to your url to use basic authentication:\n#   # servers = [\"http://user:pass@localhost:9200\"]\n#   servers = [\"http://localhost:9200\"]\n#\n#   ## Timeout for HTTP requests to the elastic search server(s)\n#   http_timeout = \"5s\"\n#\n#   ## When local is true (the default), the node will read only its own stats.\n#   ## Set local to false when you want to read the node stats from all nodes\n#   ## of the cluster.\n#   local = true\n#\n#   ## Set cluster_health to true when you want to also obtain cluster health stats\n#   cluster_health = false\n#\n#   ## Adjust cluster_health_level when you want to also obtain detailed health stats\n#   ## The options are\n#   ##  - indices (default)\n#   ##  - cluster\n#   # cluster_health_level = \"indices\"\n#\n#   ## Set cluster_stats to true when you want to also obtain cluster stats.\n#   cluster_stats = false\n#\n#   ## Only gather cluster_stats from the master node. To work this require local = true\n#   cluster_stats_only_from_master = true\n#\n#   ## Indices to collect; can be one or more indices names or _all\n#   ## Use of wildcards is allowed. Use a wildcard at the end to retrieve index names that end with a changing value, like a date.\n#   indices_include = [\"_all\"]\n#\n#   ## One of \"shards\", \"cluster\", \"indices\"\n#   indices_level = \"shards\"\n#\n#   ## node_stats is a list of sub-stats that you want to have gathered. Valid options\n#   ## are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\", \"fs\", \"transport\", \"http\",\n#   ## \"breaker\". Per default, all stats are gathered.\n#   # node_stats = [\"jvm\", \"http\"]\n#\n#   ## HTTP Basic Authentication username and password.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Sets the number of most recent indices to return for indices that are configured with a date-stamped suffix.\n#   ## Each 'indices_include' entry ending with a wildcard (*) or glob matching pattern will group together all indices that match it, and sort them\n#   ## by the date or number after the wildcard. Metrics then are gathered for only the 'num_most_recent_indices' amount of most recent indices.\n#   # num_most_recent_indices = 0\n\n\n# # Derive metrics from aggregating Elasticsearch query results\n# [[inputs.elasticsearch_query]]\n#   ## The full HTTP endpoint URL for your Elasticsearch instance\n#   ## Multiple urls can be specified as part of the same cluster,\n#   ## this means that only ONE of the urls will be written to each interval.\n#   urls = [ \"http://node1.es.example.com:9200\" ] # required.\n#\n#   ## Elasticsearch client timeout, defaults to \"5s\".\n#   # timeout = \"5s\"\n#\n#   ## Set to true to ask Elasticsearch a list of all cluster nodes,\n#   ## thus it is not necessary to list all nodes in the urls config option\n#   # enable_sniffer = false\n#\n#   ## Set the interval to check if the Elasticsearch nodes are available\n#   ## This option is only used if enable_sniffer is also set (0s to disable it)\n#   # health_check_interval = \"10s\"\n#\n#   ## HTTP basic authentication details (eg. when using x-pack)\n#   # username = \"telegraf\"\n#   # password = \"mypassword\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   [[inputs.elasticsearch_query.aggregation]]\n#     ## measurement name for the results of the aggregation query\n#     measurement_name = \"measurement\"\n#\n#     ## Elasticsearch indexes to query (accept wildcards).\n#     index = \"index-*\"\n#\n#     ## The date/time field in the Elasticsearch index (mandatory).\n#     date_field = \"@timestamp\"\n#\n#     ## If the field used for the date/time field in Elasticsearch is also using\n#     ## a custom date/time format it may be required to provide the format to\n#     ## correctly parse the field.\n#     ##\n#     ## If using one of the built in elasticsearch formats this is not required.\n#     # date_field_custom_format = \"\"\n#\n#     ## Time window to query (eg. \"1m\" to query documents from last minute).\n#     ## Normally should be set to same as collection interval\n#     query_period = \"1m\"\n#\n#     ## Lucene query to filter results\n#     # filter_query = \"*\"\n#\n#     ## Fields to aggregate values (must be numeric fields)\n#     # metric_fields = [\"metric\"]\n#\n#     ## Aggregation function to use on the metric fields\n#     ## Must be set if 'metric_fields' is set\n#     ## Valid values are: avg, sum, min, max, sum\n#     # metric_function = \"avg\"\n#\n#     ## Fields to be used as tags\n#     ## Must be text, non-analyzed fields. Metric aggregations are performed per tag\n#     # tags = [\"field.keyword\", \"field2.keyword\"]\n#\n#     ## Set to true to not ignore documents when the tag(s) above are missing\n#     # include_missing_tag = false\n#\n#     ## String value of the tag when the tag does not exist\n#     ## Used when include_missing_tag is true\n#     # missing_tag_value = \"null\"\n\n\n# # Returns ethtool statistics for given interfaces\n# [[inputs.ethtool]]\n#   ## List of interfaces to pull metrics for\n#   # interface_include = [\"eth0\"]\n#\n#   ## List of interfaces to ignore when pulling metrics.\n#   # interface_exclude = [\"eth1\"]\n#\n#   ## Some drivers declare statistics with extra whitespace, different spacing,\n#   ## and mix cases. This list, when enabled, can be used to clean the keys.\n#   ## Here are the current possible normalizations:\n#   ##  * snakecase: converts fooBarBaz to foo_bar_baz\n#   ##  * trim: removes leading and trailing whitespace\n#   ##  * lower: changes all capitalized letters to lowercase\n#   ##  * underscore: replaces spaces with underscores\n#   # normalize_keys = [\"snakecase\", \"trim\", \"lower\", \"underscore\"]\n\n\n# # Read metrics from one or more commands that can output to stdout\n# [[inputs.exec]]\n#   ## Commands array\n#   commands = [\n#     \"/tmp/test.sh\",\n#     \"/usr/bin/mycollector --foo=bar\",\n#     \"/tmp/collect_*.sh\"\n#   ]\n#\n#   ## Timeout for each command to complete.\n#   timeout = \"5s\"\n#\n#   ## measurement name suffix (for separating different commands)\n#   name_suffix = \"_mycollector\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read metrics from fail2ban.\n# [[inputs.fail2ban]]\n#   ## Use sudo to run fail2ban-client\n#   use_sudo = false\n\n\n# # Read devices value(s) from a Fibaro controller\n# [[inputs.fibaro]]\n#   ## Required Fibaro controller address/hostname.\n#   ## Note: at the time of writing this plugin, Fibaro only implemented http - no https available\n#   url = \"http://\u003ccontroller\u003e:80\"\n#\n#   ## Required credentials to access the API (http://\u003ccontroller/api/\u003ccomponent\u003e)\n#   username = \"\u003cusername\u003e\"\n#   password = \"\u003cpassword\u003e\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n\n\n# # Parse a complete file each interval\n# [[inputs.file]]\n#   ## Files to parse each interval.  Accept standard unix glob matching rules,\n#   ## as well as ** to match recursive files and directories.\n#   files = [\"/tmp/metrics.out\"]\n#\n#\n#   ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n#   ## to disable. Cautious when file name variation is high, this can increase the cardinality\n#   ## significantly. Read more about cardinality here:\n#   ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n#   # file_tag = \"\"\n#   #\n#\n#   ## Character encoding to use when interpreting the file contents.  Invalid\n#   ## characters are replaced using the unicode replacement character.  When set\n#   ## to the empty string the data is not decoded to text.\n#   ##   ex: character_encoding = \"utf-8\"\n#   ##       character_encoding = \"utf-16le\"\n#   ##       character_encoding = \"utf-16be\"\n#   ##       character_encoding = \"\"\n#   # character_encoding = \"\"\n#\n#   ## The dataformat to be read from files\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Count files in a directory\n# [[inputs.filecount]]\n#   ## Directories to gather stats about.\n#   ## This accept standard unit glob matching rules, but with the addition of\n#   ## ** as a \"super asterisk\". ie:\n#   ##   /var/log/**    -\u003e recursively find all directories in /var/log and count files in each directories\n#   ##   /var/log/*/*   -\u003e find all directories with a parent dir in /var/log and count files in each directories\n#   ##   /var/log       -\u003e count all files in /var/log and all of its subdirectories\n#   directories = [\"/var/cache/apt/archives\"]\n#\n#   ## Only count files that match the name pattern. Defaults to \"*\".\n#   name = \"*.deb\"\n#\n#   ## Count files in subdirectories. Defaults to true.\n#   recursive = false\n#\n#   ## Only count regular files. Defaults to true.\n#   regular_only = true\n#\n#   ## Follow all symlinks while walking the directory tree. Defaults to false.\n#   follow_symlinks = false\n#\n#   ## Only count files that are at least this size. If size is\n#   ## a negative number, only count files that are smaller than the\n#   ## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n#   ## Without quotes and units, interpreted as size in bytes.\n#   size = \"0B\"\n#\n#   ## Only count files that have not been touched for at least this\n#   ## duration. If mtime is negative, only count files that have been\n#   ## touched in this duration. Defaults to \"0s\".\n#   mtime = \"0s\"\n\n\n# # Read stats about given file(s)\n# [[inputs.filestat]]\n#   ## Files to gather stats about.\n#   ## These accept standard unix glob matching rules, but with the addition of\n#   ## ** as a \"super asterisk\". ie:\n#   ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n#   ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n#   ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n#   ##\n#   ## See https://github.com/gobwas/glob for more examples\n#   ##\n#   files = [\"/var/log/**.log\"]\n#\n#   ## If true, read the entire file and calculate an md5 checksum.\n#   md5 = false\n\n\n# # Read real time temps from fireboard.io servers\n# [[inputs.fireboard]]\n#   ## Specify auth token for your account\n#   auth_token = \"invalidAuthToken\"\n#   ## You can override the fireboard server URL if necessary\n#   # url = https://fireboard.io/api/v1/devices.json\n#   ## You can set a different http_timeout if you need to\n#   ## You should set a string using an number and time indicator\n#   ## for example \"12s\" for 12 seconds.\n#   # http_timeout = \"4s\"\n\n\n# # Read metrics exposed by fluentd in_monitor plugin\n# [[inputs.fluentd]]\n#   ## This plugin reads information exposed by fluentd (using /api/plugins.json endpoint).\n#   ##\n#   ## Endpoint:\n#   ## - only one URI is allowed\n#   ## - https is not supported\n#   endpoint = \"http://localhost:24220/api/plugins.json\"\n#\n#   ## Define which plugins have to be excluded (based on \"type\" field - e.g. monitor_agent)\n#   exclude = [\n# \t  \"monitor_agent\",\n# \t  \"dummy\",\n#   ]\n\n\n# # Gather repository information from GitHub hosted repositories.\n# [[inputs.github]]\n#   ## List of repositories to monitor.\n#   repositories = [\n# \t  \"influxdata/telegraf\",\n# \t  \"influxdata/influxdb\"\n#   ]\n#\n#   ## Github API access token.  Unauthenticated requests are limited to 60 per hour.\n#   # access_token = \"\"\n#\n#   ## Github API enterprise url. Github Enterprise accounts must specify their base url.\n#   # enterprise_base_url = \"\"\n#\n#   ## Timeout for HTTP requests.\n#   # http_timeout = \"5s\"\n#\n#   ## List of additional fields to query.\n# \t## NOTE: Getting those fields might involve issuing additional API-calls, so please\n# \t##       make sure you do not exceed the rate-limit of GitHub.\n# \t##\n# \t## Available fields are:\n# \t## \t- pull-requests\t\t\t-- number of open and closed pull requests (2 API-calls per repository)\n#   # additional_fields = []\n\n\n# # Read flattened metrics from one or more GrayLog HTTP endpoints\n# [[inputs.graylog]]\n#   ## API endpoint, currently supported API:\n#   ##\n#   ##   - multiple  (e.g. http://\u003chost\u003e:9000/api/system/metrics/multiple)\n#   ##   - namespace (e.g. http://\u003chost\u003e:9000/api/system/metrics/namespace/{namespace})\n#   ##\n#   ## For namespace endpoint, the metrics array will be ignored for that call.\n#   ## Endpoint can contain namespace and multiple type calls.\n#   ##\n#   ## Please check http://[graylog-server-ip]:9000/api/api-browser for full list\n#   ## of endpoints\n#   servers = [\n#     \"http://[graylog-server-ip]:9000/api/system/metrics/multiple\",\n#   ]\n#\n#   ## Set timeout (default 5 seconds)\n#   # timeout = \"5s\"\n#\n#   ## Metrics list\n#   ## List of metrics can be found on Graylog webservice documentation.\n#   ## Or by hitting the web service api at:\n#   ##   http://[graylog-host]:9000/api/system/metrics\n#   metrics = [\n#     \"jvm.cl.loaded\",\n#     \"jvm.memory.pools.Metaspace.committed\"\n#   ]\n#\n#   ## Username and password\n#   username = \"\"\n#   password = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics of haproxy, via socket or csv stats page\n# [[inputs.haproxy]]\n#   ## An array of address to gather stats about. Specify an ip on hostname\n#   ## with optional port. ie localhost, 10.10.3.33:1936, etc.\n#   ## Make sure you specify the complete path to the stats endpoint\n#   ## including the protocol, ie http://10.10.3.33:1936/haproxy?stats\n#\n#   ## If no servers are specified, then default to 127.0.0.1:1936/haproxy?stats\n#   servers = [\"http://myhaproxy.com:1936/haproxy?stats\"]\n#\n#   ## Credentials for basic HTTP authentication\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## You can also use local socket with standard wildcard globbing.\n#   ## Server address not starting with 'http' will be treated as a possible\n#   ## socket, so both examples below are valid.\n#   # servers = [\"socket:/run/haproxy/admin.sock\", \"/run/haproxy/*.sock\"]\n#\n#   ## By default, some of the fields are renamed from what haproxy calls them.\n#   ## Setting this option to true results in the plugin keeping the original\n#   ## field names.\n#   # keep_field_names = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Monitor disks' temperatures using hddtemp\n# [[inputs.hddtemp]]\n#   ## By default, telegraf gathers temps data from all disks detected by the\n#   ## hddtemp.\n#   ##\n#   ## Only collect temps from the selected disks.\n#   ##\n#   ## A * as the device name will return the temperature values of all disks.\n#   ##\n#   # address = \"127.0.0.1:7634\"\n#   # devices = [\"sda\", \"*\"]\n\n\n# # Read formatted metrics from one or more HTTP endpoints\n# [[inputs.http]]\n#   ## One or more URLs from which to read formatted metrics\n#   urls = [\n#     \"http://localhost/metrics\"\n#   ]\n#\n#   ## HTTP method\n#   # method = \"GET\"\n#\n#   ## Optional HTTP headers\n#   # headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## Optional file with Bearer token\n#   ## file content is added as an Authorization header\n#   # bearer_token = \"/path/to/file\"\n#\n#   ## Optional HTTP Basic Auth Credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## HTTP entity-body to send with POST/PUT requests.\n#   # body = \"\"\n#\n#   ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## HTTP Proxy support\n#   # http_proxy_url = \"\"\n#\n#   ## OAuth2 Client Credentials Grant\n#   # client_id = \"clientid\"\n#   # client_secret = \"secret\"\n#   # token_url = \"https://indentityprovider/oauth2/v1/token\"\n#   # scopes = [\"urn:opc:idm:__myscopes__\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional Cookie authentication\n#   # cookie_auth_url = \"https://localhost/authMe\"\n#   # cookie_auth_method = \"POST\"\n#   # cookie_auth_username = \"username\"\n#   # cookie_auth_password = \"pa$$word\"\n#   # cookie_auth_headers = '{\"Content-Type\": \"application/json\", \"X-MY-HEADER\":\"hello\"}'\n#   # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n#   ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n#   # cookie_auth_renewal = \"5m\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n#\n#   ## List of success status codes\n#   # success_status_codes = [200]\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   # data_format = \"influx\"\n\n\n# # HTTP/HTTPS request given an address a method and a timeout\n# [[inputs.http_response]]\n#   ## List of urls to query.\n#   # urls = [\"http://localhost\"]\n#\n#   ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n#   # http_proxy = \"http://localhost:8888\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## HTTP Request Method\n#   # method = \"GET\"\n#\n#   ## Whether to follow redirects from the server (defaults to false)\n#   # follow_redirects = false\n#\n#   ## Optional file with Bearer token\n#   ## file content is added as an Authorization header\n#   # bearer_token = \"/path/to/file\"\n#\n#   ## Optional HTTP Basic Auth Credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional HTTP Request Body\n#   # body = '''\n#   # {'fake':'data'}\n#   # '''\n#\n#   ## Optional name of the field that will contain the body of the response.\n#   ## By default it is set to an empty String indicating that the body's content won't be added\n#   # response_body_field = ''\n#\n#   ## Maximum allowed HTTP response body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   ## If the response body size exceeds this limit a \"body_read_error\" will be raised\n#   # response_body_max_size = \"32MiB\"\n#\n#   ## Optional substring or regex match in body of the response (case sensitive)\n#   # response_string_match = \"\\\"service_status\\\": \\\"up\\\"\"\n#   # response_string_match = \"ok\"\n#   # response_string_match = \"\\\".*_status\\\".?:.?\\\"up\\\"\"\n#\n#   ## Expected response status code.\n#   ## The status code of the response is compared to this value. If they match, the field\n#   ## \"response_status_code_match\" will be 1, otherwise it will be 0. If the\n#   ## expected status code is 0, the check is disabled and the field won't be added.\n#   # response_status_code = 0\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## HTTP Request Headers (all values must be strings)\n#   # [inputs.http_response.headers]\n#   #   Host = \"github.com\"\n#\n#   ## Optional setting to map response http headers into tags\n#   ## If the http header is not present on the request, no corresponding tag will be added\n#   ## If multiple instances of the http header are present, only the first value will be used\n#   # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n#\n#   ## Interface to use when dialing an address\n#   # interface = \"eth0\"\n\n\n# # Read flattened metrics from one or more JSON HTTP endpoints\n# [[inputs.httpjson]]\n#   ## DEPRECATED: The 'httpjson' plugin is deprecated in version 1.6.0, use 'inputs.http' instead.\n#   ## NOTE This plugin only reads numerical measurements, strings and booleans\n#   ## will be ignored.\n#\n#   ## URL of each server in the service's cluster\n#   servers = [\n#     \"http://localhost:9999/stats/\",\n#     \"http://localhost:9998/stats/\",\n#   ]\n#   ## Set response_timeout (default 5 seconds)\n#   response_timeout = \"5s\"\n#\n#   ## HTTP method to use: GET or POST (case-sensitive)\n#   method = \"GET\"\n#\n#   ## List of tag names to extract from top-level of JSON server response\n#   # tag_keys = [\n#   #   \"my_tag_1\",\n#   #   \"my_tag_2\"\n#   # ]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## HTTP parameters (all values must be strings).  For \"GET\" requests, data\n#   ## will be included in the query.  For \"POST\" requests, data will be included\n#   ## in the request body as \"x-www-form-urlencoded\".\n#   # [inputs.httpjson.parameters]\n#   #   event_type = \"cpu_spike\"\n#   #   threshold = \"0.75\"\n#\n#   ## HTTP Headers (all values must be strings)\n#   # [inputs.httpjson.headers]\n#   #   X-Auth-Token = \"my-xauth-token\"\n#   #   apiVersion = \"v1\"\n\n\n# # Gather Icinga2 status\n# [[inputs.icinga2]]\n#   ## Required Icinga2 server address\n#   # server = \"https://localhost:5665\"\n#\n#   ## Required Icinga2 object type (\"services\" or \"hosts\")\n#   # object_type = \"services\"\n#\n#   ## Credentials for basic HTTP authentication\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Gets counters from all InfiniBand cards and ports installed\n# [[inputs.infiniband]]\n#   # no configuration\n\n\n# # Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints\n# [[inputs.influxdb]]\n#   ## Works with InfluxDB debug endpoints out of the box,\n#   ## but other services can use this format too.\n#   ## See the influxdb plugin's README for more details.\n#\n#   ## Multiple URLs from which to read InfluxDB-formatted JSON\n#   ## Default is \"http://localhost:8086/debug/vars\".\n#   urls = [\n#     \"http://localhost:8086/debug/vars\"\n#   ]\n#\n#   ## Username and password to send using HTTP Basic Authentication.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## http request \u0026 header timeout\n#   timeout = \"5s\"\n\n\n# # Intel PowerStat plugin enables monitoring of platform metrics (power, TDP) and Core metrics like temperature, power and utilization.\n# [[inputs.intel_powerstat]]\n#   ## All global metrics are always collected by Intel PowerStat plugin.\n#   ## User can choose which per-CPU metrics are monitored by the plugin in cpu_metrics array.\n#   ## Empty array means no per-CPU specific metrics will be collected by the plugin - in this case only platform level\n#   ## telemetry will be exposed by Intel PowerStat plugin.\n#   ## Supported options:\n#   ## \"cpu_frequency\", \"cpu_busy_frequency\", \"cpu_temperature\", \"cpu_c1_state_residency\", \"cpu_c6_state_residency\", \"cpu_busy_cycles\"\n#   # cpu_metrics = []\n\n\n# # Collect statistics about itself\n# [[inputs.internal]]\n#   ## If true, collect telegraf memory stats.\n#   # collect_memstats = true\n\n\n# # Monitors internet speed using speedtest.net service\n# [[inputs.internet_speed]]\n#   ## Sets if runs file download test\n#   # enable_file_download = false\n#\n#   ## Caches the closest server location\n#   # cache = false\n\n\n# # This plugin gathers interrupts data from /proc/interrupts and /proc/softirqs.\n# [[inputs.interrupts]]\n#   ## When set to true, cpu metrics are tagged with the cpu.  Otherwise cpu is\n#   ## stored as a field.\n#   ##\n#   ## The default is false for backwards compatibility, and will be changed to\n#   ## true in a future version.  It is recommended to set to true on new\n#   ## deployments.\n#   # cpu_as_tag = false\n#\n#   ## To filter which IRQs to collect, make use of tagpass / tagdrop, i.e.\n#   # [inputs.interrupts.tagdrop]\n#   #   irq = [ \"NET_RX\", \"TASKLET\" ]\n\n\n# # Read metrics from the bare metal servers via IPMI\n# [[inputs.ipmi_sensor]]\n#   ## optionally specify the path to the ipmitool executable\n#   # path = \"/usr/bin/ipmitool\"\n#   ##\n#   ## Setting 'use_sudo' to true will make use of sudo to run ipmitool.\n#   ## Sudo must be configured to allow the telegraf user to run ipmitool\n#   ## without a password.\n#   # use_sudo = false\n#   ##\n#   ## optionally force session privilege level. Can be CALLBACK, USER, OPERATOR, ADMINISTRATOR\n#   # privilege = \"ADMINISTRATOR\"\n#   ##\n#   ## optionally specify one or more servers via a url matching\n#   ##  [username[:password]@][protocol[(address)]]\n#   ##  e.g.\n#   ##    root:passwd@lan(127.0.0.1)\n#   ##\n#   ## if no servers are specified, local machine sensor stats will be queried\n#   ##\n#   # servers = [\"USERID:PASSW0RD@lan(192.168.1.1)\"]\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'timeout' to avoid\n#   ## gaps or overlap in pulled data\n#   interval = \"30s\"\n#\n#   ## Timeout for the ipmitool command to complete\n#   timeout = \"20s\"\n#\n#   ## Schema Version: (Optional, defaults to version 1)\n#   metric_version = 2\n#\n#   ## Optionally provide the hex key for the IMPI connection.\n#   # hex_key = \"\"\n#\n#   ## If ipmitool should use a cache\n#   ## for me ipmitool runs about 2 to 10 times faster with cache enabled on HP G10 servers (when using ubuntu20.04)\n#   ## the cache file may not work well for you if some sensors come up late\n#   # use_cache = false\n#\n#   ## Path to the ipmitools cache file (defaults to OS temp dir)\n#   ## The provided path must exist and must be writable\n#   # cache_path = \"\"\n\n\n# # Gather packets and bytes counters from Linux ipsets\n# [[inputs.ipset]]\n#   ## By default, we only show sets which have already matched at least 1 packet.\n#   ## set include_unmatched_sets = true to gather them all.\n#   include_unmatched_sets = false\n#   ## Adjust your sudo settings appropriately if using this option (\"sudo ipset save\")\n#   use_sudo = false\n#   ## The default timeout of 1s for ipset execution can be overridden here:\n#   # timeout = \"1s\"\n\n\n# # Gather packets and bytes throughput from iptables\n# [[inputs.iptables]]\n#   ## iptables require root access on most systems.\n#   ## Setting 'use_sudo' to true will make use of sudo to run iptables.\n#   ## Users must configure sudo to allow telegraf user to run iptables with no password.\n#   ## iptables can be restricted to only list command \"iptables -nvL\".\n#   use_sudo = false\n#   ## Setting 'use_lock' to true runs iptables with the \"-w\" option.\n#   ## Adjust your sudo settings appropriately if using this option (\"iptables -w 5 -nvl\")\n#   use_lock = false\n#   ## Define an alternate executable, such as \"ip6tables\". Default is \"iptables\".\n#   # binary = \"ip6tables\"\n#   ## defines the table to monitor:\n#   table = \"filter\"\n#   ## defines the chains to monitor.\n#   ## NOTE: iptables rules without a comment will not be monitored.\n#   ## Read the plugin documentation for more information.\n#   chains = [ \"INPUT\" ]\n\n\n# # Collect virtual and real server stats from Linux IPVS\n# [[inputs.ipvs]]\n#   # no configuration\n\n\n# # Read jobs and cluster metrics from Jenkins instances\n# [[inputs.jenkins]]\n#   ## The Jenkins URL in the format \"schema://host:port\"\n#   url = \"http://my-jenkins-instance:8080\"\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## Set response_timeout\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use SSL but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional Max Job Build Age filter\n#   ## Default 1 hour, ignore builds older than max_build_age\n#   # max_build_age = \"1h\"\n#\n#   ## Optional Sub Job Depth filter\n#   ## Jenkins can have unlimited layer of sub jobs\n#   ## This config will limit the layers of pulling, default value 0 means\n#   ## unlimited pulling until no more sub jobs\n#   # max_subjob_depth = 0\n#\n#   ## Optional Sub Job Per Layer\n#   ## In workflow-multibranch-plugin, each branch will be created as a sub job.\n#   ## This config will limit to call only the lasted branches in each layer,\n#   ## empty will use default value 10\n#   # max_subjob_per_layer = 10\n#\n#   ## Jobs to include or exclude from gathering\n#   ## When using both lists, job_exclude has priority.\n#   ## Wildcards are supported: [ \"jobA/*\", \"jobB/subjob1/*\"]\n#   # job_include = [ \"*\" ]\n#   # job_exclude = [ ]\n#\n#   ## Nodes to include or exclude from gathering\n#   ## When using both lists, node_exclude has priority.\n#   # node_include = [ \"*\" ]\n#   # node_exclude = [ ]\n#\n#   ## Worker pool for jenkins plugin only\n#   ## Empty this field will use default value 5\n#   # max_connections = 5\n\n\n# # Read JMX metrics through Jolokia\n# [[inputs.jolokia]]\n#   ## DEPRECATED: The 'jolokia' plugin is deprecated in version 1.5.0, use 'inputs.jolokia2' instead.\n#   # DEPRECATED: the jolokia plugin has been deprecated in favor of the\n#   # jolokia2 plugin\n#   # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n#\n#   ## This is the context root used to compose the jolokia url\n#   ## NOTE that Jolokia requires a trailing slash at the end of the context root\n#   ## NOTE that your jolokia security policy must allow for POST requests.\n#   context = \"/jolokia/\"\n#\n#   ## This specifies the mode used\n#   # mode = \"proxy\"\n#   #\n#   ## When in proxy mode this section is used to specify further\n#   ## proxy address configurations.\n#   ## Remember to change host address to fit your environment.\n#   # [inputs.jolokia.proxy]\n#   #   host = \"127.0.0.1\"\n#   #   port = \"8080\"\n#\n#   ## Optional http timeouts\n#   ##\n#   ## response_header_timeout, if non-zero, specifies the amount of time to wait\n#   ## for a server's response headers after fully writing the request.\n#   # response_header_timeout = \"3s\"\n#   ##\n#   ## client_timeout specifies a time limit for requests made by this client.\n#   ## Includes connection time, any redirects, and reading the response body.\n#   # client_timeout = \"4s\"\n#\n#   ## Attribute delimiter\n#   ##\n#   ## When multiple attributes are returned for a single\n#   ## [inputs.jolokia.metrics], the field name is a concatenation of the metric\n#   ## name, and the attribute name, separated by the given delimiter.\n#   # delimiter = \"_\"\n#\n#   ## List of servers exposing jolokia read service\n#   [[inputs.jolokia.servers]]\n#     name = \"as-server-01\"\n#     host = \"127.0.0.1\"\n#     port = \"8080\"\n#     # username = \"myuser\"\n#     # password = \"mypassword\"\n#\n#   ## List of metrics collected on above servers\n#   ## Each metric consists in a name, a jmx path and either\n#   ## a pass or drop slice attribute.\n#   ## This collect all heap memory usage metrics.\n#   [[inputs.jolokia.metrics]]\n#     name = \"heap_memory_usage\"\n#     mbean  = \"java.lang:type=Memory\"\n#     attribute = \"HeapMemoryUsage\"\n#\n#   ## This collect thread counts metrics.\n#   [[inputs.jolokia.metrics]]\n#     name = \"thread_count\"\n#     mbean  = \"java.lang:type=Threading\"\n#     attribute = \"TotalStartedThreadCount,ThreadCount,DaemonThreadCount,PeakThreadCount\"\n#\n#   ## This collect number of class loaded/unloaded counts metrics.\n#   [[inputs.jolokia.metrics]]\n#     name = \"class_count\"\n#     mbean  = \"java.lang:type=ClassLoading\"\n#     attribute = \"LoadedClassCount,UnloadedClassCount,TotalLoadedClassCount\"\n\n\n# # Read JMX metrics from a Jolokia REST agent endpoint\n# [[inputs.jolokia2_agent]]\n#   # default_tag_prefix      = \"\"\n#   # default_field_prefix    = \"\"\n#   # default_field_separator = \".\"\n#\n#   # Add agents URLs to query\n#   urls = [\"http://localhost:8080/jolokia\"]\n#   # username = \"\"\n#   # password = \"\"\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS config\n#   # tls_ca   = \"/var/private/ca.pem\"\n#   # tls_cert = \"/var/private/client.pem\"\n#   # tls_key  = \"/var/private/client-key.pem\"\n#   # insecure_skip_verify = false\n#\n#   ## Add metrics to read\n#   [[inputs.jolokia2_agent.metric]]\n#     name  = \"java_runtime\"\n#     mbean = \"java.lang:type=Runtime\"\n#     paths = [\"Uptime\"]\n\n\n# # Read JMX metrics from a Jolokia REST proxy endpoint\n# [[inputs.jolokia2_proxy]]\n#   # default_tag_prefix      = \"\"\n#   # default_field_prefix    = \"\"\n#   # default_field_separator = \".\"\n#\n#   ## Proxy agent\n#   url = \"http://localhost:8080/jolokia\"\n#   # username = \"\"\n#   # password = \"\"\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS config\n#   # tls_ca   = \"/var/private/ca.pem\"\n#   # tls_cert = \"/var/private/client.pem\"\n#   # tls_key  = \"/var/private/client-key.pem\"\n#   # insecure_skip_verify = false\n#\n#   ## Add proxy targets to query\n#   # default_target_username = \"\"\n#   # default_target_password = \"\"\n#   [[inputs.jolokia2_proxy.target]]\n#     url = \"service:jmx:rmi:///jndi/rmi://targethost:9999/jmxrmi\"\n#     # username = \"\"\n#     # password = \"\"\n#\n#   ## Add metrics to read\n#   [[inputs.jolokia2_proxy.metric]]\n#     name  = \"java_runtime\"\n#     mbean = \"java.lang:type=Runtime\"\n#     paths = [\"Uptime\"]\n\n\n# # Read Kapacitor-formatted JSON metrics from one or more HTTP endpoints\n# [[inputs.kapacitor]]\n#   ## Multiple URLs from which to read Kapacitor-formatted JSON\n#   ## Default is \"http://localhost:9092/kapacitor/v1/debug/vars\".\n#   urls = [\n#     \"http://localhost:9092/kapacitor/v1/debug/vars\"\n#   ]\n#\n#   ## Time limit for http requests\n#   timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Get kernel statistics from /proc/vmstat\n# [[inputs.kernel_vmstat]]\n#   # no configuration\n\n\n# # Read status information from one or more Kibana servers\n# [[inputs.kibana]]\n#   ## Specify a list of one or more Kibana servers\n#   servers = [\"http://localhost:5601\"]\n#\n#   ## Timeout for HTTP requests\n#   timeout = \"5s\"\n#\n#   ## HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from the Kubernetes api\n# [[inputs.kube_inventory]]\n#   ## URL for the Kubernetes API\n#   url = \"https://127.0.0.1\"\n#\n#   ## Namespace to use. Set to \"\" to use all namespaces.\n#   # namespace = \"default\"\n#\n#   ## Use bearer token for authorization. ('bearer_token' takes priority)\n#   ## If both of these are empty, we'll use the default serviceaccount:\n#   ## at: /run/secrets/kubernetes.io/serviceaccount/token\n#   # bearer_token = \"/path/to/bearer/token\"\n#   ## OR\n#   # bearer_token_string = \"abc_123\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional Resources to exclude from gathering\n#   ## Leave them with blank with try to gather everything available.\n#   ## Values can be - \"daemonsets\", deployments\", \"endpoints\", \"ingress\", \"nodes\",\n#   ## \"persistentvolumes\", \"persistentvolumeclaims\", \"pods\", \"services\", \"statefulsets\"\n#   # resource_exclude = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n#\n#   ## Optional Resources to include when gathering\n#   ## Overrides resource_exclude if both set.\n#   # resource_include = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n#\n#   ## selectors to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all selectors as tags\n#   ## selector_exclude overrides selector_include if both set.\n#   # selector_include = []\n#   # selector_exclude = [\"*\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/path/to/cafile\"\n#   # tls_cert = \"/path/to/certfile\"\n#   # tls_key = \"/path/to/keyfile\"\n#   # tls_server_name = \"kubernetes.example.com\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from the kubernetes kubelet api\n# [[inputs.kubernetes]]\n#   ## URL for the kubelet\n#   url = \"http://127.0.0.1:10255\"\n#\n#   ## Use bearer token for authorization. ('bearer_token' takes priority)\n#   ## If both of these are empty, we'll use the default serviceaccount:\n#   ## at: /run/secrets/kubernetes.io/serviceaccount/token\n#   # bearer_token = \"/path/to/bearer/token\"\n#   ## OR\n#   # bearer_token_string = \"abc_123\"\n#\n#   ## Pod labels to be added as tags.  An empty array for both include and\n#   ## exclude will include all labels.\n#   # label_include = []\n#   # label_exclude = [\"*\"]\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from a LeoFS Server via SNMP\n# [[inputs.leofs]]\n#   ## An array of URLs of the form:\n#   ##   host [ \":\" port]\n#   servers = [\"127.0.0.1:4020\"]\n\n\n# # Provides Linux sysctl fs metrics\n# [[inputs.linux_sysctl_fs]]\n#   # no configuration\n\n\n# # Read metrics exposed by Logstash\n# [[inputs.logstash]]\n#   ## The URL of the exposed Logstash API endpoint.\n#   url = \"http://127.0.0.1:9600\"\n#\n#   ## Use Logstash 5 single pipeline API, set to true when monitoring\n#   ## Logstash 5.\n#   # single_pipeline = false\n#\n#   ## Enable optional collection components.  Can contain\n#   ## \"pipelines\", \"process\", and \"jvm\".\n#   # collect = [\"pipelines\", \"process\", \"jvm\"]\n#\n#   ## Timeout for HTTP requests.\n#   # timeout = \"5s\"\n#\n#   ## Optional HTTP Basic Auth credentials.\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Use TLS but skip chain \u0026 host verification.\n#   # insecure_skip_verify = false\n#\n#   ## Optional HTTP headers.\n#   # [inputs.logstash.headers]\n#   #   \"X-Special-Header\" = \"Special-Value\"\n\n\n# # Read metrics from local Lustre service on OST, MDS\n# [[inputs.lustre2]]\n#   ## An array of /proc globs to search for Lustre stats\n#   ## If not specified, the default will work on Lustre 2.5.x\n#   ##\n#   # ost_procfiles = [\n#   #   \"/proc/fs/lustre/obdfilter/*/stats\",\n#   #   \"/proc/fs/lustre/osd-ldiskfs/*/stats\",\n#   #   \"/proc/fs/lustre/obdfilter/*/job_stats\",\n#   # ]\n#   # mds_procfiles = [\n#   #   \"/proc/fs/lustre/mdt/*/md_stats\",\n#   #   \"/proc/fs/lustre/mdt/*/job_stats\",\n#   # ]\n\n\n# # Read metrics about LVM physical volumes, volume groups, logical volumes.\n# [[inputs.lvm]]\n# ## Use sudo to run LVM commands\n# use_sudo = false\n\n\n# # Gathers metrics from the /3.0/reports MailChimp API\n# [[inputs.mailchimp]]\n#   ## MailChimp API key\n#   ## get from https://admin.mailchimp.com/account/api/\n#   api_key = \"\" # required\n#   ## Reports for campaigns sent more than days_old ago will not be collected.\n#   ## 0 means collect all.\n#   days_old = 0\n#   ## Campaign ID to get, if empty gets all campaigns, this option overrides days_old\n#   # campaign_id = \"\"\n\n\n# # Retrieves information on a specific host in a MarkLogic Cluster\n# [[inputs.marklogic]]\n#   ## Base URL of the MarkLogic HTTP Server.\n#   url = \"http://localhost:8002\"\n#\n#   ## List of specific hostnames to retrieve information. At least (1) required.\n#   # hosts = [\"hostname1\", \"hostname2\"]\n#\n#   ## Using HTTP Basic Authentication. Management API requires 'manage-user' role privileges\n#   # username = \"myuser\"\n#   # password = \"mypassword\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from one or many mcrouter servers\n# [[inputs.mcrouter]]\n#   ## An array of address to gather stats about. Specify an ip or hostname\n#   ## with port. ie tcp://localhost:11211, tcp://10.0.0.1:11211, etc.\n# \tservers = [\"tcp://localhost:11211\", \"unix:///var/run/mcrouter.sock\"]\n#\n# \t## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n#   # timeout = \"5s\"\n\n\n# # Get md array statistics from /proc/mdstat\n# [[inputs.mdstat]]\n# \t## Sets file path\n# \t## If not specified, then default is /proc/mdstat\n# \t# file_name = \"/proc/mdstat\"\n\n\n# # Read metrics from one or many memcached servers\n# [[inputs.memcached]]\n#   ## An array of address to gather stats about. Specify an ip on hostname\n#   ## with optional port. ie localhost, 10.0.0.1:11211, etc.\n#   servers = [\"localhost:11211\"]\n#   # unix_sockets = [\"/var/run/memcached.sock\"]\n#\n#   ## Optional TLS Config\n#   # enable_tls = true\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Telegraf plugin for gathering metrics from N Mesos masters\n# [[inputs.mesos]]\n#   ## Timeout, in ms.\n#   timeout = 100\n#\n#   ## A list of Mesos masters.\n#   masters = [\"http://localhost:5050\"]\n#\n#   ## Master metrics groups to be collected, by default, all enabled.\n#   master_collections = [\n#     \"resources\",\n#     \"master\",\n#     \"system\",\n#     \"agents\",\n#     \"frameworks\",\n#     \"framework_offers\",\n#     \"tasks\",\n#     \"messages\",\n#     \"evqueue\",\n#     \"registrar\",\n#     \"allocator\",\n#   ]\n#\n#   ## A list of Mesos slaves, default is []\n#   # slaves = []\n#\n#   ## Slave metrics groups to be collected, by default, all enabled.\n#   # slave_collections = [\n#   #   \"resources\",\n#   #   \"agent\",\n#   #   \"system\",\n#   #   \"executors\",\n#   #   \"tasks\",\n#   #   \"messages\",\n#   # ]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Collects scores from a Minecraft server's scoreboard using the RCON protocol\n# [[inputs.minecraft]]\n#   ## Address of the Minecraft server.\n#   # server = \"localhost\"\n#\n#   ## Server RCON Port.\n#   # port = \"25575\"\n#\n#   ## Server RCON Password.\n#   password = \"\"\n#\n#   ## Uncomment to remove deprecated metric components.\n#   # tagdrop = [\"server\"]\n\n\n# # Generate metrics for test and demonstration purposes\n# [[inputs.mock]]\n#   ## Set the metric name to use for reporting\n#   metric_name = \"mock\"\n#\n#   ## Optional string key-value pairs of tags to add to all metrics\n#   # [inputs.mock.tags]\n#   # \"key\" = \"value\"\n#\n#   ## One or more mock data fields *must* be defined.\n#   ##\n#   ## [[inputs.mock.random]]\n#   ##   name = \"rand\"\n#   ##   min = 1.0\n#   ##   max = 6.0\n#   ## [[inputs.mock.sine_wave]]\n#   ##   name = \"wave\"\n#   ##   amplitude = 1.0\n#   ##   period = 0.5\n#   ## [[inputs.mock.step]]\n#   ##   name = \"plus_one\"\n#   ##   start = 0.0\n#   ##   step = 1.0\n#   ## [[inputs.mock.stock]]\n#   ##   name = \"abc\"\n#   ##   price = 50.00\n#   ##   volatility = 0.2\n\n\n# # Retrieve data from MODBUS slave devices\n# [[inputs.modbus]]\n#   ## Connection Configuration\n#   ##\n#   ## The plugin supports connections to PLCs via MODBUS/TCP, RTU over TCP, ASCII over TCP or\n#   ## via serial line communication in binary (RTU) or readable (ASCII) encoding\n#   ##\n#   ## Device name\n#   name = \"Device\"\n#\n#   ## Slave ID - addresses a MODBUS device on the bus\n#   ## Range: 0 - 255 [0 = broadcast; 248 - 255 = reserved]\n#   slave_id = 1\n#\n#   ## Timeout for each request\n#   timeout = \"1s\"\n#\n#   ## Maximum number of retries and the time to wait between retries\n#   ## when a slave-device is busy.\n#   # busy_retries = 0\n#   # busy_retries_wait = \"100ms\"\n#\n#   # TCP - connect via Modbus/TCP\n#   controller = \"tcp://localhost:502\"\n#\n#   ## Serial (RS485; RS232)\n#   # controller = \"file:///dev/ttyUSB0\"\n#   # baud_rate = 9600\n#   # data_bits = 8\n#   # parity = \"N\"\n#   # stop_bits = 1\n#\n#   ## Trace the connection to the modbus device as debug messages\n#   ## Note: You have to enable telegraf's debug mode to see those messages!\n#   # debug_connection = false\n#\n#   ## For Modbus over TCP you can choose between \"TCP\", \"RTUoverTCP\" and \"ASCIIoverTCP\"\n#   ## default behaviour is \"TCP\" if the controller is TCP\n#   ## For Serial you can choose between \"RTU\" and \"ASCII\"\n#   # transmission_mode = \"RTU\"\n#\n# \t## Define the configuration schema\n#   ##  |---register -- define fields per register type in the original style (only supports one slave ID)\n#   ##  |---request  -- define fields on a requests base\n#   configuration_type = \"register\"\n#\n#   ## Per register definition\n#   ##\n#\n#   ## Digital Variables, Discrete Inputs and Coils\n#   ## measurement - the (optional) measurement name, defaults to \"modbus\"\n#   ## name        - the variable name\n#   ## address     - variable address\n#\n#   discrete_inputs = [\n#     { name = \"start\",          address = [0]},\n#     { name = \"stop\",           address = [1]},\n#     { name = \"reset\",          address = [2]},\n#     { name = \"emergency_stop\", address = [3]},\n#   ]\n#   coils = [\n#     { name = \"motor1_run\",     address = [0]},\n#     { name = \"motor1_jog\",     address = [1]},\n#     { name = \"motor1_stop\",    address = [2]},\n#   ]\n#\n#   ## Analog Variables, Input Registers and Holding Registers\n#   ## measurement - the (optional) measurement name, defaults to \"modbus\"\n#   ## name        - the variable name\n#   ## byte_order  - the ordering of bytes\n#   ##  |---AB, ABCD   - Big Endian\n#   ##  |---BA, DCBA   - Little Endian\n#   ##  |---BADC       - Mid-Big Endian\n#   ##  |---CDAB       - Mid-Little Endian\n#   ## data_type  - INT16, UINT16, INT32, UINT32, INT64, UINT64,\n#   ##              FLOAT32-IEEE, FLOAT64-IEEE (the IEEE 754 binary representation)\n#   ##              FLOAT32, FIXED, UFIXED (fixed-point representation on input)\n#   ## scale      - the final numeric variable representation\n#   ## address    - variable address\n#\n#   holding_registers = [\n#     { name = \"power_factor\", byte_order = \"AB\",   data_type = \"FIXED\", scale=0.01,  address = [8]},\n#     { name = \"voltage\",      byte_order = \"AB\",   data_type = \"FIXED\", scale=0.1,   address = [0]},\n#     { name = \"energy\",       byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [5,6]},\n#     { name = \"current\",      byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [1,2]},\n#     { name = \"frequency\",    byte_order = \"AB\",   data_type = \"UFIXED\", scale=0.1,  address = [7]},\n#     { name = \"power\",        byte_order = \"ABCD\", data_type = \"UFIXED\", scale=0.1,  address = [3,4]},\n#   ]\n#   input_registers = [\n#     { name = \"tank_level\",   byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [0]},\n#     { name = \"tank_ph\",      byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [1]},\n#     { name = \"pump1_speed\",  byte_order = \"ABCD\", data_type = \"INT32\",   scale=1.0,     address = [3,4]},\n#   ]\n#\n#\n#   ## Per request definition\n#   ##\n#\n#   ## Define a request sent to the device\n#   ## Multiple of those requests can be defined. Data will be collated into metrics at the end of data collection.\n#   # [[inputs.modbus.request]]\n#     ## ID of the modbus slave device to query.\n#     ## If you need to query multiple slave-devices, create several \"request\" definitions.\n#     # slave_id = 0\n#\n#     ## Byte order of the data.\n#     ##  |---ABCD or MSW-BE -- Big Endian (Motorola)\n#     ##  |---DCBA or LSW-LE -- Little Endian (Intel)\n#     ##  |---BADC or MSW-LE -- Big Endian with byte swap\n#     ##  |---CDAB or LSW-BE -- Little Endian with byte swap\n#     # byte_order = \"ABCD\"\n#\n#     ## Type of the register for the request\n#     ## Can be \"coil\", \"discrete\", \"holding\" or \"input\"\n#     # register = \"holding\"\n#\n#     ## Name of the measurement.\n#     ## Can be overriden by the individual field definitions. Defaults to \"modbus\"\n#     # measurement = \"modbus\"\n#\n#     ## Field definitions\n#     ## Analog Variables, Input Registers and Holding Registers\n#     ## address        - address of the register to query. For coil and discrete inputs this is the bit address.\n#     ## name *1        - field name\n#     ## type *1,2      - type of the modbus field, can be INT16, UINT16, INT32, UINT32, INT64, UINT64 and\n#     ##                  FLOAT32, FLOAT64 (IEEE 754 binary representation)\n#     ## scale *1,2     - (optional) factor to scale the variable with\n#     ## output *1,2    - (optional) type of resulting field, can be INT64, UINT64 or FLOAT64. Defaults to FLOAT64 if\n#     ##                  \"scale\" is provided and to the input \"type\" class otherwise (i.e. INT* -\u003e INT64, etc).\n#     ## measurement *1 - (optional) measurement name, defaults to the setting of the request\n#     ## omit           - (optional) omit this field. Useful to leave out single values when querying many registers\n#     ##                  with a single request. Defaults to \"false\".\n#     ##\n#     ## *1: Those fields are ignored if field is omitted (\"omit\"=true)\n#     ##\n#     ## *2: Thise fields are ignored for both \"coil\" and \"discrete\"-input type of registers. For those register types\n#     ##     the fields are output as zero or one in UINT64 format by default.\n#\n#     ## Coil / discrete input example\n#     # fields = [\n#     #   { address=0, name=\"motor1_run\"},\n#     #   { address=1, name=\"jog\", measurement=\"motor\"},\n#     #   { address=2, name=\"motor1_stop\", omit=true},\n#     #   { address=3, name=\"motor1_overheating\"},\n#     # ]\n#\n#     ## Per-request tags\n#     ## These tags take precedence over predefined tags.\n#     # [[inputs.modbus.request.tags]]\n#     #\t  name = \"value\"\n#\n#     ## Holding / input example\n#     ## All of those examples will result in FLOAT64 field outputs\n#     # fields = [\n#     #   { address=0, name=\"voltage\",      type=\"INT16\",   scale=0.1   },\n#     #   { address=1, name=\"current\",      type=\"INT32\",   scale=0.001 },\n#     #   { address=3, name=\"power\",        type=\"UINT32\",  omit=true   },\n#     #   { address=5, name=\"energy\",       type=\"FLOAT32\", scale=0.001, measurement=\"W\" },\n#     #   { address=7, name=\"frequency\",    type=\"UINT32\",  scale=0.1   },\n#     #   { address=8, name=\"power_factor\", type=\"INT64\",   scale=0.01  },\n#     # ]\n#\n#     ## Holding / input example with type conversions\n#     # fields = [\n#     #   { address=0, name=\"rpm\",         type=\"INT16\"                   },  # will result in INT64 field\n#     #   { address=1, name=\"temperature\", type=\"INT16\", scale=0.1        },  # will result in FLOAT64 field\n#     #   { address=2, name=\"force\",       type=\"INT32\", output=\"FLOAT64\" },  # will result in FLOAT64 field\n#     #   { address=4, name=\"hours\",       type=\"UINT32\"                  },  # will result in UIN64 field\n#     # ]\n#\n#     ## Per-request tags\n# \t\t## These tags take precedence over predefined tags.\n#     # [[inputs.modbus.request.tags]]\n#     #\t  name = \"value\"\n#\n#\n#\n#   ## Enable workarounds required by some devices to work correctly\n#   # [inputs.modbus.workarounds]\n#     ## Pause between read requests sent to the device. This might be necessary for (slow) serial devices.\n#     # pause_between_requests = \"0ms\"\n#     ## Close the connection after every gather cycle. Usually the plugin closes the connection after a certain\n#     ## idle-timeout, however, if you query a device with limited simultaneous connectivity (e.g. serial devices)\n#     ## from multiple instances you might want to only stay connected during gather and disconnect afterwards.\n#     # close_connection_after_gather = false\n\n\n# # Read metrics from one or many MongoDB servers\n# [[inputs.mongodb]]\n#   ## An array of URLs of the form:\n#   ##   \"mongodb://\" [user \":\" pass \"@\"] host [ \":\" port]\n#   ## For example:\n#   ##   mongodb://user:auth_key@10.10.3.30:27017,\n#   ##   mongodb://10.10.3.33:18832,\n#   servers = [\"mongodb://127.0.0.1:27017?connect=direct\"]\n#\n#   ## When true, collect cluster status\n#   ## Note that the query that counts jumbo chunks triggers a COLLSCAN, which\n#   ## may have an impact on performance.\n#   # gather_cluster_status = true\n#\n#   ## When true, collect per database stats\n#   # gather_perdb_stats = false\n#\n#   ## When true, collect per collection stats\n#   # gather_col_stats = false\n#\n#   ## When true, collect usage statistics for each collection\n#   ## (insert, update, queries, remove, getmore, commands etc...).\n#   # gather_top_stat = false\n#\n#   ## List of db where collections stats are collected\n#   ## If empty, all db are concerned\n#   # col_stats_dbs = [\"local\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics and status information about processes managed by Monit\n# [[inputs.monit]]\n#   ## Monit HTTPD address\n#   address = \"http://127.0.0.1:2812\"\n#\n#   ## Username and Password for Monit\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Aggregates the contents of multiple files into a single point\n# [[inputs.multifile]]\n#   ## Base directory where telegraf will look for files.\n#   ## Omit this option to use absolute paths.\n#   base_dir = \"/sys/bus/i2c/devices/1-0076/iio:device0\"\n#\n#   ## If true, Telegraf discard all data when a single file can't be read.\n#   ## Else, Telegraf omits the field generated from this file.\n#   # fail_early = true\n#\n#   ## Files to parse each interval.\n#   [[inputs.multifile.file]]\n#     file = \"in_pressure_input\"\n#     dest = \"pressure\"\n#     conversion = \"float\"\n#   [[inputs.multifile.file]]\n#     file = \"in_temp_input\"\n#     dest = \"temperature\"\n#     conversion = \"float(3)\"\n#   [[inputs.multifile.file]]\n#     file = \"in_humidityrelative_input\"\n#     dest = \"humidityrelative\"\n#     conversion = \"float(3)\"\n\n\n# # Read metrics from one or many mysql servers\n# [[inputs.mysql]]\n#   ## specify servers via a url matching:\n#   ##  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify|custom]]\n#   ##  see https://github.com/go-sql-driver/mysql#dsn-data-source-name\n#   ##  e.g.\n#   ##    servers = [\"user:passwd@tcp(127.0.0.1:3306)/?tls=false\"]\n#   ##    servers = [\"user@tcp(127.0.0.1:3306)/?tls=false\"]\n#   #\n#   ## If no servers are specified, then localhost is used as the host.\n#   servers = [\"tcp(127.0.0.1:3306)/\"]\n#\n#   ## Selects the metric output format.\n#   ##\n#   ## This option exists to maintain backwards compatibility, if you have\n#   ## existing metrics do not set or change this value until you are ready to\n#   ## migrate to the new format.\n#   ##\n#   ## If you do not have existing metrics from this plugin set to the latest\n#   ## version.\n#   ##\n#   ## Telegraf \u003e=1.6: metric_version = 2\n#   ##           \u003c1.6: metric_version = 1 (or unset)\n#   metric_version = 2\n#\n#   ## if the list is empty, then metrics are gathered from all database tables\n#   # table_schema_databases = []\n#\n#   ## gather metrics from INFORMATION_SCHEMA.TABLES for databases provided above list\n#   # gather_table_schema = false\n#\n#   ## gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST\n#   # gather_process_list = false\n#\n#   ## gather user statistics from INFORMATION_SCHEMA.USER_STATISTICS\n#   # gather_user_statistics = false\n#\n#   ## gather auto_increment columns and max values from information schema\n#   # gather_info_schema_auto_inc = false\n#\n#   ## gather metrics from INFORMATION_SCHEMA.INNODB_METRICS\n#   # gather_innodb_metrics = false\n#\n#   ## gather metrics from SHOW SLAVE STATUS command output\n#   # gather_slave_status = false\n#\n#   ## gather metrics from all channels from SHOW SLAVE STATUS command output\n#   # gather_all_slave_channels = false\n#\n#   ## use MariaDB dialect for all channels SHOW SLAVE STATUS\n#   # mariadb_dialect = false\n#\n#   ## gather metrics from SHOW BINARY LOGS command output\n#   # gather_binary_logs = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.GLOBAL_VARIABLES\n#   # gather_global_variables = true\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE\n#   # gather_table_io_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS\n#   # gather_table_lock_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE\n#   # gather_index_io_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS\n#   # gather_event_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME\n#   # gather_file_events_stats = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST\n#   # gather_perf_events_statements = false\n#\n#   ## the limits for metrics form perf_events_statements\n#   # perf_events_statements_digest_text_limit = 120\n#   # perf_events_statements_limit = 250\n#   # perf_events_statements_time_limit = 86400\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_ACCOUNT_BY_EVENT_NAME\n#   # gather_perf_sum_per_acc_per_event         = false\n#\n#   ## list of events to be gathered for gather_perf_sum_per_acc_per_event\n#   ## in case of empty list all events will be gathered\n#   # perf_summary_events                       = []\n#\n#   ## Some queries we may want to run less often (such as SHOW GLOBAL VARIABLES)\n#   ##   example: interval_slow = \"30m\"\n#   # interval_slow = \"\"\n#\n#   ## Optional TLS Config (will be used if tls=custom parameter specified in server uri)\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Provides metrics about the state of a NATS server\n# [[inputs.nats]]\n#   ## The address of the monitoring endpoint of the NATS server\n#   server = \"http://localhost:8222\"\n#\n#   ## Maximum time to receive response\n#   # response_timeout = \"5s\"\n\n\n# # Neptune Apex data collector\n# [[inputs.neptune_apex]]\n#   ## The Neptune Apex plugin reads the publicly available status.xml data from a local Apex.\n#   ## Measurements will be logged under \"apex\".\n#\n#   ## The base URL of the local Apex(es). If you specify more than one server, they will\n#   ## be differentiated by the \"source\" tag.\n#   servers = [\n#     \"http://apex.local\",\n#   ]\n#\n#   ## The response_timeout specifies how long to wait for a reply from the Apex.\n#   #response_timeout = \"5s\"\n\n\n# # Read metrics about network interface usage\n# [[inputs.net]]\n#   ## By default, telegraf gathers stats from any up interface (excluding loopback)\n#   ## Setting interfaces will tell it to gather these explicit interfaces,\n#   ## regardless of status.\n#   ##\n#   # interfaces = [\"eth0\"]\n#   ##\n#   ## On linux systems telegraf also collects protocol stats.\n#   ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.\n#   ##\n#   # ignore_protocol_stats = false\n#   ##\n\n\n# # Collect response time of a TCP or UDP connection\n# [[inputs.net_response]]\n#   ## Protocol, must be \"tcp\" or \"udp\"\n#   ## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n#   ## a send/expect string pair (see below).\n#   protocol = \"tcp\"\n#   ## Server address (default localhost)\n#   address = \"localhost:80\"\n#\n#   ## Set timeout\n#   # timeout = \"1s\"\n#\n#   ## Set read timeout (only used if expecting a response)\n#   # read_timeout = \"1s\"\n#\n#   ## The following options are required for UDP checks. For TCP, they are\n#   ## optional. The plugin will send the given string to the server and then\n#   ## expect to receive the given 'expect' string back.\n#   ## string sent to the server\n#   # send = \"ssh\"\n#   ## expected string in answer\n#   # expect = \"ssh\"\n#\n#   ## Uncomment to remove deprecated fields\n#   # fielddrop = [\"result_type\", \"string_found\"]\n\n\n# # Read TCP metrics such as established, time wait and sockets counts.\n# [[inputs.netstat]]\n#   # no configuration\n\n\n# # Read per-mount NFS client metrics from /proc/self/mountstats\n# [[inputs.nfsclient]]\n#   ## Read more low-level metrics (optional, defaults to false)\n#   # fullstat = false\n#\n#   ## List of mounts to explictly include or exclude (optional)\n#   ## The pattern (Go regexp) is matched against the mount point (not the\n#   ## device being mounted).  If include_mounts is set, all mounts are ignored\n#   ## unless present in the list. If a mount is listed in both include_mounts\n#   ## and exclude_mounts, it is excluded.  Go regexp patterns can be used.\n#   # include_mounts = []\n#   # exclude_mounts = []\n#\n#   ## List of operations to include or exclude from collecting.  This applies\n#   ## only when fullstat=true.  Symantics are similar to {include,exclude}_mounts:\n#   ## the default is to collect everything; when include_operations is set, only\n#   ## those OPs are collected; when exclude_operations is set, all are collected\n#   ## except those listed.  If include and exclude are set, the OP is excluded.\n#   ## See /proc/self/mountstats for a list of valid operations; note that\n#   ## NFSv3 and NFSv4 have different lists.  While it is not possible to\n#   ## have different include/exclude lists for NFSv3/4, unused elements\n#   ## in the list should be okay.  It is possible to have different lists\n#   ## for different mountpoints:  use mulitple [[input.nfsclient]] stanzas,\n#   ## with their own lists.  See \"include_mounts\" above, and be careful of\n#   ## duplicate metrics.\n#   # include_operations = []\n#   # exclude_operations = []\n\n\n# # Read Nginx's basic status information (ngx_http_stub_status_module)\n# [[inputs.nginx]]\n#   # An array of Nginx stub_status URI to gather stats.\n#   urls = [\"http://localhost/server_status\"]\n#\n#   ## Optional TLS Config\n#   tls_ca = \"/etc/telegraf/ca.pem\"\n#   tls_cert = \"/etc/telegraf/cert.cer\"\n#   tls_key = \"/etc/telegraf/key.key\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   insecure_skip_verify = false\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n\n\n# # Read Nginx Plus' full status information (ngx_http_status_module)\n# [[inputs.nginx_plus]]\n#   ## An array of ngx_http_status_module or status URI to gather stats.\n#   urls = [\"http://localhost/status\"]\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read Nginx Plus Api documentation\n# [[inputs.nginx_plus_api]]\n#   ## An array of API URI to gather stats.\n#   urls = [\"http://localhost/api\"]\n#\n#   # Nginx API version, default: 3\n#   # api_version = 3\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read Nginx virtual host traffic status module information (nginx-module-sts)\n# [[inputs.nginx_sts]]\n#   ## An array of ngx_http_status_module or status URI to gather stats.\n#   urls = [\"http://localhost/status\"]\n#\n#   ## HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read nginx_upstream_check module status information (https://github.com/yaoweibin/nginx_upstream_check_module)\n# [[inputs.nginx_upstream_check]]\n#   ## An URL where Nginx Upstream check module is enabled\n#   ## It should be set to return a JSON formatted response\n#   url = \"http://127.0.0.1/status?format=json\"\n#\n#   ## HTTP method\n#   # method = \"GET\"\n#\n#   ## Optional HTTP headers\n#   # headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## Override HTTP \"Host\" header\n#   # host_header = \"check.example.com\"\n#\n#   ## Timeout for HTTP requests\n#   timeout = \"5s\"\n#\n#   ## Optional HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read Nginx virtual host traffic status module information (nginx-module-vts)\n# [[inputs.nginx_vts]]\n#   ## An array of ngx_http_status_module or status URI to gather stats.\n#   urls = [\"http://localhost/status\"]\n#\n#   ## HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from the Nomad API\n# [[inputs.nomad]]\n#   ## URL for the Nomad agent\n#   # url = \"http://127.0.0.1:4646\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n\n\n# # A plugin to collect stats from the NSD authoritative DNS name server\n# [[inputs.nsd]]\n#   ## Address of server to connect to, optionally ':port'. Defaults to the\n#   ## address in the nsd config file.\n#   server = \"127.0.0.1:8953\"\n#\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   # use_sudo = false\n#\n#   ## The default location of the nsd-control binary can be overridden with:\n#   # binary = \"/usr/sbin/nsd-control\"\n#\n#   ## The default location of the nsd config file can be overridden with:\n#   # config_file = \"/etc/nsd/nsd.conf\"\n#\n#   ## The default timeout of 1s can be overridden with:\n#   # timeout = \"1s\"\n\n\n# # Read NSQ topic and channel statistics.\n# [[inputs.nsq]]\n#   ## An array of NSQD HTTP API endpoints\n#   endpoints  = [\"http://localhost:4151\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Collect kernel snmp counters and network interface statistics\n# [[inputs.nstat]]\n#   ## file paths for proc files. If empty default paths will be used:\n#   ##    /proc/net/netstat, /proc/net/snmp, /proc/net/snmp6\n#   ## These can also be overridden with env variables, see README.\n#   proc_net_netstat = \"/proc/net/netstat\"\n#   proc_net_snmp = \"/proc/net/snmp\"\n#   proc_net_snmp6 = \"/proc/net/snmp6\"\n#   ## dump metrics with 0 values too\n#   dump_zeros       = true\n\n\n# # Get standard NTP query metrics, requires ntpq executable.\n# [[inputs.ntpq]]\n#   ## If false, set the -n ntpq flag. Can reduce metric gather time.\n#   dns_lookup = true\n\n\n# # Pulls statistics from nvidia GPUs attached to the host\n# [[inputs.nvidia_smi]]\n#   ## Optional: path to nvidia-smi binary, defaults \"/usr/bin/nvidia-smi\"\n#   ## We will first try to locate the nvidia-smi binary with the explicitly specified value (or default value),\n#   ## if it is not found, we will try to locate it on PATH(exec.LookPath), if it is still not found, an error will be returned\n#   # bin_path = \"/usr/bin/nvidia-smi\"\n#\n#   ## Optional: timeout for GPU polling\n#   # timeout = \"5s\"\n\n\n# # Retrieve data from OPCUA devices\n# [[inputs.opcua]]\n#   ## Metric name\n#   # name = \"opcua\"\n#   #\n#   ## OPC UA Endpoint URL\n#   # endpoint = \"opc.tcp://localhost:4840\"\n#   #\n#   ## Maximum time allowed to establish a connect to the endpoint.\n#   # connect_timeout = \"10s\"\n#   #\n#   ## Maximum time allowed for a request over the estabilished connection.\n#   # request_timeout = \"5s\"\n#   #\n#   ## Security policy, one of \"None\", \"Basic128Rsa15\", \"Basic256\",\n#   ## \"Basic256Sha256\", or \"auto\"\n#   # security_policy = \"auto\"\n#   #\n#   ## Security mode, one of \"None\", \"Sign\", \"SignAndEncrypt\", or \"auto\"\n#   # security_mode = \"auto\"\n#   #\n#   ## Path to cert.pem. Required when security mode or policy isn't \"None\".\n#   ## If cert path is not supplied, self-signed cert and key will be generated.\n#   # certificate = \"/etc/telegraf/cert.pem\"\n#   #\n#   ## Path to private key.pem. Required when security mode or policy isn't \"None\".\n#   ## If key path is not supplied, self-signed cert and key will be generated.\n#   # private_key = \"/etc/telegraf/key.pem\"\n#   #\n#   ## Authentication Method, one of \"Certificate\", \"UserName\", or \"Anonymous\".  To\n#   ## authenticate using a specific ID, select 'Certificate' or 'UserName'\n#   # auth_method = \"Anonymous\"\n#   #\n#   ## Username. Required for auth_method = \"UserName\"\n#   # username = \"\"\n#   #\n#   ## Password. Required for auth_method = \"UserName\"\n#   # password = \"\"\n#   #\n#   ## Option to select the metric timestamp to use. Valid options are:\n#   ##     \"gather\" -- uses the time of receiving the data in telegraf\n#   ##     \"server\" -- uses the timestamp provided by the server\n#   ##     \"source\" -- uses the timestamp provided by the source\n#   # timestamp = \"gather\"\n#   #\n#   ## Node ID configuration\n#   ## name              - field name to use in the output\n#   ## namespace         - OPC UA namespace of the node (integer value 0 thru 3)\n#   ## identifier_type   - OPC UA ID type (s=string, i=numeric, g=guid, b=opaque)\n#   ## identifier        - OPC UA ID (tag as shown in opcua browser)\n#   ## Example:\n#   ## {name=\"ProductUri\", namespace=\"0\", identifier_type=\"i\", identifier=\"2262\"}\n#   # nodes = [\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #]\n#   #\n#   ## Node Group\n#   ## Sets defaults for OPC UA namespace and ID type so they aren't required in\n#   ## every node.  A group can also have a metric name that overrides the main\n#   ## plugin metric name.\n#   ##\n#   ## Multiple node groups are allowed\n#   #[[inputs.opcua.group]]\n#   ## Group Metric name. Overrides the top level name.  If unset, the\n#   ## top level name is used.\n#   # name =\n#   #\n#   ## Group default namespace. If a node in the group doesn't set its\n#   ## namespace, this is used.\n#   # namespace =\n#   #\n#   ## Group default identifier type. If a node in the group doesn't set its\n#   ## namespace, this is used.\n#   # identifier_type =\n#   #\n#   ## Node ID Configuration.  Array of nodes with the same settings as above.\n#   # nodes = [\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #]\n#\n#   ## Enable workarounds required by some devices to work correctly\n#   # [inputs.opcua.workarounds]\n#     ## Set additional valid status codes, StatusOK (0x0) is always considered valid\n#     # additional_valid_status_codes = [\"0xC0\"]\n\n\n# # OpenLDAP cn=Monitor plugin\n# [[inputs.openldap]]\n#   host = \"localhost\"\n#   port = 389\n#\n#   # ldaps, starttls, or no encryption. default is an empty string, disabling all encryption.\n#   # note that port will likely need to be changed to 636 for ldaps\n#   # valid options: \"\" | \"starttls\" | \"ldaps\"\n#   tls = \"\"\n#\n#   # skip peer certificate verification. Default is false.\n#   insecure_skip_verify = false\n#\n#   # Path to PEM-encoded Root certificate to use to verify server certificate\n#   tls_ca = \"/etc/ssl/certs.pem\"\n#\n#   # dn/password to bind with. If bind_dn is empty, an anonymous bind is performed.\n#   bind_dn = \"\"\n#   bind_password = \"\"\n#\n#   # Reverse metric names so they sort more naturally. Recommended.\n#   # This defaults to false if unset, but is set to true when generating a new config\n#   reverse_metric_names = true\n\n\n# # Get standard NTP query metrics from OpenNTPD.\n# [[inputs.openntpd]]\n#   ## Run ntpctl binary with sudo.\n#   # use_sudo = false\n#\n#   ## Location of the ntpctl binary.\n#   # binary = \"/usr/sbin/ntpctl\"\n#\n#   ## Maximum time the ntpctl binary is allowed to run.\n#   # timeout = \"5ms\"\n\n\n# # A plugin to collect stats from Opensmtpd - a validating, recursive, and caching DNS resolver \n# [[inputs.opensmtpd]]\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   #use_sudo = false\n#\n#   ## The default location of the smtpctl binary can be overridden with:\n#   binary = \"/usr/sbin/smtpctl\"\n#\n#   ## The default timeout of 1000ms can be overridden with (in milliseconds):\n#   timeout = 1000\n\n\n# # Collects performance metrics from OpenStack services\n# [[inputs.openstack]]\n#   ## The recommended interval to poll is '30m'\n#\n#   ## The identity endpoint to authenticate against and get the service catalog from.\n#   authentication_endpoint = \"https://my.openstack.cloud:5000\"\n#\n#   ## The domain to authenticate against when using a V3 identity endpoint.\n#   # domain = \"default\"\n#\n#   ## The project to authenticate as.\n#   # project = \"admin\"\n#\n#   ## User authentication credentials. Must have admin rights.\n#   username = \"admin\"\n#   password = \"password\"\n#\n#   ## Available services are:\n#   ## \"agents\", \"aggregates\", \"flavors\", \"hypervisors\", \"networks\", \"nova_services\",\n#   ## \"ports\", \"projects\", \"servers\", \"services\", \"stacks\", \"storage_pools\", \"subnets\", \"volumes\"\n#   # enabled_services = [\"services\", \"projects\", \"hypervisors\", \"flavors\", \"networks\", \"volumes\"]\n#\n#   ## Collect Server Diagnostics\n#   # server_diagnotics = false\n#\n#   ## output secrets (such as adminPass(for server) and UserID(for volume)).\n#   # output_secrets = false\n#\n#   ## Amount of time allowed to complete the HTTP(s) request.\n#   # timeout = \"5s\"\n#\n#   ## HTTP Proxy support\n#   # http_proxy_url = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Options for tags received from Openstack\n#   # tag_prefix = \"openstack_tag_\"\n#   # tag_value = \"true\"\n#\n#   ## Timestamp format for timestamp data recieved from Openstack.\n#   ## If false format is unix nanoseconds.\n#   # human_readable_timestamps = false\n#\n#   ## Measure Openstack call duration\n#   # measure_openstack_requests = false\n\n\n# # Read current weather and forecasts data from openweathermap.org\n# [[inputs.openweathermap]]\n#   ## OpenWeatherMap API key.\n#   app_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n#\n#   ## City ID's to collect weather data from.\n#   city_id = [\"5391959\"]\n#\n#   ## Language of the description field. Can be one of \"ar\", \"bg\",\n#   ## \"ca\", \"cz\", \"de\", \"el\", \"en\", \"fa\", \"fi\", \"fr\", \"gl\", \"hr\", \"hu\",\n#   ## \"it\", \"ja\", \"kr\", \"la\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\",\n#   ## \"se\", \"sk\", \"sl\", \"es\", \"tr\", \"ua\", \"vi\", \"zh_cn\", \"zh_tw\"\n#   # lang = \"en\"\n#\n#   ## APIs to fetch; can contain \"weather\" or \"forecast\".\n#   fetch = [\"weather\", \"forecast\"]\n#\n#   ## OpenWeatherMap base URL\n#   # base_url = \"https://api.openweathermap.org/\"\n#\n#   ## Timeout for HTTP response.\n#   # response_timeout = \"5s\"\n#\n#   ## Preferred unit system for temperature and wind speed. Can be one of\n#   ## \"metric\", \"imperial\", or \"standard\".\n#   # units = \"metric\"\n#\n#   ## Query interval; OpenWeatherMap updates their weather data every 10\n#   ## minutes.\n#   interval = \"10m\"\n\n\n# # Read metrics of passenger using passenger-status\n# [[inputs.passenger]]\n#   ## Path of passenger-status.\n#   ##\n#   ## Plugin gather metric via parsing XML output of passenger-status\n#   ## More information about the tool:\n#   ##   https://www.phusionpassenger.com/library/admin/apache/overall_status_report.html\n#   ##\n#   ## If no path is specified, then the plugin simply execute passenger-status\n#   ## hopefully it can be found in your PATH\n#   command = \"passenger-status -v --show=xml\"\n\n\n# # Gather counters from PF\n# [[inputs.pf]]\n#   ## PF require root access on most systems.\n#   ## Setting 'use_sudo' to true will make use of sudo to run pfctl.\n#   ## Users must configure sudo to allow telegraf user to run pfctl with no password.\n#   ## pfctl can be restricted to only list command \"pfctl -s info\".\n#   use_sudo = false\n\n\n# # Read metrics of phpfpm, via HTTP status page or socket\n# [[inputs.phpfpm]]\n#   ## An array of addresses to gather stats about. Specify an ip or hostname\n#   ## with optional port and path\n#   ##\n#   ## Plugin can be configured in three modes (either can be used):\n#   ##   - http: the URL must start with http:// or https://, ie:\n#   ##       \"http://localhost/status\"\n#   ##       \"http://192.168.130.1/status?full\"\n#   ##\n#   ##   - unixsocket: path to fpm socket, ie:\n#   ##       \"/var/run/php5-fpm.sock\"\n#   ##      or using a custom fpm status path:\n#   ##       \"/var/run/php5-fpm.sock:fpm-custom-status-path\"\n#   ##\n#   ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n#   ##       \"fcgi://10.0.0.12:9000/status\"\n#   ##       \"cgi://10.0.10.12:9001/status\"\n#   ##\n#   ## Example of multiple gathering from local socket and remote host\n#   ## urls = [\"http://192.168.1.20/status\", \"/tmp/fpm.sock\"]\n#   urls = [\"http://localhost/status\"]\n#\n#   ## Duration allowed to complete HTTP requests.\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Ping given url(s) and return statistics\n# [[inputs.ping]]\n#   ## Hosts to send ping packets to.\n#   urls = [\"example.org\"]\n#\n#   ## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n#   ## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n#   ## the plugin will send pings directly.\n#   ##\n#   ## While the default is \"exec\" for backwards compatibility, new deployments\n#   ## are encouraged to use the \"native\" method for improved compatibility and\n#   ## performance.\n#   # method = \"exec\"\n#\n#   ## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n#   ## option of the ping command.\n#   # count = 1\n#\n#   ## Time to wait between sending ping packets in seconds.  Operates like the\n#   ## \"-i\" option of the ping command.\n#   # ping_interval = 1.0\n#\n#   ## If set, the time to wait for a ping response in seconds.  Operates like\n#   ## the \"-W\" option of the ping command.\n#   # timeout = 1.0\n#\n#   ## If set, the total ping deadline, in seconds.  Operates like the -w option\n#   ## of the ping command.\n#   # deadline = 10\n#\n#   ## Interface or source address to send ping from.  Operates like the -I or -S\n#   ## option of the ping command.\n#   # interface = \"\"\n#\n#   ## Percentiles to calculate. This only works with the native method.\n#   # percentiles = [50, 95, 99]\n#\n#   ## Specify the ping executable binary.\n#   # binary = \"ping\"\n#\n#   ## Arguments for ping command. When arguments is not empty, the command from\n#   ## the binary option will be used and other options (ping_interval, timeout,\n#   ## etc) will be ignored.\n#   # arguments = [\"-c\", \"3\"]\n#\n#   ## Use only IPv6 addresses when resolving a hostname.\n#   # ipv6 = false\n#\n#   ## Number of data bytes to be sent. Corresponds to the \"-s\"\n#   ## option of the ping command. This only works with the native method.\n#   # size = 56\n\n\n# # Measure postfix queue statistics\n# [[inputs.postfix]]\n#   ## Postfix queue directory. If not provided, telegraf will try to use\n#   ## 'postconf -h queue_directory' to determine it.\n#   # queue_directory = \"/var/spool/postfix\"\n\n\n# # Read metrics from one or many PowerDNS servers\n# [[inputs.powerdns]]\n#   ## An array of sockets to gather stats about.\n#   ## Specify a path to unix socket.\n#   unix_sockets = [\"/var/run/pdns.controlsocket\"]\n\n\n# # Read metrics from one or many PowerDNS Recursor servers\n# [[inputs.powerdns_recursor]]\n#   ## Path to the Recursor control socket.\n#   unix_sockets = [\"/var/run/pdns_recursor.controlsocket\"]\n#\n#   ## Directory to create receive socket.  This default is likely not writable,\n#   ## please reference the full plugin documentation for a recommended setup.\n#   # socket_dir = \"/var/run/\"\n#   ## Socket permissions for the receive socket.\n#   # socket_mode = \"0666\"\n\n\n# # Monitor process cpu and memory usage\n# [[inputs.procstat]]\n#   ## PID file to monitor process\n#   pid_file = \"/var/run/nginx.pid\"\n#   ## executable name (ie, pgrep \u003cexe\u003e)\n#   # exe = \"nginx\"\n#   ## pattern as argument for pgrep (ie, pgrep -f \u003cpattern\u003e)\n#   # pattern = \"nginx\"\n#   ## user as argument for pgrep (ie, pgrep -u \u003cuser\u003e)\n#   # user = \"nginx\"\n#   ## Systemd unit name, supports globs when include_systemd_children is set to true\n#   # systemd_unit = \"nginx.service\"\n#   # include_systemd_children = false\n#   ## CGroup name or path, supports globs\n#   # cgroup = \"systemd/system.slice/nginx.service\"\n#\n#   ## Windows service name\n#   # win_service = \"\"\n#\n#   ## override for process_name\n#   ## This is optional; default is sourced from /proc/\u003cpid\u003e/status\n#   # process_name = \"bar\"\n#\n#   ## Field name prefix\n#   # prefix = \"\"\n#\n#   ## When true add the full cmdline as a tag.\n#   # cmdline_tag = false\n#\n#   ## Mode to use when calculating CPU usage. Can be one of 'solaris' or 'irix'.\n#   # mode = \"irix\"\n#\n#   ## Add the PID as a tag instead of as a field.  When collecting multiple\n#   ## processes with otherwise matching tags this setting should be enabled to\n#   ## ensure each process has a unique identity.\n#   ##\n#   ## Enabling this option may result in a large number of series, especially\n#   ## when processes have a short lifetime.\n#   # pid_tag = false\n#\n#   ## Method to use when finding process IDs.  Can be one of 'pgrep', or\n#   ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while\n#   ## the native finder performs the search directly in a manor dependent on the\n#   ## platform.  Default is 'pgrep'\n#   # pid_finder = \"pgrep\"\n\n\n# # Provides metrics from Proxmox nodes (Proxmox Virtual Environment \u003e 6.2).\n# [[inputs.proxmox]]\n#   ## API connection configuration. The API token was introduced in Proxmox v6.2. Required permissions for user and token: PVEAuditor role on /.\n#   base_url = \"https://localhost:8006/api2/json\"\n#   api_token = \"USER@REALM!TOKENID=UUID\"\n#   ## Node name, defaults to OS hostname\n#   # node_name = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   insecure_skip_verify = false\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n\n\n# # Reads last_run_summary.yaml file and converts to measurements\n# [[inputs.puppetagent]]\n#   ## Location of puppet last run summary file\n#   location = \"/var/lib/puppet/state/last_run_summary.yaml\"\n\n\n# # Reads metrics from RabbitMQ servers via the Management Plugin\n# [[inputs.rabbitmq]]\n#   ## Management Plugin url. (default: http://localhost:15672)\n#   # url = \"http://localhost:15672\"\n#   ## Credentials\n#   # username = \"guest\"\n#   # password = \"guest\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional request timeouts\n#   ##\n#   ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n#   ## for a server's response headers after fully writing the request.\n#   # header_timeout = \"3s\"\n#   ##\n#   ## client_timeout specifies a time limit for requests made by this client.\n#   ## Includes connection time, any redirects, and reading the response body.\n#   # client_timeout = \"4s\"\n#\n#   ## A list of nodes to gather as the rabbitmq_node measurement. If not\n#   ## specified, metrics for all nodes are gathered.\n#   # nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n#\n#   ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n#   ## specified, metrics for all exchanges are gathered.\n#   # exchanges = [\"telegraf\"]\n#\n#   ## Metrics to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all metrics\n#   ## Currently the following metrics are supported: \"exchange\", \"federation\", \"node\", \"overview\", \"queue\"\n#   # metric_include = []\n#   # metric_exclude = []\n#\n#   ## Queues to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all queues\n#   queue_name_include = []\n#   queue_name_exclude = []\n#\n#   ## Federation upstreams include and exclude when gathering the rabbitmq_federation measurement.\n#   ## If neither are specified, metrics for all federation upstreams are gathered.\n#   ## Federation link metrics will only be gathered for queues and exchanges\n#   ## whose non-federation metrics will be collected (e.g a queue excluded\n#   ## by the 'queue_name_exclude' option will also be excluded from federation).\n#   ## Globs accepted.\n#   # federation_upstream_include = [\"dataCentre-*\"]\n#   # federation_upstream_exclude = []\n\n\n# # Read raindrops stats (raindrops - real-time stats for preforking Rack servers)\n# [[inputs.raindrops]]\n#   ## An array of raindrops middleware URI to gather stats.\n#   urls = [\"http://localhost:8080/_raindrops\"]\n\n\n# # Reads metrics from RavenDB servers via the Monitoring Endpoints\n# [[inputs.ravendb]]\n#   ## Node URL and port that RavenDB is listening on. By default,\n#   ## attempts to connect securely over HTTPS, however, if the user\n#   ## is running a local unsecure development cluster users can use\n#   ## HTTP via a URL like \"http://localhost:8080\"\n#   url = \"https://localhost:4433\"\n#\n#   ## RavenDB X509 client certificate setup\n#   # tls_cert = \"/etc/telegraf/raven.crt\"\n#   # tls_key = \"/etc/telegraf/raven.key\"\n#\n#   ## Optional request timeout\n#   ##\n#   ## Timeout, specifies the amount of time to wait\n#   ## for a server's response headers after fully writing the request and\n#   ## time limit for requests made by this client\n#   # timeout = \"5s\"\n#\n#   ## List of statistics which are collected\n#   # At least one is required\n#   # Allowed values: server, databases, indexes, collections\n#   #\n#   # stats_include = [\"server\", \"databases\", \"indexes\", \"collections\"]\n#\n#   ## List of db where database stats are collected\n#   ## If empty, all db are concerned\n#   # db_stats_dbs = []\n#\n#   ## List of db where index status are collected\n#   ## If empty, all indexes from all db are concerned\n#   # index_stats_dbs = []\n#\n#   ## List of db where collection status are collected\n#   ## If empty, all collections from all db are concerned\n#   # collection_stats_dbs = []\n\n\n# # Read CPU, Fans, Powersupply and Voltage metrics of hardware server through redfish APIs\n# [[inputs.redfish]]\n#   ## Server url\n#   address = \"https://127.0.0.1:5000\"\n#\n#   ## Username, Password for hardware server\n#   username = \"root\"\n#   password = \"password123456\"\n#\n#   ## ComputerSystemId\n#   computer_system_id=\"2M220100SL\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from one or many redis servers\n# [[inputs.redis]]\n#   ## specify servers via a url matching:\n#   ##  [protocol://][:password]@address[:port]\n#   ##  e.g.\n#   ##    tcp://localhost:6379\n#   ##    tcp://:password@192.168.99.100\n#   ##    unix:///var/run/redis.sock\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no port is specified, 6379 is used\n#   servers = [\"tcp://localhost:6379\"]\n#\n#   ## Optional. Specify redis commands to retrieve values\n#   # [[inputs.redis.commands]]\n#   #   # The command to run where each argument is a separate element\n#   #   command = [\"get\", \"sample-key\"]\n#   #   # The field to store the result in\n#   #   field = \"sample-key-value\"\n#   #   # The type of the result\n#   #   # Can be \"string\", \"integer\", or \"float\"\n#   #   type = \"string\"\n#\n#   ## specify server password\n#   # password = \"s#cr@t%\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Read metrics from one or many redis-sentinel servers\n# [[inputs.redis_sentinel]]\n#   ## specify servers via a url matching:\n#   ##  [protocol://][:password]@address[:port]\n#   ##  e.g.\n#   ##    tcp://localhost:26379\n#   ##    tcp://:password@192.168.99.100\n#   ##    unix:///var/run/redis-sentinel.sock\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no port is specified, 26379 is used\n#   # servers = [\"tcp://localhost:26379\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Read metrics from one or many RethinkDB servers\n# [[inputs.rethinkdb]]\n#   ## An array of URI to gather stats about. Specify an ip or hostname\n#   ## with optional port add password. ie,\n#   ##   rethinkdb://user:auth_key@10.10.3.30:28105,\n#   ##   rethinkdb://10.10.3.33:18832,\n#   ##   10.0.0.1:10000, etc.\n#   servers = [\"127.0.0.1:28015\"]\n#   ##\n#   ## If you use actual rethinkdb of \u003e 2.3.0 with username/password authorization,\n#   ## protocol have to be named \"rethinkdb2\" - it will use 1_0 H.\n#   # servers = [\"rethinkdb2://username:password@127.0.0.1:28015\"]\n#   ##\n#   ## If you use older versions of rethinkdb (\u003c2.2) with auth_key, protocol\n#   ## have to be named \"rethinkdb\".\n#   # servers = [\"rethinkdb://username:auth_key@127.0.0.1:28015\"]\n\n\n# # Read metrics one or many Riak servers\n# [[inputs.riak]]\n#   # Specify a list of one or more riak http servers\n#   servers = [\"http://localhost:8098\"]\n\n\n# # Read API usage and limits for a Salesforce organisation\n# [[inputs.salesforce]]\n#   ## specify your credentials\n#   ##\n#   username = \"your_username\"\n#   password = \"your_password\"\n#   ##\n#   ## (optional) security token\n#   # security_token = \"your_security_token\"\n#   ##\n#   ## (optional) environment type (sandbox or production)\n#   ## default is: production\n#   ##\n#   # environment = \"production\"\n#   ##\n#   ## (optional) API version (default: \"39.0\")\n#   ##\n#   # version = \"39.0\"\n\n\n# # Monitor sensors, requires lm-sensors package\n# [[inputs.sensors]]\n#   ## Remove numbers from field names.\n#   ## If true, a field name like 'temp1_input' will be changed to 'temp_input'.\n#   # remove_numbers = true\n#\n#   ## Timeout is the maximum amount of time that the sensors command can run.\n#   # timeout = \"5s\"\n\n\n# # Read metrics from storage devices supporting S.M.A.R.T.\n# [[inputs.smart]]\n#   ## Optionally specify the path to the smartctl executable\n#   # path_smartctl = \"/usr/bin/smartctl\"\n#\n#   ## Optionally specify the path to the nvme-cli executable\n#   # path_nvme = \"/usr/bin/nvme\"\n#\n#   ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n#   ## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n#   ## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n#   # enable_extensions = [\"auto-on\"]\n#\n#   ## On most platforms used cli utilities requires root access.\n#   ## Setting 'use_sudo' to true will make use of sudo to run smartctl or nvme-cli.\n#   ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli\n#   ## without a password.\n#   # use_sudo = false\n#\n#   ## Skip checking disks in this power mode. Defaults to\n#   ## \"standby\" to not wake up disks that have stopped rotating.\n#   ## See --nocheck in the man pages for smartctl.\n#   ## smartctl version 5.41 and 5.42 have faulty detection of\n#   ## power mode and might require changing this value to\n#   ## \"never\" depending on your disks.\n#   # nocheck = \"standby\"\n#\n#   ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n#   ## information from each drive into the 'smart_attribute' measurement.\n#   # attributes = false\n#\n#   ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n#   # excludes = [ \"/dev/pass6\" ]\n#\n#   ## Optionally specify devices and device type, if unset\n#   ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n#   ## and all found will be included except for the excluded in excludes.\n#   # devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n#\n#   ## Timeout for the cli command to complete.\n#   # timeout = \"30s\"\n#\n#   ## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n#   ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n#   ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n#   ## SMART data - one individual array drive at the time. In such case please set this configuration option\n#   ## to \"sequential\" to get readings for all drives.\n#   ## valid options: concurrent, sequential\n#   # read_method = \"concurrent\"\n\n\n# # Retrieves SNMP values from remote agents\n# [[inputs.snmp]]\n#   ## Agent addresses to retrieve values from.\n#   ##   format:  agents = [\"\u003cscheme://\u003e\u003chostname\u003e:\u003cport\u003e\"]\n#   ##   scheme:  optional, either udp, udp4, udp6, tcp, tcp4, tcp6.\n#   ##            default is udp\n#   ##   port:    optional\n#   ##   example: agents = [\"udp://127.0.0.1:161\"]\n#   ##            agents = [\"tcp://127.0.0.1:161\"]\n#   ##            agents = [\"udp4://v4only-snmp-agent\"]\n#   agents = [\"udp://127.0.0.1:161\"]\n#\n#   ## Timeout for each request.\n#   # timeout = \"5s\"\n#\n#   ## SNMP version; can be 1, 2, or 3.\n#   # version = 2\n#\n#   ## Path to mib files\n#   # path = [\"/usr/share/snmp/mibs\"]\n#\n#   ## Agent host tag; the tag used to reference the source host\n#   # agent_host_tag = \"agent_host\"\n#\n#   ## SNMP community string.\n#   # community = \"public\"\n#\n#   ## Number of retries to attempt.\n#   # retries = 3\n#\n#   ## The GETBULK max-repetitions parameter.\n#   # max_repetitions = 10\n#\n#   ## SNMPv3 authentication and encryption options.\n#   ##\n#   ## Security Name.\n#   # sec_name = \"myuser\"\n#   ## Authentication protocol; one of \"MD5\", \"SHA\", \"SHA224\", \"SHA256\", \"SHA384\", \"SHA512\" or \"\".\n#   # auth_protocol = \"MD5\"\n#   ## Authentication password.\n#   # auth_password = \"pass\"\n#   ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n#   # sec_level = \"authNoPriv\"\n#   ## Context Name.\n#   # context_name = \"\"\n#   ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\" or \"\".\n#   # priv_protocol = \"\"\n#   ## Privacy password used for encrypted messages.\n#   # priv_password = \"\"\n#\n#   ## Add fields and tables defining the variables you wish to collect.  This\n#   ## example collects the system uptime and interface variables.  Reference the\n#   ## full plugin documentation for configuration details.\n\n\n# # DEPRECATED! PLEASE USE inputs.snmp INSTEAD.\n# [[inputs.snmp_legacy]]\n#   ## DEPRECATED: The 'snmp_legacy' plugin is deprecated in version 1.0.0, use 'inputs.snmp' instead.\n#   ## Use 'oids.txt' file to translate oids to names\n#   ## To generate 'oids.txt' you need to run:\n#   ##   snmptranslate -m all -Tz -On | sed -e 's/\"//g' \u003e /tmp/oids.txt\n#   ## Or if you have an other MIB folder with custom MIBs\n#   ##   snmptranslate -M /mycustommibfolder -Tz -On -m all | sed -e 's/\"//g' \u003e oids.txt\n#   snmptranslate_file = \"/tmp/oids.txt\"\n#   [[inputs.snmp.host]]\n#     address = \"192.168.2.2:161\"\n#     # SNMP community\n#     community = \"public\" # default public\n#     # SNMP version (1, 2 or 3)\n#     # Version 3 not supported yet\n#     version = 2 # default 2\n#     # SNMP response timeout\n#     timeout = 2.0 # default 2.0\n#     # SNMP request retries\n#     retries = 2 # default 2\n#     # Which get/bulk do you want to collect for this host\n#     collect = [\"mybulk\", \"sysservices\", \"sysdescr\"]\n#     # Simple list of OIDs to get, in addition to \"collect\"\n#     get_oids = []\n#\n#   [[inputs.snmp.host]]\n#     address = \"192.168.2.3:161\"\n#     community = \"public\"\n#     version = 2\n#     timeout = 2.0\n#     retries = 2\n#     collect = [\"mybulk\"]\n#     get_oids = [\n#         \"ifNumber\",\n#         \".1.3.6.1.2.1.1.3.0\",\n#     ]\n#\n#   [[inputs.snmp.get]]\n#     name = \"ifnumber\"\n#     oid = \"ifNumber\"\n#\n#   [[inputs.snmp.get]]\n#     name = \"interface_speed\"\n#     oid = \"ifSpeed\"\n#     instance = \"0\"\n#\n#   [[inputs.snmp.get]]\n#     name = \"sysuptime\"\n#     oid = \".1.3.6.1.2.1.1.3.0\"\n#     unit = \"second\"\n#\n#   [[inputs.snmp.bulk]]\n#     name = \"mybulk\"\n#     max_repetition = 127\n#     oid = \".1.3.6.1.2.1.1\"\n#\n#   [[inputs.snmp.bulk]]\n#     name = \"ifoutoctets\"\n#     max_repetition = 127\n#     oid = \"ifOutOctets\"\n#\n#   [[inputs.snmp.host]]\n#     address = \"192.168.2.13:161\"\n#     #address = \"127.0.0.1:161\"\n#     community = \"public\"\n#     version = 2\n#     timeout = 2.0\n#     retries = 2\n#     #collect = [\"mybulk\", \"sysservices\", \"sysdescr\", \"systype\"]\n#     collect = [\"sysuptime\" ]\n#     [[inputs.snmp.host.table]]\n#       name = \"iftable3\"\n#       include_instances = [\"enp5s0\", \"eth1\"]\n#\n#   # SNMP TABLEs\n#   # table without mapping neither subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable1\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#\n#   # table without mapping but with subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable2\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#     sub_tables = [\".1.3.6.1.2.1.2.2.1.13\"]\n#\n#   # table with mapping but without subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable3\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#     # if empty. get all instances\n#     mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n#     # if empty, get all subtables\n#\n#   # table with both mapping and subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable4\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#     # if empty get all instances\n#     mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n#     # if empty get all subtables\n#     # sub_tables could be not \"real subtables\"\n#     sub_tables=[\".1.3.6.1.2.1.2.2.1.13\", \"bytes_recv\", \"bytes_send\"]\n\n\n# # Gather indicators from established connections, using iproute2's `ss` command.\n# [[inputs.socketstat]]\n#   ## ss can display information about tcp, udp, raw, unix, packet, dccp and sctp sockets\n#   ## List of protocol types to collect\n#   # protocols = [ \"tcp\", \"udp\" ]\n#   ## The default timeout of 1s for ss execution can be overridden here:\n#   # timeout = \"1s\"\n\n\n# # Read stats from one or more Solr servers or cores\n# [[inputs.solr]]\n#   ## specify a list of one or more Solr servers\n#   servers = [\"http://localhost:8983\"]\n#\n#   ## specify a list of one or more Solr cores (default - all)\n#   # cores = [\"main\"]\n#\n#   ## Optional HTTP Basic Auth Credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n\n\n# # Gather timeseries from Google Cloud Platform v3 monitoring API\n# [[inputs.stackdriver]]\n#   ## GCP Project\n#   project = \"erudite-bloom-151019\"\n#\n#   ## Include timeseries that start with the given metric type.\n#   metric_type_prefix_include = [\n#     \"compute.googleapis.com/\",\n#   ]\n#\n#   ## Exclude timeseries that start with the given metric type.\n#   # metric_type_prefix_exclude = []\n#\n#   ## Many metrics are updated once per minute; it is recommended to override\n#   ## the agent level interval with a value of 1m or greater.\n#   interval = \"1m\"\n#\n#   ## Maximum number of API calls to make per second.  The quota for accounts\n#   ## varies, it can be viewed on the API dashboard:\n#   ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n#   # rate_limit = 14\n#\n#   ## The delay and window options control the number of points selected on\n#   ## each gather.  When set, metrics are gathered between:\n#   ##   start: now() - delay - window\n#   ##   end:   now() - delay\n#   #\n#   ## Collection delay; if set too low metrics may not yet be available.\n#   # delay = \"5m\"\n#   #\n#   ## If unset, the window will start at 1m and be updated dynamically to span\n#   ## the time between calls (approximately the length of the plugin interval).\n#   # window = \"1m\"\n#\n#   ## TTL for cached list of metric types.  This is the maximum amount of time\n#   ## it may take to discover new metrics.\n#   # cache_ttl = \"1h\"\n#\n#   ## If true, raw bucket counts are collected for distribution value types.\n#   ## For a more lightweight collection, you may wish to disable and use\n#   ## distribution_aggregation_aligners instead.\n#   # gather_raw_distribution_buckets = true\n#\n#   ## Aggregate functions to be used for metrics whose value type is\n#   ## distribution.  These aggregate values are recorded in in addition to raw\n#   ## bucket counts; if they are enabled.\n#   ##\n#   ## For a list of aligner strings see:\n#   ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n#   # distribution_aggregation_aligners = [\n#   # \t\"ALIGN_PERCENTILE_99\",\n#   # \t\"ALIGN_PERCENTILE_95\",\n#   # \t\"ALIGN_PERCENTILE_50\",\n#   # ]\n#\n#   ## Filters can be added to reduce the number of time series matched.  All\n#   ## functions are supported: starts_with, ends_with, has_substring, and\n#   ## one_of.  Only the '=' operator is supported.\n#   ##\n#   ## The logical operators when combining filters are defined statically using\n#   ## the following values:\n#   ##   filter ::= \u003cresource_labels\u003e {AND \u003cmetric_labels\u003e}\n#   ##   resource_labels ::= \u003cresource_labels\u003e {OR \u003cresource_label\u003e}\n#   ##   metric_labels ::= \u003cmetric_labels\u003e {OR \u003cmetric_label\u003e}\n#   ##\n#   ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n#   #\n#   ## Resource labels refine the time series selection with the following expression:\n#   ##   resource.labels.\u003ckey\u003e = \u003cvalue\u003e\n#   # [[inputs.stackdriver.filter.resource_labels]]\n#   #   key = \"instance_name\"\n#   #   value = 'starts_with(\"localhost\")'\n#   #\n#   ## Metric labels refine the time series selection with the following expression:\n#   ##   metric.labels.\u003ckey\u003e = \u003cvalue\u003e\n#   #  [[inputs.stackdriver.filter.metric_labels]]\n#   #  \t key = \"device_name\"\n#   #  \t value = 'one_of(\"sda\", \"sdb\")'\n\n\n# # Get synproxy counter statistics from procfs\n# [[inputs.synproxy]]\n#   # no configuration\n\n\n# # Sysstat metrics collector\n# [[inputs.sysstat]]\n#   ## Path to the sadc command.\n#   #\n#   ## Common Defaults:\n#   ##   Debian/Ubuntu: /usr/lib/sysstat/sadc\n#   ##   Arch:          /usr/lib/sa/sadc\n#   ##   RHEL/CentOS:   /usr/lib64/sa/sadc\n#   sadc_path = \"/usr/lib/sa/sadc\" # required\n#\n#   ## Path to the sadf command, if it is not in PATH\n#   # sadf_path = \"/usr/bin/sadf\"\n#\n#   ## Activities is a list of activities, that are passed as argument to the\n#   ## sadc collector utility (e.g: DISK, SNMP etc...)\n#   ## The more activities that are added, the more data is collected.\n#   # activities = [\"DISK\"]\n#\n#   ## Group metrics to measurements.\n#   ##\n#   ## If group is false each metric will be prefixed with a description\n#   ## and represents itself a measurement.\n#   ##\n#   ## If Group is true, corresponding metrics are grouped to a single measurement.\n#   # group = true\n#\n#   ## Options for the sadf command. The values on the left represent the sadf\n#   ## options and the values on the right their description (which are used for\n#   ## grouping and prefixing metrics).\n#   ##\n#   ## Run 'sar -h' or 'man sar' to find out the supported options for your\n#   ## sysstat version.\n#   [inputs.sysstat.options]\n#     -C = \"cpu\"\n#     -B = \"paging\"\n#     -b = \"io\"\n#     -d = \"disk\"             # requires DISK activity\n#     \"-n ALL\" = \"network\"\n#     \"-P ALL\" = \"per_cpu\"\n#     -q = \"queue\"\n#     -R = \"mem\"\n#     -r = \"mem_util\"\n#     -S = \"swap_util\"\n#     -u = \"cpu_util\"\n#     -v = \"inode\"\n#     -W = \"swap\"\n#     -w = \"task\"\n#   #  -H = \"hugepages\"        # only available for newer linux distributions\n#   #  \"-I ALL\" = \"interrupts\" # requires INT activity\n#\n#   ## Device tags can be used to add additional tags for devices.\n#   ## For example the configuration below adds a tag vg with value rootvg for\n#   ## all metrics with sda devices.\n#   # [[inputs.sysstat.device_tags.sda]]\n#   #  vg = \"rootvg\"\n\n\n# # Gather systemd units state\n# [[inputs.systemd_units]]\n#   ## Set timeout for systemctl execution\n#   # timeout = \"1s\"\n#   #\n#   ## Filter for a specific unit type, default is \"service\", other possible\n#   ## values are \"socket\", \"target\", \"device\", \"mount\", \"automount\", \"swap\",\n#   ## \"timer\", \"path\", \"slice\" and \"scope \":\n#   # unittype = \"service\"\n#   #\n#   ## Filter for a specific pattern, default is \"\" (i.e. all), other possible\n#   ## values are valid pattern for systemctl, e.g. \"a*\" for all units with\n#   ## names starting with \"a\"\n#   # pattern = \"\"\n#   ## pattern = \"telegraf* influxdb*\"\n#   ## pattern = \"a*\"\n\n\n# # Reads metrics from a Teamspeak 3 Server via ServerQuery\n# [[inputs.teamspeak]]\n#   ## Server address for Teamspeak 3 ServerQuery\n#   # server = \"127.0.0.1:10011\"\n#   ## Username for ServerQuery\n#   username = \"serverqueryuser\"\n#   ## Password for ServerQuery\n#   password = \"secret\"\n#   ## Array of virtual servers\n#   # virtual_servers = [1]\n\n\n# # Read metrics about temperature\n# [[inputs.temp]]\n#   # no configuration\n\n\n# # Read Tengine's basic status information (ngx_http_reqstat_module)\n# [[inputs.tengine]]\n#   # An array of Tengine reqstat module URI to gather stats.\n#   urls = [\"http://127.0.0.1/us\"]\n#\n#   # HTTP response timeout (default: 5s)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.cer\"\n#   # tls_key = \"/etc/telegraf/key.key\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Gather metrics from the Tomcat server status page.\n# [[inputs.tomcat]]\n#   ## URL of the Tomcat server status\n#   # url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n#\n#   ## HTTP Basic Auth Credentials\n#   # username = \"tomcat\"\n#   # password = \"s3cret\"\n#\n#   ## Request timeout\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Inserts sine and cosine waves for demonstration purposes\n# [[inputs.trig]]\n#   ## Set the amplitude\n#   amplitude = 10.0\n\n\n# # Read Twemproxy stats data\n# [[inputs.twemproxy]]\n#   ## Twemproxy stats address and port (no scheme)\n#   addr = \"localhost:22222\"\n#   ## Monitor pool name\n#   pools = [\"redis_pool\", \"mc_pool\"]\n\n\n# # A plugin to collect stats from the Unbound DNS resolver\n# [[inputs.unbound]]\n#   ## Address of server to connect to, read from unbound conf default, optionally ':port'\n#   ## Will lookup IP if given a hostname\n#   server = \"127.0.0.1:8953\"\n#\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   # use_sudo = false\n#\n#   ## The default location of the unbound-control binary can be overridden with:\n#   # binary = \"/usr/sbin/unbound-control\"\n#\n#   ## The default location of the unbound config file can be overridden with:\n#   # config_file = \"/etc/unbound/unbound.conf\"\n#\n#   ## The default timeout of 1s can be overridden with:\n#   # timeout = \"1s\"\n#\n#   ## When set to true, thread metrics are tagged with the thread id.\n#   ##\n#   ## The default is false for backwards compatibility, and will be changed to\n#   ## true in a future version.  It is recommended to set to true on new\n#   ## deployments.\n#   thread_as_tag = false\n\n\n# # Read uWSGI metrics.\n# [[inputs.uwsgi]]\n#   ## List with urls of uWSGI Stats servers. URL must match pattern:\n#   ## scheme://address[:port]\n#   ##\n#   ## For example:\n#   ## servers = [\"tcp://localhost:5050\", \"http://localhost:1717\", \"unix:///tmp/statsock\"]\n#   servers = [\"tcp://127.0.0.1:1717\"]\n#\n#   ## General connection timeout\n#   # timeout = \"5s\"\n\n\n# # A plugin to collect stats from Varnish HTTP Cache\n# [[inputs.varnish]]\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   #use_sudo = false\n#\n#   ## The default location of the varnishstat binary can be overridden with:\n#   binary = \"/usr/bin/varnishstat\"\n#\n#   ## Additional custom arguments for the varnishstat command\n#   # binary_args = [\"-f\", \"MAIN.*\"]\n#\n#   ## The default location of the varnishadm binary can be overriden with:\n#   adm_binary = \"/usr/bin/varnishadm\"\n#\n#   ## Custom arguments for the varnishadm command\n#   # adm_binary_args = [\"\"]\n#\n#   ## Metric version defaults to metric_version=1, use metric_version=2 for removal of nonactive vcls.\n#   metric_version = 1\n#\n#   ## Additional regexps to override builtin conversion of varnish metrics into telegraf metrics.\n#   ## Regexp group \"_vcl\" is used for extracting the VCL name. Metrics that contains nonactive VCL's are skipped.\n#   ## Regexp group \"_field\" overrides field name. Other named regexp groups are used as tags.\n#   # regexps = ['XCNT\\.(?P\u003c_vcl\u003e[\\w\\-]*)\\.(?P\u003cgroup\u003e[\\w\\-.+]*)\\.(?P\u003c_field\u003e[\\w\\-.+]*)\\.val']\n#\n#   ## By default, telegraf gather stats for 3 metric points.\n#   ## Setting stats will override the defaults shown below.\n#   ## Glob matching can be used, ie, stats = [\"MAIN.*\"]\n#   ## stats may also be set to [\"*\"], which will collect all stats\n#   stats = [\"MAIN.cache_hit\", \"MAIN.cache_miss\", \"MAIN.uptime\"]\n#\n#   ## Optional name for the varnish instance (or working directory) to query\n#   ## Usually append after -n in varnish cli\n#   # instance_name = instanceName\n#\n#   ## Timeout for varnishstat command\n#   # timeout = \"1s\"\n\n\n# # Read metrics from the Vault API\n# [[inputs.vault]]\n#   ## URL for the Vault agent\n#   # url = \"http://127.0.0.1:8200\"\n#\n#   ## Use Vault token for authorization.\n#   ## Vault token configuration is mandatory.\n#   ## If both are empty or both are set, an error is thrown.\n#   # token_file = \"/path/to/auth/token\"\n#   ## OR\n#   token = \"s.CDDrgg5zPv5ssI0Z2P4qxJj2\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n\n\n# # Collect Wireguard server interface and peer statistics\n# [[inputs.wireguard]]\n#   ## Optional list of Wireguard device/interface names to query.\n#   ## If omitted, all Wireguard interfaces are queried.\n#   # devices = [\"wg0\"]\n\n\n# # Monitor wifi signal strength and quality\n# [[inputs.wireless]]\n#   ## Sets 'proc' directory path\n#   ## If not specified, then default is /proc\n#   # host_proc = \"/proc\"\n\n\n# # Reads metrics from a SSL certificate\n# [[inputs.x509_cert]]\n#   ## List certificate sources\n#   ## Prefix your entry with 'file://' if you intend to use relative paths\n#   sources = [\"tcp://example.org:443\", \"https://influxdata.com:443\",\n#             \"udp://127.0.0.1:4433\", \"/etc/ssl/certs/ssl-cert-snakeoil.pem\",\n#             \"/etc/mycerts/*.mydomain.org.pem\", \"file:///path/to/*.pem\"]\n#\n#   ## Timeout for SSL connection\n#   # timeout = \"5s\"\n#\n#   ## Pass a different name into the TLS request (Server Name Indication)\n#   ##   example: server_name = \"myhost.example.org\"\n#   # server_name = \"\"\n#\n#   ## Don't include root or intermediate certificates in output\n#   # exclude_root_certs = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n\n\n# # Gathers Metrics From a Dell EMC XtremIO Storage Array's V3 API\n# [[inputs.xtremio]]\n#   ## XtremIO User Interface Endpoint\n#   url = \"https://xtremio.example.com/\" # required\n#\n#   ## Credentials\n#   username = \"user1\"\n#   password = \"pass123\"\n#\n#   ## Metrics to collect from the XtremIO\n#   # collectors = [\"bbus\",\"clusters\",\"ssds\",\"volumes\",\"xms\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use SSL but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, pools and datasets\n# [[inputs.zfs]]\n#   ## ZFS kstat path. Ignored on FreeBSD\n#   ## If not specified, then default is:\n#   # kstatPath = \"/proc/spl/kstat/zfs\"\n#\n#   ## By default, telegraf gather all zfs stats\n#   ## If not specified, then default is:\n#   # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n#   ## For Linux, the default is:\n#   # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n#   #   \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n#   ## By default, don't gather zpool stats\n#   # poolMetrics = false\n#   ## By default, don't gather zdataset stats\n#   # datasetMetrics = false\n\n\n# # Reads 'mntr' stats from one or many zookeeper servers\n# [[inputs.zookeeper]]\n#   ## An array of address to gather stats about. Specify an ip or hostname\n#   ## with port. ie localhost:2181, 10.0.0.1:2181, etc.\n#\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no port is specified, 2181 is used\n#   servers = [\":2181\"]\n#\n#   ## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # enable_tls = true\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n###############################################################################\n#                            SERVICE INPUT PLUGINS                            #\n###############################################################################\n\n\n# # Pull Metric Statistics from Aliyun CMS\n# [[inputs.aliyuncms]]\n#   ## Aliyun Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Ram RoleArn credential\n#   ## 2) AccessKey STS token credential\n#   ## 3) AccessKey credential\n#   ## 4) Ecs Ram Role credential\n#   ## 5) RSA keypair credential\n#   ## 6) Environment variables credential\n#   ## 7) Instance metadata credential\n#\n#   # access_key_id = \"\"\n#   # access_key_secret = \"\"\n#   # access_key_sts_token = \"\"\n#   # role_arn = \"\"\n#   # role_session_name = \"\"\n#   # private_key = \"\"\n#   # public_key_id = \"\"\n#   # role_name = \"\"\n#\n#   ## Specify the ali cloud region list to be queried for metrics and objects discovery\n#   ## If not set, all supported regions (see below) would be covered, it can provide a significant load on API, so the recommendation here\n#   ## is to limit the list as much as possible. Allowed values: https://www.alibabacloud.com/help/zh/doc-detail/40654.htm\n#   ## Default supported regions are:\n#   ## 21 items: cn-qingdao,cn-beijing,cn-zhangjiakou,cn-huhehaote,cn-hangzhou,cn-shanghai,cn-shenzhen,\n#   ##           cn-heyuan,cn-chengdu,cn-hongkong,ap-southeast-1,ap-southeast-2,ap-southeast-3,ap-southeast-5,\n#   ##           ap-south-1,ap-northeast-1,us-west-1,us-east-1,eu-central-1,eu-west-1,me-east-1\n#   ##\n#   ## From discovery perspective it set the scope for object discovery, the discovered info can be used to enrich\n#   ## the metrics with objects attributes/tags. Discovery is supported not for all projects (if not supported, then\n#   ## it will be reported on the start - for example for 'acs_cdn' project:\n#   ## 'E! [inputs.aliyuncms] Discovery tool is not activated: no discovery support for project \"acs_cdn\"' )\n#   ## Currently, discovery supported for the following projects:\n#   ## - acs_ecs_dashboard\n#   ## - acs_rds_dashboard\n#   ## - acs_slb_dashboard\n#   ## - acs_vpc_eip\n#   regions = [\"cn-hongkong\"]\n#\n#   # The minimum period for AliyunCMS metrics is 1 minute (60s). However not all\n#   # metrics are made available to the 1 minute period. Some are collected at\n#   # 3 minute, 5 minute, or larger intervals.\n#   # See: https://help.aliyun.com/document_detail/51936.html?spm=a2c4g.11186623.2.18.2bc1750eeOw1Pv\n#   # Note that if a period is configured that is smaller than the minimum for a\n#   # particular metric, that metric will not be returned by the Aliyun OpenAPI\n#   # and will not be collected by Telegraf.\n#   #\n#   ## Requested AliyunCMS aggregation Period (required - must be a multiple of 60s)\n#   period = \"5m\"\n#\n#   ## Collection Delay (required - must account for metrics availability via AliyunCMS API)\n#   delay = \"1m\"\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n#   ## gaps or overlap in pulled data\n#   interval = \"5m\"\n#\n#   ## Metric Statistic Project (required)\n#   project = \"acs_slb_dashboard\"\n#\n#   ## Maximum requests per second, default value is 200\n#   ratelimit = 200\n#\n#   ## How often the discovery API call executed (default 1m)\n#   #discovery_interval = \"1m\"\n#\n#   ## Metrics to Pull (Required)\n#   [[inputs.aliyuncms.metrics]]\n#   ## Metrics names to be requested,\n#   ## described here (per project): https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n#   names = [\"InstanceActiveConnection\", \"InstanceNewConnection\"]\n#\n#   ## Dimension filters for Metric (these are optional).\n#   ## This allows to get additional metric dimension. If dimension is not specified it can be returned or\n#   ## the data can be aggregated - it depends on particular metric, you can find details here: https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n#   ##\n#   ## Note, that by default dimension filter includes the list of discovered objects in scope (if discovery is enabled)\n#   ## Values specified here would be added into the list of discovered objects.\n#   ## You can specify either single dimension:\n#   #dimensions = '{\"instanceId\": \"p-example\"}'\n#\n#   ## Or you can specify several dimensions at once:\n#   #dimensions = '[{\"instanceId\": \"p-example\"},{\"instanceId\": \"q-example\"}]'\n#\n#   ## Enrichment tags, can be added from discovery (if supported)\n#   ## Notation is \u003cmeasurement_tag_name\u003e:\u003cJMES query path (https://jmespath.org/tutorial.html)\u003e\n#   ## To figure out which fields are available, consult the Describe\u003cObjectType\u003e API per project.\n#   ## For example, for SLB: https://api.aliyun.com/#/?product=Slb\u0026version=2014-05-15\u0026api=DescribeLoadBalancers\u0026params={}\u0026tab=MOCK\u0026lang=GO\n#   #tag_query_path = [\n#   #    \"address:Address\",\n#   #    \"name:LoadBalancerName\",\n#   #    \"cluster_owner:Tags.Tag[?TagKey=='cs.cluster.name'].TagValue | [0]\"\n#   #    ]\n#   ## The following tags added by default: regionId (if discovery enabled), userId, instanceId.\n#\n#   ## Allow metrics without discovery data, if discovery is enabled. If set to true, then metric without discovery\n#   ## data would be emitted, otherwise dropped. This cane be of help, in case debugging dimension filters, or partial coverage\n#   ## of discovery scope vs monitoring scope\n#   #allow_dps_without_discovery = false\n\n\n# # AMQP consumer plugin\n# [[inputs.amqp_consumer]]\n#   ## Brokers to consume from.  If multiple brokers are specified a random broker\n#   ## will be selected anytime a connection is established.  This can be\n#   ## helpful for load balancing when not using a dedicated load balancer.\n#   brokers = [\"amqp://localhost:5672/influxdb\"]\n#\n#   ## Authentication credentials for the PLAIN auth_method.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Name of the exchange to declare.  If unset, no exchange will be declared.\n#   exchange = \"telegraf\"\n#\n#   ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n#   # exchange_type = \"topic\"\n#\n#   ## If true, exchange will be passively declared.\n#   # exchange_passive = false\n#\n#   ## Exchange durability can be either \"transient\" or \"durable\".\n#   # exchange_durability = \"durable\"\n#\n#   ## Additional exchange arguments.\n#   # exchange_arguments = { }\n#   # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n#\n#   ## AMQP queue name.\n#   queue = \"telegraf\"\n#\n#   ## AMQP queue durability can be \"transient\" or \"durable\".\n#   queue_durability = \"durable\"\n#\n#   ## If true, queue will be passively declared.\n#   # queue_passive = false\n#\n#   ## A binding between the exchange and queue using this binding key is\n#   ## created.  If unset, no binding is created.\n#   binding_key = \"#\"\n#\n#   ## Maximum number of messages server should give to the worker.\n#   # prefetch_count = 50\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Auth method. PLAIN and EXTERNAL are supported\n#   ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n#   ## described here: https://www.rabbitmq.com/plugins.html\n#   # auth_method = \"PLAIN\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Content encoding for message payloads, can be set to \"gzip\" to or\n#   ## \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read Cassandra metrics through Jolokia\n# [[inputs.cassandra]]\n#   ## DEPRECATED: The 'cassandra' plugin is deprecated in version 1.7.0, use 'inputs.jolokia2' with the 'cassandra.conf' example configuration instead.\n#   ## DEPRECATED: The cassandra plugin has been deprecated.  Please use the\n#   ## jolokia2 plugin instead.\n#   ##\n#   ## see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n#\n#   context = \"/jolokia/read\"\n#   ## List of cassandra servers exposing jolokia read service\n#   servers = [\"myuser:mypassword@10.10.10.1:8778\",\"10.10.10.2:8778\",\":8778\"]\n#   ## List of metrics collected on above servers\n#   ## Each metric consists of a jmx path.\n#   ## This will collect all heap memory usage metrics from the jvm and\n#   ## ReadLatency metrics for all keyspaces and tables.\n#   ## \"type=Table\" in the query works with Cassandra3.0. Older versions might\n#   ## need to use \"type=ColumnFamily\"\n#   metrics  = [\n#     \"/java.lang:type=Memory/HeapMemoryUsage\",\n#     \"/org.apache.cassandra.metrics:type=Table,keyspace=*,scope=*,name=ReadLatency\"\n#   ]\n\n\n# # Cisco model-driven telemetry (MDT) input plugin for IOS XR, IOS XE and NX-OS platforms\n# [[inputs.cisco_telemetry_mdt]]\n#  ## Telemetry transport can be \"tcp\" or \"grpc\".  TLS is only supported when\n#  ## using the grpc transport.\n#  transport = \"grpc\"\n#\n#  ## Address and port to host telemetry listener\n#  service_address = \":57000\"\n#\n#  ## Enable TLS; grpc transport only.\n#  # tls_cert = \"/etc/telegraf/cert.pem\"\n#  # tls_key = \"/etc/telegraf/key.pem\"\n#\n#  ## Enable TLS client authentication and define allowed CA certificates; grpc\n#  ##  transport only.\n#  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#  ## Define (for certain nested telemetry measurements with embedded tags) which fields are tags\n#  # embedded_tags = [\"Cisco-IOS-XR-qos-ma-oper:qos/interface-table/interface/input/service-policy-names/service-policy-instance/statistics/class-stats/class-name\"]\n#\n#  ## Define aliases to map telemetry encoding paths to simple measurement names\n#  [inputs.cisco_telemetry_mdt.aliases]\n#    ifstats = \"ietf-interfaces:interfaces-state/interface/statistics\"\n#  ##Define Property Xformation, please refer README and https://pubhub.devnetcloud.com/media/dme-docs-9-3-3/docs/appendix/ for Model details.\n#  [inputs.cisco_telemetry_mdt.dmes]\n#    ModTs = \"ignore\"\n#    CreateTs = \"ignore\"\n\n\n# # Read metrics from one or many ClickHouse servers\n# [[inputs.clickhouse]]\n#   ## Username for authorization on ClickHouse server\n#   ## example: username = \"default\"\n#   username = \"default\"\n#\n#   ## Password for authorization on ClickHouse server\n#   ## example: password = \"super_secret\"\n#\n#   ## HTTP(s) timeout while getting metrics values\n#   ## The timeout includes connection time, any redirects, and reading the response body.\n#   ##   example: timeout = 1s\n#   # timeout = 5s\n#\n#   ## List of servers for metrics scraping\n#   ## metrics scrape via HTTP(s) clickhouse interface\n#   ## https://clickhouse.tech/docs/en/interfaces/http/\n#   ##    example: servers = [\"http://127.0.0.1:8123\",\"https://custom-server.mdb.yandexcloud.net\"]\n#   servers         = [\"http://127.0.0.1:8123\"]\n#\n#   ## If \"auto_discovery\"\" is \"true\" plugin tries to connect to all servers available in the cluster\n#   ## with using same \"user:password\" described in \"user\" and \"password\" parameters\n#   ## and get this server hostname list from \"system.clusters\" table\n#   ## see\n#   ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters\n#   ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers\n#   ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n#   ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables\n#   ##    example: auto_discovery = false\n#   # auto_discovery = true\n#\n#   ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n#   ## when this filter present then \"WHERE cluster IN (...)\" filter will apply\n#   ## please use only full cluster names here, regexp and glob filters is not allowed\n#   ## for \"/etc/clickhouse-server/config.d/remote.xml\"\n#   ## \u003cyandex\u003e\n#   ##  \u003cremote_servers\u003e\n#   ##    \u003cmy-own-cluster\u003e\n#   ##        \u003cshard\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-ru-1.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-ru-2.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##        \u003c/shard\u003e\n#   ##        \u003cshard\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-eu-1.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-eu-2.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##        \u003c/shard\u003e\n#   ##    \u003c/my-onw-cluster\u003e\n#   ##  \u003c/remote_servers\u003e\n#   ##\n#   ## \u003c/yandex\u003e\n#   ##\n#   ## example: cluster_include = [\"my-own-cluster\"]\n#   # cluster_include = []\n#\n#   ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n#   ## when this filter present then \"WHERE cluster NOT IN (...)\" filter will apply\n#   ##    example: cluster_exclude = [\"my-internal-not-discovered-cluster\"]\n#   # cluster_exclude = []\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from Google PubSub\n# [[inputs.cloud_pubsub]]\n#   ## Required. Name of Google Cloud Platform (GCP) Project that owns\n#   ## the given PubSub subscription.\n#   project = \"my-project\"\n#\n#   ## Required. Name of PubSub subscription to ingest metrics from.\n#   subscription = \"my-subscription\"\n#\n#   ## Required. Data format to consume.\n#   ## Each data format has its own unique set of configuration options.\n#   ## Read more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n#   ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n#   ## Application Default Credentials, which is preferred.\n#   # credentials_file = \"path/to/my/creds.json\"\n#\n#   ## Optional. Number of seconds to wait before attempting to restart the\n#   ## PubSub subscription receiver after an unexpected error.\n#   ## If the streaming pull for a PubSub Subscription fails (receiver),\n#   ## the agent attempts to restart receiving messages after this many seconds.\n#   # retry_delay_seconds = 5\n#\n#   ## Optional. Maximum byte length of a message to consume.\n#   ## Larger messages are dropped with an error. If less than 0 or unspecified,\n#   ## treated as no limit.\n#   # max_message_len = 1000000\n#\n#   ## Optional. Maximum messages to read from PubSub that have not been written\n#   ## to an output. Defaults to 1000.\n#   ## For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message contains 10 metrics and the output\n#   ## metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## The following are optional Subscription ReceiveSettings in PubSub.\n#   ## Read more about these values:\n#   ## https://godoc.org/cloud.google.com/go/pubsub#ReceiveSettings\n#\n#   ## Optional. Maximum number of seconds for which a PubSub subscription\n#   ## should auto-extend the PubSub ACK deadline for each message. If less than\n#   ## 0, auto-extension is disabled.\n#   # max_extension = 0\n#\n#   ## Optional. Maximum number of unprocessed messages in PubSub\n#   ## (unacknowledged but not yet expired in PubSub).\n#   ## A value of 0 is treated as the default PubSub value.\n#   ## Negative values will be treated as unlimited.\n#   # max_outstanding_messages = 0\n#\n#   ## Optional. Maximum size in bytes of unprocessed messages in PubSub\n#   ## (unacknowledged but not yet expired in PubSub).\n#   ## A value of 0 is treated as the default PubSub value.\n#   ## Negative values will be treated as unlimited.\n#   # max_outstanding_bytes = 0\n#\n#   ## Optional. Max number of goroutines a PubSub Subscription receiver can spawn\n#   ## to pull messages from PubSub concurrently. This limit applies to each\n#   ## subscription separately and is treated as the PubSub default if less than\n#   ## 1. Note this setting does not limit the number of messages that can be\n#   ## processed concurrently (use \"max_outstanding_messages\" instead).\n#   # max_receiver_go_routines = 0\n#\n#   ## Optional. If true, Telegraf will attempt to base64 decode the\n#   ## PubSub message data before parsing\n#   # base64_data = false\n\n\n# # Google Cloud Pub/Sub Push HTTP listener\n# [[inputs.cloud_pubsub_push]]\n#   ## Address and port to host HTTP listener on\n#   service_address = \":8080\"\n#\n#   ## Application secret to verify messages originate from Cloud Pub/Sub\n#   # token = \"\"\n#\n#   ## Path to listen to.\n#   # path = \"/\"\n#\n#   ## Maximum duration before timing out read of the request\n#   # read_timeout = \"10s\"\n#   ## Maximum duration before timing out write of the response. This should be set to a value\n#   ## large enough that you can send at least 'metric_batch_size' number of messages within the\n#   ## duration.\n#   # write_timeout = \"10s\"\n#\n#   ## Maximum allowed http request body size in bytes.\n#   ## 0 means to use the default of 524,288,00 bytes (500 mebibytes)\n#   # max_body_size = \"500MB\"\n#\n#   ## Whether to add the pubsub metadata, such as message attributes and subscription as a tag.\n#   # add_meta = false\n#\n#   ## Optional. Maximum messages to read from PubSub that have not been written\n#   ## to an output. Defaults to 1000.\n#   ## For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message contains 10 metrics and the output\n#   ## metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Ingests files in a directory and then moves them to a target directory.\n# [[inputs.directory_monitor]]\n#   ## The directory to monitor and read files from.\n#   directory = \"\"\n#   #\n#   ## The directory to move finished files to.\n#   finished_directory = \"\"\n#   #\n#   ## The directory to move files to upon file error.\n#   ## If not provided, erroring files will stay in the monitored directory.\n#   # error_directory = \"\"\n#   #\n#   ## The amount of time a file is allowed to sit in the directory before it is picked up.\n#   ## This time can generally be low but if you choose to have a very large file written to the directory and it's potentially slow,\n#   ## set this higher so that the plugin will wait until the file is fully copied to the directory.\n#   # directory_duration_threshold = \"50ms\"\n#   #\n#   ## A list of the only file names to monitor, if necessary. Supports regex. If left blank, all files are ingested.\n#   # files_to_monitor = [\"^.*\\.csv\"]\n#   #\n#   ## A list of files to ignore, if necessary. Supports regex.\n#   # files_to_ignore = [\".DS_Store\"]\n#   #\n#   ## Maximum lines of the file to process that have not yet be written by the\n#   ## output. For best throughput set to the size of the output's metric_buffer_limit.\n#   ## Warning: setting this number higher than the output's metric_buffer_limit can cause dropped metrics.\n#   # max_buffered_metrics = 10000\n#   #\n#   ## The maximum amount of file paths to queue up for processing at once, before waiting until files are processed to find more files.\n#   ## Lowering this value will result in *slightly* less memory use, with a potential sacrifice in speed efficiency, if absolutely necessary.\n#   #\tfile_queue_size = 100000\n#   #\n#   ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n#   ## to disable. Cautious when file name variation is high, this can increase the cardinality\n#   ## significantly. Read more about cardinality here:\n#   ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n#   # file_tag = \"\"\n#   #\n#   ## The dataformat to be read from the files.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   ## NOTE: We currently only support parsing newline-delimited JSON. See the format here: https://github.com/ndjson/ndjson-spec\n#   data_format = \"influx\"\n\n\n# # Read logging output from the Docker engine\n# [[inputs.docker_log]]\n#   ## Docker Endpoint\n#   ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n#   ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n#   # endpoint = \"unix:///var/run/docker.sock\"\n#\n#   ## When true, container logs are read from the beginning; otherwise\n#   ## reading begins at the end of the log.\n#   # from_beginning = false\n#\n#   ## Timeout for Docker API calls.\n#   # timeout = \"5s\"\n#\n#   ## Containers to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all containers\n#   # container_name_include = []\n#   # container_name_exclude = []\n#\n#   ## Container states to include and exclude. Globs accepted.\n#   ## When empty only containers in the \"running\" state will be captured.\n#   # container_state_include = []\n#   # container_state_exclude = []\n#\n#   ## docker labels to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all labels as tags\n#   # docker_label_include = []\n#   # docker_label_exclude = []\n#\n#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n#   source_tag = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Azure Event Hubs service input plugin\n# [[inputs.eventhub_consumer]]\n#   ## The default behavior is to create a new Event Hub client from environment variables.\n#   ## This requires one of the following sets of environment variables to be set:\n#   ##\n#   ## 1) Expected Environment Variables:\n#   ##    - \"EVENTHUB_CONNECTION_STRING\"\n#   ##\n#   ## 2) Expected Environment Variables:\n#   ##    - \"EVENTHUB_NAMESPACE\"\n#   ##    - \"EVENTHUB_NAME\"\n#   ##    - \"EVENTHUB_KEY_NAME\"\n#   ##    - \"EVENTHUB_KEY_VALUE\"\n#\n#   ## 3) Expected Environment Variables:\n#   ##    - \"EVENTHUB_NAMESPACE\"\n#   ##    - \"EVENTHUB_NAME\"\n#   ##    - \"AZURE_TENANT_ID\"\n#   ##    - \"AZURE_CLIENT_ID\"\n#   ##    - \"AZURE_CLIENT_SECRET\"\n#\n#   ## Uncommenting the option below will create an Event Hub client based solely on the connection string.\n#   ## This can either be the associated environment variable or hard coded directly.\n#   ## If this option is uncommented, environment variables will be ignored.\n#   ## Connection string should contain EventHubName (EntityPath)\n#   # connection_string = \"\"\n#\n#   ## Set persistence directory to a valid folder to use a file persister instead of an in-memory persister\n#   # persistence_dir = \"\"\n#\n#   ## Change the default consumer group\n#   # consumer_group = \"\"\n#\n#   ## By default the event hub receives all messages present on the broker, alternative modes can be set below.\n#   ## The timestamp should be in https://github.com/toml-lang/toml#offset-date-time format (RFC 3339).\n#   ## The 3 options below only apply if no valid persister is read from memory or file (e.g. first run).\n#   # from_timestamp =\n#   # latest = true\n#\n#   ## Set a custom prefetch count for the receiver(s)\n#   # prefetch_count = 1000\n#\n#   ## Add an epoch to the receiver(s)\n#   # epoch = 0\n#\n#   ## Change to set a custom user agent, \"telegraf\" is used by default\n#   # user_agent = \"telegraf\"\n#\n#   ## To consume from a specific partition, set the partition_ids option.\n#   ## An empty array will result in receiving from all partitions.\n#   # partition_ids = [\"0\",\"1\"]\n#\n#   ## Max undelivered messages\n#   # max_undelivered_messages = 1000\n#\n#   ## Set either option below to true to use a system property as timestamp.\n#   ## You have the choice between EnqueuedTime and IoTHubEnqueuedTime.\n#   ## It is recommended to use this setting when the data itself has no timestamp.\n#   # enqueued_time_as_ts = true\n#   # iot_hub_enqueued_time_as_ts = true\n#\n#   ## Tags or fields to create from keys present in the application property bag.\n#   ## These could for example be set by message enrichments in Azure IoT Hub.\n#   # application_property_tags = []\n#   # application_property_fields = []\n#\n#   ## Tag or field name to use for metadata\n#   ## By default all metadata is disabled\n#   # sequence_number_field = \"SequenceNumber\"\n#   # enqueued_time_field = \"EnqueuedTime\"\n#   # offset_field = \"Offset\"\n#   # partition_id_tag = \"PartitionID\"\n#   # partition_key_tag = \"PartitionKey\"\n#   # iot_hub_device_connection_id_tag = \"IoTHubDeviceConnectionID\"\n#   # iot_hub_auth_generation_id_tag = \"IoTHubAuthGenerationID\"\n#   # iot_hub_connection_auth_method_tag = \"IoTHubConnectionAuthMethod\"\n#   # iot_hub_connection_module_id_tag = \"IoTHubConnectionModuleID\"\n#   # iot_hub_enqueued_time_field = \"IoTHubEnqueuedTime\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Run executable as long-running input plugin\n# [[inputs.execd]]\n#   ## Program to run as daemon\n#   command = [\"telegraf-smartctl\", \"-d\", \"/dev/sda\"]\n#\n#   ## Define how the process is signaled on each collection interval.\n#   ## Valid values are:\n#   ##   \"none\"   : Do not signal anything.\n#   ##              The process must output metrics by itself.\n#   ##   \"STDIN\"   : Send a newline on STDIN.\n#   ##   \"SIGHUP\"  : Send a HUP signal. Not available on Windows.\n#   ##   \"SIGUSR1\" : Send a USR1 signal. Not available on Windows.\n#   ##   \"SIGUSR2\" : Send a USR2 signal. Not available on Windows.\n#   signal = \"none\"\n#\n#   ## Delay before the process is restarted after an unexpected termination\n#   restart_delay = \"10s\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # gNMI telemetry input plugin\n# [[inputs.gnmi]]\n#  ## Address and port of the gNMI GRPC server\n#  addresses = [\"10.49.234.114:57777\"]\n#\n#  ## define credentials\n#  username = \"cisco\"\n#  password = \"cisco\"\n#\n#  ## gNMI encoding requested (one of: \"proto\", \"json\", \"json_ietf\", \"bytes\")\n#  # encoding = \"proto\"\n#\n#  ## redial in case of failures after\n#  redial = \"10s\"\n#\n#  ## enable client-side TLS and define CA to authenticate the device\n#  # enable_tls = true\n#  # tls_ca = \"/etc/telegraf/ca.pem\"\n#  # insecure_skip_verify = true\n#\n#  ## define client-side TLS certificate \u0026 key to authenticate to the device\n#  # tls_cert = \"/etc/telegraf/cert.pem\"\n#  # tls_key = \"/etc/telegraf/key.pem\"\n#\n#  ## gNMI subscription prefix (optional, can usually be left empty)\n#  ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n#  # origin = \"\"\n#  # prefix = \"\"\n#  # target = \"\"\n#\n#  ## Define additional aliases to map telemetry encoding paths to simple measurement names\n#  #[inputs.gnmi.aliases]\n#  #  ifcounters = \"openconfig:/interfaces/interface/state/counters\"\n#\n#  [[inputs.gnmi.subscription]]\n#   ## Name of the measurement that will be emitted\n#   name = \"ifcounters\"\n#\n#   ## Origin and path of the subscription\n#   ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n#   ##\n#   ## origin usually refers to a (YANG) data model implemented by the device\n#   ## and path to a specific substructure inside it that should be subscribed to (similar to an XPath)\n#   ## YANG models can be found e.g. here: https://github.com/YangModels/yang/tree/master/vendor/cisco/xr\n#   origin = \"openconfig-interfaces\"\n#   path = \"/interfaces/interface/state/counters\"\n#\n#   # Subscription mode (one of: \"target_defined\", \"sample\", \"on_change\") and interval\n#   subscription_mode = \"sample\"\n#   sample_interval = \"10s\"\n#\n#   ## Suppress redundant transmissions when measured values are unchanged\n#   # suppress_redundant = false\n#\n#   ## If suppression is enabled, send updates at least every X seconds anyway\n#   # heartbeat_interval = \"60s\"\n#\n#   #[[inputs.gnmi.subscription]]\n#     # name = \"descr\"\n#     # origin = \"openconfig-interfaces\"\n#     # path = \"/interfaces/interface/state/description\"\n#     # subscription_mode = \"on_change\"\n#\n#     ## If tag_only is set, the subscription in question will be utilized to maintain a map of\n#     ## tags to apply to other measurements emitted by the plugin, by matching path keys\n#     ## All fields from the tag-only subscription will be applied as tags to other readings,\n#     ## in the format \u003cname\u003e_\u003cfieldBase\u003e.\n#     # tag_only = true\n\n\n# # Accept metrics over InfluxDB 1.x HTTP API\n# [[inputs.http_listener]]\n#   ## DEPRECATED: The 'http_listener' plugin is deprecated in version 1.9.0, has been renamed to 'influxdb_listener', use 'inputs.influxdb_listener' or 'inputs.http_listener_v2' instead.\n#   ## Address and port to host InfluxDB listener on\n#   service_address = \":8186\"\n#\n#   ## maximum duration before timing out read of the request\n#   read_timeout = \"10s\"\n#   ## maximum duration before timing out write of the response\n#   write_timeout = \"10s\"\n#\n#   ## Maximum allowed HTTP request body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   max_body_size = \"32MiB\"\n#\n#   ## Optional tag name used to store the database.\n#   ## If the write has a database in the query string then it will be kept in this tag name.\n#   ## This tag can be used in downstream outputs.\n#   ## The default value of nothing means it will be off and the database will not be recorded.\n#   # database_tag = \"\"\n#\n#   ## If set the retention policy specified in the write query will be added as\n#   ## the value of this tag name.\n#   # retention_policy_tag = \"\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   tls_cert = \"/etc/telegraf/cert.pem\"\n#   tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional username and password to accept for HTTP basic authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # basic_username = \"foobar\"\n#   # basic_password = \"barfoo\"\n#\n#   ## Influx line protocol parser\n#   ## 'internal' is the default. 'upstream' is a newer parser that is faster\n#   ## and more memory efficient.\n#   # parser_type = \"internal\"\n\n\n# # Generic HTTP write listener\n# [[inputs.http_listener_v2]]\n#   ## Address and port to host HTTP listener on\n#   service_address = \":8080\"\n#\n#   ## Paths to listen to.\n#   # paths = [\"/telegraf\"]\n#\n#   ## Save path as http_listener_v2_path tag if set to true\n#   # path_tag = false\n#\n#   ## HTTP methods to accept.\n#   # methods = [\"POST\", \"PUT\"]\n#\n#   ## maximum duration before timing out read of the request\n#   # read_timeout = \"10s\"\n#   ## maximum duration before timing out write of the response\n#   # write_timeout = \"10s\"\n#\n#   ## Maximum allowed http request body size in bytes.\n#   ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n#   # max_body_size = \"500MB\"\n#\n#   ## Part of the request to consume.  Available options are \"body\" and\n#   ## \"query\".\n#   # data_source = \"body\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional username and password to accept for HTTP basic authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # basic_username = \"foobar\"\n#   # basic_password = \"barfoo\"\n#\n#   ## Optional setting to map http headers into tags\n#   ## If the http header is not present on the request, no corresponding tag will be added\n#   ## If multiple instances of the http header are present, only the first value will be used\n#   # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Accept metrics over InfluxDB 1.x HTTP API\n# [[inputs.influxdb_listener]]\n#   ## Address and port to host InfluxDB listener on\n#   service_address = \":8186\"\n#\n#   ## maximum duration before timing out read of the request\n#   read_timeout = \"10s\"\n#   ## maximum duration before timing out write of the response\n#   write_timeout = \"10s\"\n#\n#   ## Maximum allowed HTTP request body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   max_body_size = \"32MiB\"\n#\n#   ## Optional tag name used to store the database.\n#   ## If the write has a database in the query string then it will be kept in this tag name.\n#   ## This tag can be used in downstream outputs.\n#   ## The default value of nothing means it will be off and the database will not be recorded.\n#   # database_tag = \"\"\n#\n#   ## If set the retention policy specified in the write query will be added as\n#   ## the value of this tag name.\n#   # retention_policy_tag = \"\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   tls_cert = \"/etc/telegraf/cert.pem\"\n#   tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional username and password to accept for HTTP basic authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # basic_username = \"foobar\"\n#   # basic_password = \"barfoo\"\n#\n#   ## Influx line protocol parser\n#   ## 'internal' is the default. 'upstream' is a newer parser that is faster\n#   ## and more memory efficient.\n#   # parser_type = \"internal\"\n\n\n# # Accept metrics over InfluxDB 2.x HTTP API\n# [[inputs.influxdb_v2_listener]]\n#   ## Address and port to host InfluxDB listener on\n#   ## (Double check the port. Could be 9999 if using OSS Beta)\n#   service_address = \":8086\"\n#\n#   ## Maximum allowed HTTP request body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   # max_body_size = \"32MiB\"\n#\n#   ## Optional tag to determine the bucket.\n#   ## If the write has a bucket in the query string then it will be kept in this tag name.\n#   ## This tag can be used in downstream outputs.\n#   ## The default value of nothing means it will be off and the database will not be recorded.\n#   # bucket_tag = \"\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional token to accept for HTTP authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # token = \"some-long-shared-secret-token\"\n#\n#   ## Influx line protocol parser\n#   ## 'internal' is the default. 'upstream' is a newer parser that is faster\n#   ## and more memory efficient.\n#   # parser_type = \"internal\"\n\n\n# # Intel Performance Monitoring Unit plugin exposes Intel PMU metrics available through Linux Perf subsystem\n# [[inputs.intel_pmu]]\n#   ## List of filesystem locations of JSON files that contain PMU event definitions.\n#   event_definitions = [\"/var/cache/pmu/GenuineIntel-6-55-4-core.json\", \"/var/cache/pmu/GenuineIntel-6-55-4-uncore.json\"]\n#\n#   ## List of core events measurement entities. There can be more than one core_events sections.\n#   [[inputs.intel_pmu.core_events]]\n#     ## List of events to be counted. Event names shall match names from event_definitions files.\n#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n#     ## If absent, all core events from provided event_definitions are counted skipping unresolvable ones.\n#     events = [\"INST_RETIRED.ANY\", \"CPU_CLK_UNHALTED.THREAD_ANY:config1=0x4043200000000k\"]\n#\n#     ## Limits the counting of events to core numbers specified.\n#     ## If absent, events are counted on all cores.\n#     ## Single \"0\", multiple \"0,1,2\" and range \"0-2\" notation is supported for each array element.\n#     ##   example: cores = [\"0,2\", \"4\", \"12-16\"]\n#     cores = [\"0\"]\n#\n#     ## Indicator that plugin shall attempt to run core_events.events as a single perf group.\n#     ## If absent or set to false, each event is counted individually. Defaults to false.\n#     ## This limits the number of events that can be measured to a maximum of available hardware counters per core.\n#     ## Could vary depending on type of event, use of fixed counters.\n#     # perf_group = false\n#\n#     ## Optionally set a custom tag value that will be added to every measurement within this events group.\n#     ## Can be applied to any group of events, unrelated to perf_group setting.\n#     # events_tag = \"\"\n#\n#   ## List of uncore event measurement entities. There can be more than one uncore_events sections.\n#   [[inputs.intel_pmu.uncore_events]]\n#     ## List of events to be counted. Event names shall match names from event_definitions files.\n#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n#     ## If absent, all uncore events from provided event_definitions are counted skipping unresolvable ones.\n#     events = [\"UNC_CHA_CLOCKTICKS\", \"UNC_CHA_TOR_OCCUPANCY.IA_MISS\"]\n#\n#     ## Limits the counting of events to specified sockets.\n#     ## If absent, events are counted on all sockets.\n#     ## Single \"0\", multiple \"0,1\" and range \"0-1\" notation is supported for each array element.\n#     ##   example: sockets = [\"0-2\"]\n#     sockets = [\"0\"]\n#\n#     ## Indicator that plugin shall provide an aggregated value for multiple units of same type distributed in an uncore.\n#     ## If absent or set to false, events for each unit are exposed as separate metric. Defaults to false.\n#     # aggregate_uncore_units = false\n#\n#     ## Optionally set a custom tag value that will be added to every measurement within this events group.\n#     # events_tag = \"\"\n\n\n# # Intel Resource Director Technology plugin\n# [[inputs.intel_rdt]]\n# \t## Optionally set sampling interval to Nx100ms.\n# \t## This value is propagated to pqos tool. Interval format is defined by pqos itself.\n# \t## If not provided or provided 0, will be set to 10 = 10x100ms = 1s.\n# \t# sampling_interval = \"10\"\n# \t\n# \t## Optionally specify the path to pqos executable.\n# \t## If not provided, auto discovery will be performed.\n# \t# pqos_path = \"/usr/local/bin/pqos\"\n#\n# \t## Optionally specify if IPC and LLC_Misses metrics shouldn't be propagated.\n# \t## If not provided, default value is false.\n# \t# shortened_metrics = false\n# \t\n# \t## Specify the list of groups of CPU core(s) to be provided as pqos input.\n# \t## Mandatory if processes aren't set and forbidden if processes are specified.\n# \t## e.g. [\"0-3\", \"4,5,6\"] or [\"1-3,4\"]\n# \t# cores = [\"0-3\"]\n# \t\n# \t## Specify the list of processes for which Metrics will be collected.\n# \t## Mandatory if cores aren't set and forbidden if cores are specified.\n# \t## e.g. [\"qemu\", \"pmd\"]\n# \t# processes = [\"process\"]\n#\n# \t## Specify if the pqos process should be called with sudo.\n# \t## Mandatory if the telegraf process does not run as root.\n# \t# use_sudo = false\n\n\n# # Read JTI OpenConfig Telemetry from listed sensors\n# [[inputs.jti_openconfig_telemetry]]\n#   ## List of device addresses to collect telemetry from\n#   servers = [\"localhost:1883\"]\n#\n#   ## Authentication details. Username and password are must if device expects\n#   ## authentication. Client ID must be unique when connecting from multiple instances\n#   ## of telegraf to the same device\n#   username = \"user\"\n#   password = \"pass\"\n#   client_id = \"telegraf\"\n#\n#   ## Frequency to get data\n#   sample_frequency = \"1000ms\"\n#\n#   ## Sensors to subscribe for\n#   ## A identifier for each sensor can be provided in path by separating with space\n#   ## Else sensor path will be used as identifier\n#   ## When identifier is used, we can provide a list of space separated sensors.\n#   ## A single subscription will be created with all these sensors and data will\n#   ## be saved to measurement with this identifier name\n#   sensors = [\n#    \"/interfaces/\",\n#    \"collection /components/ /lldp\",\n#   ]\n#\n#   ## We allow specifying sensor group level reporting rate. To do this, specify the\n#   ## reporting rate in Duration at the beginning of sensor paths / collection\n#   ## name. For entries without reporting rate, we use configured sample frequency\n#   sensors = [\n#    \"1000ms customReporting /interfaces /lldp\",\n#    \"2000ms collection /components\",\n#    \"/interfaces\",\n#   ]\n#\n#   ## Optional TLS Config\n#   # enable_tls = true\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Delay between retry attempts of failed RPC calls or streams. Defaults to 1000ms.\n#   ## Failed streams/calls will not be retried if 0 is provided\n#   retry_delay = \"1000ms\"\n#\n#   ## To treat all string values as tags, set this to true\n#   str_as_tags = false\n\n\n# # Read metrics from Kafka topics\n# [[inputs.kafka_consumer]]\n#   ## Kafka brokers.\n#   brokers = [\"localhost:9092\"]\n#\n#   ## Topics to consume.\n#   topics = [\"telegraf\"]\n#\n#   ## When set this tag will be added to all metrics with the topic as the value.\n#   # topic_tag = \"\"\n#\n#   ## Optional Client id\n#   # client_id = \"Telegraf\"\n#\n#   ## Set the minimal supported Kafka version.  Setting this enables the use of new\n#   ## Kafka features and APIs.  Must be 0.10.2.0 or greater.\n#   ##   ex: version = \"1.1.0\"\n#   # version = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## SASL authentication credentials.  These settings should typically be used\n#   ## with TLS encryption enabled\n#   # sasl_username = \"kafka\"\n#   # sasl_password = \"secret\"\n#\n#   ## Optional SASL:\n#   ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n#   ## (defaults to PLAIN)\n#   # sasl_mechanism = \"\"\n#\n#   ## used if sasl_mechanism is GSSAPI (experimental)\n#   # sasl_gssapi_service_name = \"\"\n#   # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n#   # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n#   # sasl_gssapi_kerberos_config_path = \"/\"\n#   # sasl_gssapi_realm = \"realm\"\n#   # sasl_gssapi_key_tab_path = \"\"\n#   # sasl_gssapi_disable_pafxfast = false\n#\n#   ## used if sasl_mechanism is OAUTHBEARER (experimental)\n#   # sasl_access_token = \"\"\n#\n#   ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n#   # sasl_version = 1\n#\n#   # Disable Kafka metadata full fetch\n#   # metadata_full = false\n#\n#   ## Name of the consumer group.\n#   # consumer_group = \"telegraf_metrics_consumers\"\n#\n#   ## Compression codec represents the various compression codecs recognized by\n#   ## Kafka in messages.\n#   ##  0 : None\n#   ##  1 : Gzip\n#   ##  2 : Snappy\n#   ##  3 : LZ4\n#   ##  4 : ZSTD\n#    # compression_codec = 0\n#\n#   ## Initial offset position; one of \"oldest\" or \"newest\".\n#   # offset = \"oldest\"\n#\n#   ## Consumer group partition assignment strategy; one of \"range\", \"roundrobin\" or \"sticky\".\n#   # balance_strategy = \"range\"\n#\n#   ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n#   ## larger messages are dropped\n#   max_message_len = 1000000\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Maximum amount of time the consumer should take to process messages. If\n#   ## the debug log prints messages from sarama about 'abandoning subscription\n#   ## to [topic] because consuming was taking too long', increase this value to\n#   ## longer than the time taken by the output plugin(s).\n#   ##\n#   ## Note that the effective timeout could be between 'max_processing_time' and\n#   ## '2 * max_processing_time'.\n#   # max_processing_time = \"100ms\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read metrics from Kafka topic(s)\n# [[inputs.kafka_consumer_legacy]]\n#   ## DEPRECATED: The 'kafka_consumer_legacy' plugin is deprecated in version 1.4.0, use 'inputs.kafka_consumer' instead, NOTE: 'kafka_consumer' only supports Kafka v0.8+.\n#   ## topic(s) to consume\n#   topics = [\"telegraf\"]\n#\n#   ## an array of Zookeeper connection strings\n#   zookeeper_peers = [\"localhost:2181\"]\n#\n#   ## Zookeeper Chroot\n#   zookeeper_chroot = \"\"\n#\n#   ## the name of the consumer group\n#   consumer_group = \"telegraf_metrics_consumers\"\n#\n#   ## Offset (must be either \"oldest\" or \"newest\")\n#   offset = \"oldest\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n#   ## larger messages are dropped\n#   max_message_len = 65536\n\n\n# # Configuration for the AWS Kinesis input.\n# [[inputs.kinesis_consumer]]\n#   ## Amazon REGION of kinesis endpoint.\n#   region = \"ap-southeast-2\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   # access_key = \"\"\n#   # secret_key = \"\"\n#   # token = \"\"\n#   # role_arn = \"\"\n#   # web_identity_token_file = \"\"\n#   # role_session_name = \"\"\n#   # profile = \"\"\n#   # shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Kinesis StreamName must exist prior to starting telegraf.\n#   streamname = \"StreamName\"\n#\n#   ## Shard iterator type (only 'TRIM_HORIZON' and 'LATEST' currently supported)\n#   # shard_iterator_type = \"TRIM_HORIZON\"\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ##\n#   ## The content encoding of the data from kinesis\n#   ## If you are processing a cloudwatch logs kinesis stream then set this to \"gzip\"\n#   ## as AWS compresses cloudwatch log data before it is sent to kinesis (aws\n#   ## also base64 encodes the zip byte data before pushing to the stream.  The base64 decoding\n#   ## is done automatically by the golang sdk, as data is read from kinesis)\n#   ##\n#   # content_encoding = \"identity\"\n#\n#   ## Optional\n#   ## Configuration for a dynamodb checkpoint\n#   [inputs.kinesis_consumer.checkpoint_dynamodb]\n# \t## unique name for this consumer\n# \tapp_name = \"default\"\n# \ttable_name = \"default\"\n\n\n# # Listener capable of handling KNX bus messages provided through a KNX-IP Interface.\n# [[inputs.knx_listener]]\n#   ## Type of KNX-IP interface.\n#   ## Can be either \"tunnel\" or \"router\".\n#   # service_type = \"tunnel\"\n#\n#   ## Address of the KNX-IP interface.\n#   service_address = \"localhost:3671\"\n#\n#   ## Measurement definition(s)\n#   # [[inputs.knx_listener.measurement]]\n#   #   ## Name of the measurement\n#   #   name = \"temperature\"\n#   #   ## Datapoint-Type (DPT) of the KNX messages\n#   #   dpt = \"9.001\"\n#   #   ## List of Group-Addresses (GAs) assigned to the measurement\n#   #   addresses = [\"5/5/1\"]\n#\n#   # [[inputs.knx_listener.measurement]]\n#   #   name = \"illumination\"\n#   #   dpt = \"9.004\"\n#   #   addresses = [\"5/5/3\"]\n\n\n# # Read metrics off Arista LANZ, via socket\n# [[inputs.lanz]]\n#   ## URL to Arista LANZ endpoint\n#   servers = [\n#     \"tcp://127.0.0.1:50001\"\n#   ]\n\n\n\n# # Read metrics from MQTT topic(s)\n# [[inputs.mqtt_consumer]]\n#   ## Broker URLs for the MQTT server or cluster.  To connect to multiple\n#   ## clusters or standalone servers, use a separate plugin instance.\n#   ##   example: servers = [\"tcp://localhost:1883\"]\n#   ##            servers = [\"ssl://localhost:1883\"]\n#   ##            servers = [\"ws://localhost:1883\"]\n#   servers = [\"tcp://127.0.0.1:1883\"]\n#   ## Topics that will be subscribed to.\n#   topics = [\n#     \"telegraf/host01/cpu\",\n#     \"telegraf/+/mem\",\n#     \"sensors/#\",\n#   ]\n#   # topic_fields = \"_/_/_/temperature\"\n#   ## The message topic will be stored in a tag specified by this value.  If set\n#   ## to the empty string no topic tag will be created.\n#   # topic_tag = \"topic\"\n#   ## QoS policy for messages\n#   ##   0 = at most once\n#   ##   1 = at least once\n#   ##   2 = exactly once\n#   ##\n#   ## When using a QoS of 1 or 2, you should enable persistent_session to allow\n#   ## resuming unacknowledged messages.\n#   # qos = 0\n#   ## Connection timeout for initial connection in seconds\n#   # connection_timeout = \"30s\"\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#   ## Persistent session disables clearing of the client session on connection.\n#   ## In order for this option to work you must also set client_id to identify\n#   ## the client.  To receive messages that arrived while the client is offline,\n#   ## also set the qos option to 1 or 2 and don't forget to also set the QoS when\n#   ## publishing.\n#   # persistent_session = false\n#   ## If unset, a random client ID will be generated.\n#   # client_id = \"\"\n#   ## Username and password to connect MQTT server.\n#   # username = \"telegraf\"\n#   # password = \"metricsmetricsmetricsmetrics\"\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#   ## Enable extracting tag values from MQTT topics\n#   ## _ denotes an ignored entry in the topic path\n#   ## [[inputs.mqtt_consumer.topic_parsing]]\n#   ##  topic = \"\"\n#   ##  measurement = \"\"\n#   ##  tags = \"\"\n#   ##  fields = \"\"\n#   ## [inputs.mqtt_consumer.topic_parsing.types]\n#   ##\n\n\n# # Read metrics from NATS subject(s)\n# [[inputs.nats_consumer]]\n#   ## urls of NATS servers\n#   servers = [\"nats://localhost:4222\"]\n#\n#   ## subject(s) to consume\n#   subjects = [\"telegraf\"]\n#\n#   ## name a queue group\n#   queue_group = \"telegraf_consumers\"\n#\n#   ## Optional credentials\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional NATS 2.0 and NATS NGS compatible user credentials\n#   # credentials = \"/etc/telegraf/nats.creds\"\n#\n#   ## Use Transport Layer Security\n#   # secure = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Sets the limits for pending msgs and bytes for each subscription\n#   ## These shouldn't need to be adjusted except in very high throughput scenarios\n#   # pending_message_limit = 65536\n#   # pending_bytes_limit = 67108864\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read NSQ topic for metrics.\n# [[inputs.nsq_consumer]]\n#   ## An array representing the NSQD TCP HTTP Endpoints\n#   nsqd = [\"localhost:4150\"]\n#\n#   ## An array representing the NSQLookupd HTTP Endpoints\n#   nsqlookupd = [\"localhost:4161\"]\n#   topic = \"telegraf\"\n#   channel = \"consumer\"\n#   max_in_flight = 100\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Receive OpenTelemetry traces, metrics, and logs over gRPC\n# [[inputs.opentelemetry]]\n#   ## Override the default (0.0.0.0:4317) destination OpenTelemetry gRPC service\n#   ## address:port\n#   # service_address = \"0.0.0.0:4317\"\n#\n#   ## Override the default (5s) new connection timeout\n#   # timeout = \"5s\"\n#\n#   ## Override the default (prometheus-v1) metrics schema.\n#   ## Supports: \"prometheus-v1\", \"prometheus-v2\"\n#   ## For more information about the alternatives, read the Prometheus input\n#   ## plugin notes.\n#   # metrics_schema = \"prometheus-v1\"\n#\n#   ## Optional TLS Config.\n#   ## For advanced options: https://github.com/influxdata/telegraf/blob/v1.18.3/docs/TLS.md\n#   ##\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#   ## Add service certificate and key.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n\n\n# # Read metrics from one or many pgbouncer servers\n# [[inputs.pgbouncer]]\n#   ## specify address via a url matching:\n#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n#   ##       ?sslmode=[disable|verify-ca|verify-full]\n#   ## or a simple string:\n#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n#   ##\n#   ## All connection parameters are optional.\n#   ##\n#   address = \"host=localhost user=pgbouncer sslmode=disable\"\n\n\n# # Read metrics from one or many postgresql servers\n# [[inputs.postgresql]]\n#   ## specify address via a url matching:\n#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n#   ##       ?sslmode=[disable|verify-ca|verify-full]\n#   ## or a simple string:\n#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n#   ##\n#   ## All connection parameters are optional.\n#   ##\n#   ## Without the dbname parameter, the driver will default to a database\n#   ## with the same name as the user. This dbname is just for instantiating a\n#   ## connection with the server and doesn't restrict the databases we are trying\n#   ## to grab metrics for.\n#   ##\n#   address = \"host=localhost user=postgres sslmode=disable\"\n#   ## A custom name for the database that will be used as the \"server\" tag in the\n#   ## measurement output. If not specified, a default one generated from\n#   ## the connection address is used.\n#   # outputaddress = \"db01\"\n#\n#   ## connection configuration.\n#   ## maxlifetime - specify the maximum lifetime of a connection.\n#   ## default is forever (0s)\n#   max_lifetime = \"0s\"\n#\n#   ## A  list of databases to explicitly ignore.  If not specified, metrics for all\n#   ## databases are gathered.  Do NOT use with the 'databases' option.\n#   # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n#\n#   ## A list of databases to pull metrics about. If not specified, metrics for all\n#   ## databases are gathered.  Do NOT use with the 'ignored_databases' option.\n#   # databases = [\"app_production\", \"testing\"]\n#\n#   ## Whether to use prepared statements when connecting to the database.\n#   ## This should be set to false when connecting through a PgBouncer instance\n#   ## with pool_mode set to transaction.\n#   # prepared_statements = true\n\n\n# # Read metrics from one or many postgresql servers\n# [[inputs.postgresql_extensible]]\n#   ## specify address via a url matching:\n#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n#   ##       ?sslmode=[disable|verify-ca|verify-full]\n#   ## or a simple string:\n#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n#   #\n#   ## All connection parameters are optional.  #\n#   ## Without the dbname parameter, the driver will default to a database\n#   ## with the same name as the user. This dbname is just for instantiating a\n#   ## connection with the server and doesn't restrict the databases we are trying\n#   ## to grab metrics for.\n#   #\n#   address = \"host=localhost user=postgres sslmode=disable\"\n#\n#   ## connection configuration.\n#   ## maxlifetime - specify the maximum lifetime of a connection.\n#   ## default is forever (0s)\n#   max_lifetime = \"0s\"\n#\n#   ## Whether to use prepared statements when connecting to the database.\n#   ## This should be set to false when connecting through a PgBouncer instance\n#   ## with pool_mode set to transaction.\n#   # prepared_statements = true\n#\n#   ## A list of databases to pull metrics about. If not specified, metrics for all\n#   ## databases are gathered.\n#   ## databases = [\"app_production\", \"testing\"]\n#   #\n#   ## A custom name for the database that will be used as the \"server\" tag in the\n#   ## measurement output. If not specified, a default one generated from\n#   ## the connection address is used.\n#   # outputaddress = \"db01\"\n#   #\n#   ## Define the toml config where the sql queries are stored\n#   ## New queries can be added, if the withdbname is set to true and there is no\n#   ## databases defined in the 'databases field', the sql query is ended by a\n#   ## 'is not null' in order to make the query succeed.\n#   ## Example :\n#   ## The sqlquery : \"SELECT * FROM pg_stat_database where datname\" become\n#   ## \"SELECT * FROM pg_stat_database where datname IN ('postgres', 'pgbench')\"\n#   ## because the databases variable was set to ['postgres', 'pgbench' ] and the\n#   ## withdbname was true. Be careful that if the withdbname is set to false you\n#   ## don't have to define the where clause (aka with the dbname) the tagvalue\n#   ## field is used to define custom tags (separated by commas)\n#   ## The optional \"measurement\" value can be used to override the default\n#   ## output measurement name (\"postgresql\").\n#   ##\n#   ## The script option can be used to specify the .sql file path.\n#   ## If script and sqlquery options specified at same time, sqlquery will be used\n#   ##\n#   ## the tagvalue field is used to define custom tags (separated by comas).\n#   ## the query is expected to return columns which match the names of the\n#   ## defined tags. The values in these columns must be of a string-type,\n#   ## a number-type or a blob-type.\n#   ##\n#   ## The timestamp field is used to override the data points timestamp value. By\n#   ## default, all rows inserted with current time. By setting a timestamp column,\n#   ## the row will be inserted with that column's value.\n#   ##\n#   ## Structure :\n#   ## [[inputs.postgresql_extensible.query]]\n#   ##   sqlquery string\n#   ##   version string\n#   ##   withdbname boolean\n#   ##   tagvalue string (comma separated)\n#   ##   measurement string\n#   ##   timestamp string\n#   [[inputs.postgresql_extensible.query]]\n#     sqlquery=\"SELECT * FROM pg_stat_database\"\n#     version=901\n#     withdbname=false\n#     tagvalue=\"\"\n#     measurement=\"\"\n#   [[inputs.postgresql_extensible.query]]\n#     sqlquery=\"SELECT * FROM pg_stat_bgwriter\"\n#     version=901\n#     withdbname=false\n#     tagvalue=\"postgresql.stats\"\n\n\n# # Read metrics from one or many prometheus clients\n# [[inputs.prometheus]]\n#   ## An array of urls to scrape metrics from.\n#   urls = [\"http://localhost:9100/metrics\"]\n#\n#   ## Metric version controls the mapping from Prometheus metrics into\n#   ## Telegraf metrics.  When using the prometheus_client output, use the same\n#   ## value in both plugins to ensure metrics are round-tripped without\n#   ## modification.\n#   ##\n#   ##   example: metric_version = 1;\n#   ##            metric_version = 2; recommended version\n#   # metric_version = 1\n#\n#   ## Url tag name (tag containing scrapped url. optional, default is \"url\")\n#   # url_tag = \"url\"\n#\n#   ## Whether the timestamp of the scraped metrics will be ignored.\n#   ## If set to true, the gather time will be used.\n#   # ignore_timestamp = false\n#\n#   ## An array of Kubernetes services to scrape metrics from.\n#   # kubernetes_services = [\"http://my-service-dns.my-namespace:9100/metrics\"]\n#\n#   ## Kubernetes config file to create client from.\n#   # kube_config = \"/path/to/kubernetes.config\"\n#\n#   ## Scrape Kubernetes pods for the following prometheus annotations:\n#   ## - prometheus.io/scrape: Enable scraping for this pod\n#   ## - prometheus.io/scheme: If the metrics endpoint is secured then you will need to\n#   ##     set this to 'https' \u0026 most likely set the tls config.\n#   ## - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.\n#   ## - prometheus.io/port: If port is not 9102 use this annotation\n#   # monitor_kubernetes_pods = true\n#   ## Get the list of pods to scrape with either the scope of\n#   ## - cluster: the kubernetes watch api (default, no need to specify)\n#   ## - node: the local cadvisor api; for scalability. Note that the config node_ip or the environment variable NODE_IP must be set to the host IP.\n#   # pod_scrape_scope = \"cluster\"\n#   ## Only for node scrape scope: node IP of the node that telegraf is running on.\n#   ## Either this config or the environment variable NODE_IP must be set.\n#   # node_ip = \"10.180.1.1\"\n# \t## Only for node scrape scope: interval in seconds for how often to get updated pod list for scraping.\n# \t## Default is 60 seconds.\n# \t# pod_scrape_interval = 60\n#   ## Restricts Kubernetes monitoring to a single namespace\n#   ##   ex: monitor_kubernetes_pods_namespace = \"default\"\n#   # monitor_kubernetes_pods_namespace = \"\"\n#   # label selector to target pods which have the label\n#   # kubernetes_label_selector = \"env=dev,app=nginx\"\n#   # field selector to target pods\n#   # eg. To scrape pods on a specific node\n#   # kubernetes_field_selector = \"spec.nodeName=$HOSTNAME\"\n#\n#   ## Scrape Services available in Consul Catalog\n#   # [inputs.prometheus.consul]\n#   #   enabled = true\n#   #   agent = \"http://localhost:8500\"\n#   #   query_interval = \"5m\"\n#\n#   #   [[inputs.prometheus.consul.query]]\n#   #     name = \"a service name\"\n#   #     tag = \"a service tag\"\n#   #     url = 'http://{{if ne .ServiceAddress \"\"}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}'\n#   #     [inputs.prometheus.consul.query.tags]\n#   #       host = \"{{.Node}}\"\n#\n#   ## Use bearer token for authorization. ('bearer_token' takes priority)\n#   # bearer_token = \"/path/to/bearer/token\"\n#   ## OR\n#   # bearer_token_string = \"abc_123\"\n#\n#   ## HTTP Basic Authentication username and password. ('bearer_token' and\n#   ## 'bearer_token_string' take priority)\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Specify timeout duration for slower prometheus clients (default is 3s)\n#   # response_timeout = \"3s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # RAS plugin exposes counter metrics for Machine Check Errors provided by RASDaemon (sqlite3 output is required).\n# [[inputs.ras]]\n#   ## Optional path to RASDaemon sqlite3 database.\n#   ## Default: /var/lib/rasdaemon/ras-mc_event.db\n#   # db_path = \"\"\n\n\n# # Riemann protobuff listener.\n# [[inputs.riemann_listener]]\n#   ## URL to listen on.\n#   ## Default is \"tcp://:5555\"\n#   # service_address = \"tcp://:8094\"\n#   # service_address = \"tcp://127.0.0.1:http\"\n#   # service_address = \"tcp4://:8094\"\n#   # service_address = \"tcp6://:8094\"\n#   # service_address = \"tcp6://[2001:db8::1]:8094\"\n#\n#   ## Maximum number of concurrent connections.\n#   ## 0 (default) is unlimited.\n#   # max_connections = 1024\n#   ## Read timeout.\n#   ## 0 (default) is unlimited.\n#   # read_timeout = \"30s\"\n#   ## Optional TLS configuration.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key  = \"/etc/telegraf/key.pem\"\n#   ## Enables client authentication if set.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#   ## Maximum socket buffer size (in bytes when no unit specified).\n#   # read_buffer_size = \"64KiB\"\n#   ## Period between keep alive probes.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n\n\n# # SFlow V5 Protocol Listener\n# [[inputs.sflow]]\n#   ## Address to listen for sFlow packets.\n#   ##   example: service_address = \"udp://:6343\"\n#   ##            service_address = \"udp4://:6343\"\n#   ##            service_address = \"udp6://:6343\"\n#   service_address = \"udp://:6343\"\n#\n#   ## Set the size of the operating system's receive buffer.\n#   ##   example: read_buffer_size = \"64KiB\"\n#   # read_buffer_size = \"\"\n\n\n# # Receive SNMP traps\n# [[inputs.snmp_trap]]\n#   ## Transport, local address, and port to listen on.  Transport must\n#   ## be \"udp://\".  Omit local address to listen on all interfaces.\n#   ##   example: \"udp://127.0.0.1:1234\"\n#   ##\n#   ## Special permissions may be required to listen on a port less than\n#   ## 1024.  See README.md for details\n#   ##\n#   # service_address = \"udp://:162\"\n#   ##\n#   ## Path to mib files\n#   # path = [\"/usr/share/snmp/mibs\"]\n#   ##\n#   ## Snmp version, defaults to 2c\n#   # version = \"2c\"\n#   ## SNMPv3 authentication and encryption options.\n#   ##\n#   ## Security Name.\n#   # sec_name = \"myuser\"\n#   ## Authentication protocol; one of \"MD5\", \"SHA\" or \"\".\n#   # auth_protocol = \"MD5\"\n#   ## Authentication password.\n#   # auth_password = \"pass\"\n#   ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n#   # sec_level = \"authNoPriv\"\n#   ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\", \"AES192\", \"AES192C\", \"AES256\", \"AES256C\" or \"\".\n#   # priv_protocol = \"\"\n#   ## Privacy password used for encrypted messages.\n#   # priv_password = \"\"\n\n\n# # Generic socket listener capable of handling multiple socket types.\n# [[inputs.socket_listener]]\n#   ## URL to listen on\n#   # service_address = \"tcp://:8094\"\n#   # service_address = \"tcp://127.0.0.1:http\"\n#   # service_address = \"tcp4://:8094\"\n#   # service_address = \"tcp6://:8094\"\n#   # service_address = \"tcp6://[2001:db8::1]:8094\"\n#   # service_address = \"udp://:8094\"\n#   # service_address = \"udp4://:8094\"\n#   # service_address = \"udp6://:8094\"\n#   # service_address = \"unix:///tmp/telegraf.sock\"\n#   # service_address = \"unixgram:///tmp/telegraf.sock\"\n#\n#   ## Change the file mode bits on unix sockets.  These permissions may not be\n#   ## respected by some platforms, to safely restrict write permissions it is best\n#   ## to place the socket into a directory that has previously been created\n#   ## with the desired permissions.\n#   ##   ex: socket_mode = \"777\"\n#   # socket_mode = \"\"\n#\n#   ## Maximum number of concurrent connections.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   ## 0 (default) is unlimited.\n#   # max_connections = 1024\n#\n#   ## Read timeout.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   ## 0 (default) is unlimited.\n#   # read_timeout = \"30s\"\n#\n#   ## Optional TLS configuration.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key  = \"/etc/telegraf/key.pem\"\n#   ## Enables client authentication if set.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Maximum socket buffer size (in bytes when no unit specified).\n#   ## For stream sockets, once the buffer fills up, the sender will start backing up.\n#   ## For datagram sockets, once the buffer fills up, metrics will start dropping.\n#   ## Defaults to the OS default.\n#   # read_buffer_size = \"64KiB\"\n#\n#   ## Period between keep alive probes.\n#   ## Only applies to TCP sockets.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   # data_format = \"influx\"\n#\n#   ## Content encoding for message payloads, can be set to \"gzip\" to or\n#   ## \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n\n\n# # Read metrics from SQL queries\n# [[inputs.sql]]\n#   ## Database Driver\n#   ## See https://github.com/influxdata/telegraf/blob/master/docs/SQL_DRIVERS_INPUT.md for\n#   ## a list of supported drivers.\n#   driver = \"mysql\"\n#\n#   ## Data source name for connecting\n#   ## The syntax and supported options depends on selected driver.\n#   dsn = \"username:password@mysqlserver:3307/dbname?param=value\"\n#\n#   ## Timeout for any operation\n#   ## Note that the timeout for queries is per query not per gather.\n#   # timeout = \"5s\"\n#\n#   ## Connection time limits\n#   ## By default the maximum idle time and maximum lifetime of a connection is unlimited, i.e. the connections\n#   ## will not be closed automatically. If you specify a positive time, the connections will be closed after\n#   ## idleing or existing for at least that amount of time, respectively.\n#   # connection_max_idle_time = \"0s\"\n#   # connection_max_life_time = \"0s\"\n#\n#   ## Connection count limits\n#   ## By default the number of open connections is not limited and the number of maximum idle connections\n#   ## will be inferred from the number of queries specified. If you specify a positive number for any of the\n#   ## two options, connections will be closed when reaching the specified limit. The number of idle connections\n#   ## will be clipped to the maximum number of connections limit if any.\n#   # connection_max_open = 0\n#   # connection_max_idle = auto\n#\n#   [[inputs.sql.query]]\n#     ## Query to perform on the server\n#     query=\"SELECT user,state,latency,score FROM Scoreboard WHERE application \u003e 0\"\n#     ## Alternatively to specifying the query directly you can select a file here containing the SQL query.\n#     ## Only one of 'query' and 'query_script' can be specified!\n#     # query_script = \"/path/to/sql/script.sql\"\n#\n#     ## Name of the measurement\n#     ## In case both measurement and 'measurement_col' are given, the latter takes precedence.\n#     # measurement = \"sql\"\n#\n#     ## Column name containing the name of the measurement\n#     ## If given, this will take precedence over the 'measurement' setting. In case a query result\n#     ## does not contain the specified column, we fall-back to the 'measurement' setting.\n#     # measurement_column = \"\"\n#\n#     ## Column name containing the time of the measurement\n#     ## If ommited, the time of the query will be used.\n#     # time_column = \"\"\n#\n#     ## Format of the time contained in 'time_col'\n#     ## The time must be 'unix', 'unix_ms', 'unix_us', 'unix_ns', or a golang time format.\n#     ## See https://golang.org/pkg/time/#Time.Format for details.\n#     # time_format = \"unix\"\n#\n#     ## Column names containing tags\n#     ## An empty include list will reject all columns and an empty exclude list will not exclude any column.\n#     ## I.e. by default no columns will be returned as tag and the tags are empty.\n#     # tag_columns_include = []\n#     # tag_columns_exclude = []\n#\n# \t\t## Column names containing fields (explicit types)\n#     ## Convert the given columns to the corresponding type. Explicit type conversions take precedence over\n# \t\t## the automatic (driver-based) conversion below.\n# \t\t## NOTE: Columns should not be specified for multiple types or the resulting type is undefined.\n#     # field_columns_float = []\n#     # field_columns_int = []\n# \t\t# field_columns_uint = []\n# \t\t# field_columns_bool = []\n# \t\t# field_columns_string = []\n#\n#     ## Column names containing fields (automatic types)\n#     ## An empty include list is equivalent to '[*]' and all returned columns will be accepted. An empty\n#     ## exclude list will not exclude any column. I.e. by default all columns will be returned as fields.\n#     ## NOTE: We rely on the database driver to perform automatic datatype conversion.\n#     # field_columns_include = []\n#     # field_columns_exclude = []\n\n\n# # Read metrics from Microsoft SQL Server\n# [[inputs.sqlserver]]\n# ## Specify instances to monitor with a list of connection strings.\n# ## All connection parameters are optional.\n# ## By default, the host is localhost, listening on default port, TCP 1433.\n# ##   for Windows, the user is the currently running AD user (SSO).\n# ##   See https://github.com/denisenkom/go-mssqldb for detailed connection\n# ##   parameters, in particular, tls connections can be created like so:\n# ##   \"encrypt=true;certificate=\u003ccert\u003e;hostNameInCertificate=\u003cSqlServer host fqdn\u003e\"\n# servers = [\n#   \"Server=192.168.1.10;Port=1433;User Id=\u003cuser\u003e;Password=\u003cpw\u003e;app name=telegraf;log=1;\",\n# ]\n#\n# ## Authentication method\n# ## valid methods: \"connection_string\", \"AAD\"\n# # auth_method = \"connection_string\"\n#\n# ## \"database_type\" enables a specific set of queries depending on the database type.\n# ## In the config file, the sql server plugin section should be repeated each with a set of servers for a specific database_type.\n# ## Possible values for database_type are - \"SQLServer\" or \"AzureSQLDB\" or \"AzureSQLManagedInstance\" or \"AzureSQLPool\"\n#\n# database_type = \"SQLServer\"\n#\n# ## A list of queries to include. If not specified, all the below listed queries are used.\n# include_query = []\n#\n# ## A list of queries to explicitly ignore.\n# exclude_query = [\"SQLServerAvailabilityReplicaStates\", \"SQLServerDatabaseReplicaStates\"]\n#\n# ## Queries enabled by default for database_type = \"SQLServer\" are -\n# ## SQLServerPerformanceCounters, SQLServerWaitStatsCategorized, SQLServerDatabaseIO, SQLServerProperties, SQLServerMemoryClerks,\n# ## SQLServerSchedulers, SQLServerRequests, SQLServerVolumeSpace, SQLServerCpu, SQLServerAvailabilityReplicaStates, SQLServerDatabaseReplicaStates\n#\n# ## Queries enabled by default for database_type = \"AzureSQLDB\" are -\n# ## AzureSQLDBResourceStats, AzureSQLDBResourceGovernance, AzureSQLDBWaitStats, AzureSQLDBDatabaseIO, AzureSQLDBServerProperties,\n# ## AzureSQLDBOsWaitstats, AzureSQLDBMemoryClerks, AzureSQLDBPerformanceCounters, AzureSQLDBRequests, AzureSQLDBSchedulers\n#\n# ## Queries enabled by default for database_type = \"AzureSQLManagedInstance\" are -\n# ## AzureSQLMIResourceStats, AzureSQLMIResourceGovernance, AzureSQLMIDatabaseIO, AzureSQLMIServerProperties, AzureSQLMIOsWaitstats,\n# ## AzureSQLMIMemoryClerks, AzureSQLMIPerformanceCounters, AzureSQLMIRequests, AzureSQLMISchedulers\n#\n# ## Queries enabled by default for database_type = \"AzureSQLPool\" are -\n# ## AzureSQLPoolResourceStats, AzureSQLPoolResourceGovernance, AzureSQLPoolDatabaseIO, AzureSQLPoolWaitStats,\n# ## AzureSQLPoolMemoryClerks, AzureSQLPoolPerformanceCounters, AzureSQLPoolSchedulers\n\n\n# # Statsd UDP/TCP Server\n# [[inputs.statsd]]\n#   ## Protocol, must be \"tcp\", \"udp\", \"udp4\" or \"udp6\" (default=udp)\n#   protocol = \"udp\"\n#\n#   ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)\n#   max_tcp_connections = 250\n#\n#   ## Enable TCP keep alive probes (default=false)\n#   tcp_keep_alive = false\n#\n#   ## Specifies the keep-alive period for an active network connection.\n#   ## Only applies to TCP sockets and will be ignored if tcp_keep_alive is false.\n#   ## Defaults to the OS configuration.\n#   # tcp_keep_alive_period = \"2h\"\n#\n#   ## Address and port to host UDP listener on\n#   service_address = \":8125\"\n#\n#   ## The following configuration options control when telegraf clears it's cache\n#   ## of previous values. If set to false, then telegraf will only clear it's\n#   ## cache when the daemon is restarted.\n#   ## Reset gauges every interval (default=true)\n#   delete_gauges = true\n#   ## Reset counters every interval (default=true)\n#   delete_counters = true\n#   ## Reset sets every interval (default=true)\n#   delete_sets = true\n#   ## Reset timings \u0026 histograms every interval (default=true)\n#   delete_timings = true\n#\n#   ## Percentiles to calculate for timing \u0026 histogram stats\n#   percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0]\n#\n#   ## separator to use between elements of a statsd metric\n#   metric_separator = \"_\"\n#\n#   ## Parses tags in the datadog statsd format\n#   ## http://docs.datadoghq.com/guides/dogstatsd/\n#   parse_data_dog_tags = false\n#\n#   ## Parses datadog extensions to the statsd format\n#   datadog_extensions = false\n#\n#   ## Parses distributions metric as specified in the datadog statsd format\n#   ## https://docs.datadoghq.com/developers/metrics/types/?tab=distribution#definition\n#   datadog_distributions = false\n#\n#   ## Statsd data translation templates, more info can be read here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md\n#   # templates = [\n#   #     \"cpu.* measurement*\"\n#   # ]\n#\n#   ## Number of UDP messages allowed to queue up, once filled,\n#   ## the statsd server will start dropping packets\n#   allowed_pending_messages = 10000\n#\n#   ## Number of timing/histogram values to track per-measurement in the\n#   ## calculation of percentiles. Raising this limit increases the accuracy\n#   ## of percentiles but also increases the memory usage and cpu time.\n#   percentile_limit = 1000\n#\n#   ## Max duration (TTL) for each metric to stay cached/reported without being updated.\n#   #max_ttl = \"1000h\"\n#\n#   ## Sanitize name method\n#   ## By default, telegraf will pass names directly as they are received.\n#   ## However, upstream statsd now does sanitization of names which can be\n#   ## enabled by using the \"upstream\" method option. This option will a) replace\n#   ## white space with '_', replace '/' with '-', and remove charachters not\n#   ## matching 'a-zA-Z_\\-0-9\\.;='.\n#   #sanitize_name_method = \"\"\n\n\n# # Suricata stats and alerts plugin\n# [[inputs.suricata]]\n#   ## Data sink for Suricata stats and alerts logs\n#   # This is expected to be a filename of a\n#   # unix socket to be created for listening.\n#   source = \"/var/run/suricata-stats.sock\"\n#\n#   # Delimiter for flattening field keys, e.g. subitem \"alert\" of \"detect\"\n#   # becomes \"detect_alert\" when delimiter is \"_\".\n#   delimiter = \"_\"\n#\n#   ## Detect alert logs\n#   # alerts = false\n\n\n# # Accepts syslog messages following RFC5424 format with transports as per RFC5426, RFC5425, or RFC6587\n# [[inputs.syslog]]\n#   ## Specify an ip or hostname with port - eg., tcp://localhost:6514, tcp://10.0.0.1:6514\n#   ## Protocol, address and port to host the syslog receiver.\n#   ## If no host is specified, then localhost is used.\n#   ## If no port is specified, 6514 is used (RFC5425#section-4.1).\n#   server = \"tcp://:6514\"\n#\n#   ## TLS Config\n#   # tls_allowed_cacerts = [\"/etc/telegraf/ca.pem\"]\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Period between keep alive probes.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   # keep_alive_period = \"5m\"\n#\n#   ## Maximum number of concurrent connections (default = 0).\n#   ## 0 means unlimited.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   # max_connections = 1024\n#\n#   ## Read timeout is the maximum time allowed for reading a single message (default = 5s).\n#   ## 0 means unlimited.\n#   # read_timeout = \"5s\"\n#\n#   ## The framing technique with which it is expected that messages are transported (default = \"octet-counting\").\n#   ## Whether the messages come using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n#   ## or the non-transparent framing technique (RFC6587#section-3.4.2).\n#   ## Must be one of \"octet-counting\", \"non-transparent\".\n#   # framing = \"octet-counting\"\n#\n#   ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n#   ## Must be one of \"LF\", or \"NUL\".\n#   # trailer = \"LF\"\n#\n#   ## Whether to parse in best effort mode or not (default = false).\n#   ## By default best effort parsing is off.\n#   # best_effort = false\n#\n#   ## The RFC standard to use for message parsing\n#   ## By default RFC5424 is used. RFC3164 only supports UDP transport (no streaming support)\n#   ## Must be one of \"RFC5424\", or \"RFC3164\".\n#   # syslog_standard = \"RFC5424\"\n#\n#   ## Character to prepend to SD-PARAMs (default = \"_\").\n#   ## A syslog message can contain multiple parameters and multiple identifiers within structured data section.\n#   ## Eg., [id1 name1=\"val1\" name2=\"val2\"][id2 name1=\"val1\" nameA=\"valA\"]\n#   ## For each combination a field is created.\n#   ## Its name is created concatenating identifier, sdparam_separator, and parameter name.\n#   # sdparam_separator = \"_\"\n\n\n# # Parse the new lines appended to a file\n# [[inputs.tail]]\n#   ## File names or a pattern to tail.\n#   ## These accept standard unix glob matching rules, but with the addition of\n#   ## ** as a \"super asterisk\". ie:\n#   ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n#   ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n#   ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n#   ##   \"/var/log/log[!1-2]*  -\u003e tail files without 1-2\n#   ##   \"/var/log/log[^1-2]*  -\u003e identical behavior as above\n#   ## See https://github.com/gobwas/glob for more examples\n#   ##\n#   files = [\"/var/mymetrics.out\"]\n#\n#   ## Read file from beginning.\n#   # from_beginning = false\n#\n#   ## Whether file is a named pipe\n#   # pipe = false\n#\n#   ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n#   # watch_method = \"inotify\"\n#\n#   ## Maximum lines of the file to process that have not yet be written by the\n#   ## output.  For best throughput set based on the number of metrics on each\n#   ## line and the size of the output's metric_batch_size.\n#   # max_undelivered_lines = 1000\n#\n#   ## Character encoding to use when interpreting the file contents.  Invalid\n#   ## characters are replaced using the unicode replacement character.  When set\n#   ## to the empty string the data is not decoded to text.\n#   ##   ex: character_encoding = \"utf-8\"\n#   ##       character_encoding = \"utf-16le\"\n#   ##       character_encoding = \"utf-16be\"\n#   ##       character_encoding = \"\"\n#   # character_encoding = \"\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Set the tag that will contain the path of the tailed file. If you don't want this tag, set it to an empty string.\n#   # path_tag = \"path\"\n#\n#   ## multiline parser/codec\n#   ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html\n#   #[inputs.tail.multiline]\n#     ## The pattern should be a regexp which matches what you believe to be an\n# \t## indicator that the field is part of an event consisting of multiple lines of log data.\n#     #pattern = \"^\\s\"\n#\n#     ## This field must be either \"previous\" or \"next\".\n# \t## If a line matches the pattern, \"previous\" indicates that it belongs to the previous line,\n# \t## whereas \"next\" indicates that the line belongs to the next one.\n#     #match_which_line = \"previous\"\n#\n#     ## The invert_match field can be true or false (defaults to false).\n#     ## If true, a message not matching the pattern will constitute a match of the multiline\n# \t## filter and the what will be applied. (vice-versa is also true)\n#     #invert_match = false\n#\n#     ## After the specified timeout, this plugin sends a multiline event even if no new pattern\n# \t## is found to start a new event. The default timeout is 5s.\n#     #timeout = 5s\n\n\n# # Generic TCP listener\n# [[inputs.tcp_listener]]\n#   ## DEPRECATED: The 'tcp_listener' plugin is deprecated in version 1.3.0, use 'inputs.socket_listener' instead.\n#   # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n#   # socket_listener plugin\n#   # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n\n# # Generic UDP listener\n# [[inputs.udp_listener]]\n#   ## DEPRECATED: The 'udp_listener' plugin is deprecated in version 1.3.0, use 'inputs.socket_listener' instead.\n#   # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n#   # socket_listener plugin\n#   # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n\n# # Read metrics from VMware vCenter\n# [[inputs.vsphere]]\n#   ## List of vCenter URLs to be monitored. These three lines must be uncommented\n#   ## and edited for the plugin to work.\n#   vcenters = [ \"https://vcenter.local/sdk\" ]\n#   username = \"user@corp.local\"\n#   password = \"secret\"\n#\n#   ## VMs\n#   ## Typical VM metrics (if omitted or empty, all metrics are collected)\n#   # vm_include = [ \"/*/vm/**\"] # Inventory path to VMs to collect (by default all are collected)\n#   # vm_exclude = [] # Inventory paths to exclude\n#   vm_metric_include = [\n#     \"cpu.demand.average\",\n#     \"cpu.idle.summation\",\n#     \"cpu.latency.average\",\n#     \"cpu.readiness.average\",\n#     \"cpu.ready.summation\",\n#     \"cpu.run.summation\",\n#     \"cpu.usagemhz.average\",\n#     \"cpu.used.summation\",\n#     \"cpu.wait.summation\",\n#     \"mem.active.average\",\n#     \"mem.granted.average\",\n#     \"mem.latency.average\",\n#     \"mem.swapin.average\",\n#     \"mem.swapinRate.average\",\n#     \"mem.swapout.average\",\n#     \"mem.swapoutRate.average\",\n#     \"mem.usage.average\",\n#     \"mem.vmmemctl.average\",\n#     \"net.bytesRx.average\",\n#     \"net.bytesTx.average\",\n#     \"net.droppedRx.summation\",\n#     \"net.droppedTx.summation\",\n#     \"net.usage.average\",\n#     \"power.power.average\",\n#     \"virtualDisk.numberReadAveraged.average\",\n#     \"virtualDisk.numberWriteAveraged.average\",\n#     \"virtualDisk.read.average\",\n#     \"virtualDisk.readOIO.latest\",\n#     \"virtualDisk.throughput.usage.average\",\n#     \"virtualDisk.totalReadLatency.average\",\n#     \"virtualDisk.totalWriteLatency.average\",\n#     \"virtualDisk.write.average\",\n#     \"virtualDisk.writeOIO.latest\",\n#     \"sys.uptime.latest\",\n#   ]\n#   # vm_metric_exclude = [] ## Nothing is excluded by default\n#   # vm_instances = true ## true by default\n#\n#   ## Hosts\n#   ## Typical host metrics (if omitted or empty, all metrics are collected)\n#   # host_include = [ \"/*/host/**\"] # Inventory path to hosts to collect (by default all are collected)\n#   # host_exclude [] # Inventory paths to exclude\n#   host_metric_include = [\n#     \"cpu.coreUtilization.average\",\n#     \"cpu.costop.summation\",\n#     \"cpu.demand.average\",\n#     \"cpu.idle.summation\",\n#     \"cpu.latency.average\",\n#     \"cpu.readiness.average\",\n#     \"cpu.ready.summation\",\n#     \"cpu.swapwait.summation\",\n#     \"cpu.usage.average\",\n#     \"cpu.usagemhz.average\",\n#     \"cpu.used.summation\",\n#     \"cpu.utilization.average\",\n#     \"cpu.wait.summation\",\n#     \"disk.deviceReadLatency.average\",\n#     \"disk.deviceWriteLatency.average\",\n#     \"disk.kernelReadLatency.average\",\n#     \"disk.kernelWriteLatency.average\",\n#     \"disk.numberReadAveraged.average\",\n#     \"disk.numberWriteAveraged.average\",\n#     \"disk.read.average\",\n#     \"disk.totalReadLatency.average\",\n#     \"disk.totalWriteLatency.average\",\n#     \"disk.write.average\",\n#     \"mem.active.average\",\n#     \"mem.latency.average\",\n#     \"mem.state.latest\",\n#     \"mem.swapin.average\",\n#     \"mem.swapinRate.average\",\n#     \"mem.swapout.average\",\n#     \"mem.swapoutRate.average\",\n#     \"mem.totalCapacity.average\",\n#     \"mem.usage.average\",\n#     \"mem.vmmemctl.average\",\n#     \"net.bytesRx.average\",\n#     \"net.bytesTx.average\",\n#     \"net.droppedRx.summation\",\n#     \"net.droppedTx.summation\",\n#     \"net.errorsRx.summation\",\n#     \"net.errorsTx.summation\",\n#     \"net.usage.average\",\n#     \"power.power.average\",\n#     \"storageAdapter.numberReadAveraged.average\",\n#     \"storageAdapter.numberWriteAveraged.average\",\n#     \"storageAdapter.read.average\",\n#     \"storageAdapter.write.average\",\n#     \"sys.uptime.latest\",\n#   ]\n#     ## Collect IP addresses? Valid values are \"ipv4\" and \"ipv6\"\n#   # ip_addresses = [\"ipv6\", \"ipv4\" ]\n#\n#   # host_metric_exclude = [] ## Nothing excluded by default\n#   # host_instances = true ## true by default\n#\n#\n#   ## Clusters\n#   # cluster_include = [ \"/*/host/**\"] # Inventory path to clusters to collect (by default all are collected)\n#   # cluster_exclude = [] # Inventory paths to exclude\n#   # cluster_metric_include = [] ## if omitted or empty, all metrics are collected\n#   # cluster_metric_exclude = [] ## Nothing excluded by default\n#   # cluster_instances = false ## false by default\n#\n#   ## Datastores\n#   # datastore_include = [ \"/*/datastore/**\"] # Inventory path to datastores to collect (by default all are collected)\n#   # datastore_exclude = [] # Inventory paths to exclude\n#   # datastore_metric_include = [] ## if omitted or empty, all metrics are collected\n#   # datastore_metric_exclude = [] ## Nothing excluded by default\n#   # datastore_instances = false ## false by default\n#\n#   ## Datacenters\n#   # datacenter_include = [ \"/*/host/**\"] # Inventory path to clusters to collect (by default all are collected)\n#   # datacenter_exclude = [] # Inventory paths to exclude\n#   datacenter_metric_include = [] ## if omitted or empty, all metrics are collected\n#   datacenter_metric_exclude = [ \"*\" ] ## Datacenters are not collected by default.\n#   # datacenter_instances = false ## false by default\n#\n#   ## Plugin Settings\n#   ## separator character to use for measurement and field names (default: \"_\")\n#   # separator = \"_\"\n#\n#   ## number of objects to retrieve per query for realtime resources (vms and hosts)\n#   ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n#   # max_query_objects = 256\n#\n#   ## number of metrics to retrieve per query for non-realtime resources (clusters and datastores)\n#   ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n#   # max_query_metrics = 256\n#\n#   ## number of go routines to use for collection and discovery of objects and metrics\n#   # collect_concurrency = 1\n#   # discover_concurrency = 1\n#\n#   ## the interval before (re)discovering objects subject to metrics collection (default: 300s)\n#   # object_discovery_interval = \"300s\"\n#\n#   ## timeout applies to any of the api request made to vcenter\n#   # timeout = \"60s\"\n#\n#   ## When set to true, all samples are sent as integers. This makes the output\n#   ## data types backwards compatible with Telegraf 1.9 or lower. Normally all\n#   ## samples from vCenter, with the exception of percentages, are integer\n#   ## values, but under some conditions, some averaging takes place internally in\n#   ## the plugin. Setting this flag to \"false\" will send values as floats to\n#   ## preserve the full precision when averaging takes place.\n#   # use_int_samples = true\n#\n#   ## Custom attributes from vCenter can be very useful for queries in order to slice the\n#   ## metrics along different dimension and for forming ad-hoc relationships. They are disabled\n#   ## by default, since they can add a considerable amount of tags to the resulting metrics. To\n#   ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include\n#   ## to select the attributes you want to include.\n#   ## By default, since they can add a considerable amount of tags to the resulting metrics. To\n#   ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include\n#   ## to select the attributes you want to include.\n#   # custom_attribute_include = []\n#   # custom_attribute_exclude = [\"*\"]\n#\n#   ## The number of vSphere 5 minute metric collection cycles to look back for non-realtime metrics. In\n#   ## some versions (6.7, 7.0 and possible more), certain metrics, such as cluster metrics, may be reported\n#   ## with a significant delay (\u003e30min). If this happens, try increasing this number. Please note that increasing\n#   ## it too much may cause performance issues.\n#   # metric_lookback = 3\n#\n#   ## Optional SSL Config\n#   # ssl_ca = \"/path/to/cafile\"\n#   # ssl_cert = \"/path/to/certfile\"\n#   # ssl_key = \"/path/to/keyfile\"\n#   ## Use SSL but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## The Historical Interval value must match EXACTLY the interval in the daily\n#   # \"Interval Duration\" found on the VCenter server under Configure \u003e General \u003e Statistics \u003e Statistic intervals\n#   # historical_interval = \"5m\"\n\n\n# # A Webhooks Event collector\n# [[inputs.webhooks]]\n#   ## Address and port to host Webhook listener on\n#   service_address = \":1619\"\n#\n#   [inputs.webhooks.filestack]\n#     path = \"/filestack\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.github]\n#     path = \"/github\"\n#     # secret = \"\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.mandrill]\n#     path = \"/mandrill\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.rollbar]\n#     path = \"/rollbar\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.papertrail]\n#     path = \"/papertrail\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.particle]\n#     path = \"/particle\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n\n\n# # This plugin implements the Zipkin http server to gather trace and timing data needed to troubleshoot latency problems in microservice architectures.\n# [[inputs.zipkin]]\n#   # path = \"/api/v1/spans\" # URL path for span data\n#   # port = 9411            # Port on which Telegraf listens\n",
            "vars": {
              "DOCKER_INFLUXDB_HOSTNAME": "ec2-35-80-237-168.us-west-2.compute.amazonaws.com",
              "DOCKER_INFLUXDB_INIT_ADMIN_TOKEN": "39477970d8cb52692dcfc689f1a2614ac074ca94e5e231489fbcaa00ccac7763",
              "DOCKER_INFLUXDB_INIT_BUCKET": "telegraf",
              "DOCKER_INFLUXDB_INIT_ORG": "jon",
              "DOCKER_INFLUXDB_INIT_PORT": "8086"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.docker_composer",
      "mode": "managed",
      "type": "local_file",
      "name": "create_composer_environment",
      "provider": "provider[\"registry.terraform.io/hashicorp/local\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "content": "DOCKER_INFLUXDB_INIT_MODE=setup\n\n## Environment variables used during the setup and operation of the stack\n#\n\n# Primary InfluxDB admin/superuser credentials\n#\nDOCKER_INFLUXDB_INIT_USERNAME=jon\nDOCKER_INFLUXDB_INIT_PASSWORD=adminpwd\nDOCKER_INFLUXDB_INIT_ADMIN_TOKEN=39477970d8cb52692dcfc689f1a2614ac074ca94e5e231489fbcaa00ccac7763\n\n# Primary InfluxDB organization \u0026 bucket definitions\n# \nDOCKER_INFLUXDB_INIT_ORG=jon\nDOCKER_INFLUXDB_INIT_BUCKET=telegraf\n\n# Primary InfluxDB bucket retention period\n#\n# NOTE: Valid units are nanoseconds (ns), microseconds(us), milliseconds (ms)\n# seconds (s), minutes (m), hours (h), days (d), and weeks (w).\nDOCKER_INFLUXDB_INIT_RETENTION=4d\n\n\n# InfluxDB port \u0026 hostname definitions\n#\nDOCKER_INFLUXDB_INIT_PORT=8086\nDOCKER_INFLUXDB_INIT_HOST=influxdb\n\n# Telegraf configuration file\n# \n# Will be mounted to container and used as telegraf configuration\nTELEGRAF_CFG_PATH=./telegraf/telegraf.conf\n\n# Grafana port definition\nGRAFANA_PORT=3000\n",
            "content_base64": null,
            "directory_permission": "0777",
            "file_permission": "0777",
            "filename": "./install_tig/.env",
            "id": "c22ec2cb063c09fa36151236f5e586b94a55e49c",
            "sensitive_content": null,
            "source": null
          },
          "sensitive_attributes": [],
          "dependencies": [
            "module.docker_composer.data.template_file.init_compose_env"
          ]
        }
      ]
    },
    {
      "module": "module.docker_composer",
      "mode": "managed",
      "type": "local_file",
      "name": "create_telegraf_config",
      "provider": "provider[\"registry.terraform.io/hashicorp/local\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "content": "\n[global_tags]\n  # dc = \"us-east-1\" # will tag all metrics with dc=us-east-1\n  # rack = \"1a\"\n  ## Environment variables can be used as tags, and throughout the config file\n  # user = \"$USER\"\n\n\n# Configuration for telegraf agent\n[agent]\n  ## Default data collection interval for all inputs\n  interval = \"10s\"\n  ## Rounds collection interval to 'interval'\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\n  round_interval = true\n\n  ## Telegraf will send metrics to outputs in batches of at most\n  ## metric_batch_size metrics.\n  ## This controls the size of writes that Telegraf sends to output plugins.\n  metric_batch_size = 1000\n\n  ## Maximum number of unwritten metrics per output.  Increasing this value\n  ## allows for longer periods of output downtime without dropping metrics at the\n  ## cost of higher maximum memory usage.\n  metric_buffer_limit = 10000\n\n  ## Collection jitter is used to jitter the collection by a random amount.\n  ## Each plugin will sleep for a random time within jitter before collecting.\n  ## This can be used to avoid many plugins querying things like sysfs at the\n  ## same time, which can have a measurable effect on the system.\n  collection_jitter = \"0s\"\n\n  ## Collection offset is used to shift the collection by the given amount.\n  ## This can be be used to avoid many plugins querying constraint devices\n  ## at the same time by manually scheduling them in time.\n  # collection_offset = \"0s\"\n\n  ## Default flushing interval for all outputs. Maximum flush_interval will be\n  ## flush_interval + flush_jitter\n  flush_interval = \"10s\"\n  ## Jitter the flush interval by a random amount. This is primarily to avoid\n  ## large write spikes for users running a large number of telegraf instances.\n  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\n  flush_jitter = \"0s\"\n\n  ## Collected metrics are rounded to the precision specified. Precision is\n  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).\n  ## Valid time units are \"ns\", \"us\" (or \"Âµs\"), \"ms\", \"s\".\n  ##\n  ## By default or when set to \"0s\", precision will be set to the same\n  ## timestamp order as the collection interval, with the maximum being 1s:\n  ##   ie, when interval = \"10s\", precision will be \"1s\"\n  ##       when interval = \"250ms\", precision will be \"1ms\"\n  ##\n  ## Precision will NOT be used for service inputs. It is up to each individual\n  ## service input to set the timestamp at the appropriate precision.\n  precision = \"\"\n\n  ## Log at debug level.\n  # debug = false\n  ## Log only error level messages.\n  # quiet = false\n\n  ## Log target controls the destination for logs and can be one of \"file\",\n  ## \"stderr\" or, on Windows, \"eventlog\".  When set to \"file\", the output file\n  ## is determined by the \"logfile\" setting.\n  # logtarget = \"file\"\n\n  ## Name of the file to be logged to when using the \"file\" logtarget.  If set to\n  ## the empty string then logs are written to stderr.\n  # logfile = \"\"\n\n  ## The logfile will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.  Logs are rotated only when\n  ## written to, if there is no log activity rotation may be delayed.\n  # logfile_rotation_interval = \"0d\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # logfile_rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # logfile_rotation_max_archives = 5\n\n  ## Pick a timezone to use when logging or type 'local' for local time.\n  ## Example: America/Chicago\n  # log_with_timezone = \"\"\n\n  ## Override default hostname, if empty use os.Hostname()\n  hostname = \"\"\n  ## If set to true, do no set the \"host\" tag in the telegraf agent.\n  omit_hostname = false\n\n###############################################################################\n#                            OUTPUT PLUGINS                                   #\n###############################################################################\n\n\n# Configuration for sending metrics to InfluxDB\n# [[outputs.influxdb]]\n  ## The full HTTP or UDP URL for your InfluxDB instance.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  # urls = [\"unix:///var/run/influxdb.sock\"]\n  # urls = [\"udp://127.0.0.1:8089\"]\n  # urls = [\"http://127.0.0.1:8086\"]\n\n  ## The target database for metrics; will be created as needed.\n  ## For UDP url endpoint database needs to be configured on server side.\n  # database = \"telegraf\"\n\n  ## The value of this tag will be used to determine the database.  If this\n  ## tag is not set the 'database' option is used as the default.\n  # database_tag = \"\"\n\n  ## If true, the 'database_tag' will not be included in the written metric.\n  # exclude_database_tag = false\n\n  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using\n  ## Telegraf with a user without permissions to create databases or when the\n  ## database already exists.\n  # skip_database_creation = false\n\n  ## Name of existing retention policy to write to.  Empty string writes to\n  ## the default retention policy.  Only takes effect when using HTTP.\n  # retention_policy = \"\"\n\n  ## The value of this tag will be used to determine the retention policy.  If this\n  ## tag is not set the 'retention_policy' option is used as the default.\n  # retention_policy_tag = \"\"\n\n  ## If true, the 'retention_policy_tag' will not be included in the written metric.\n  # exclude_retention_policy_tag = false\n\n  ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\".\n  ## Only takes effect when using HTTP.\n  # write_consistency = \"any\"\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## HTTP Basic Auth\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## UDP payload size is the maximum packet size to send.\n  # udp_payload = \"512B\"\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: \"42u\".  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  # influx_uint_support = false\n\n\n# # Configuration for Amon Server to send metrics to.\n# [[outputs.amon]]\n#   ## Amon Server Key\n#   server_key = \"my-server-key\" # required.\n#\n#   ## Amon Instance URL\n#   amon_instance = \"https://youramoninstance\" # required\n#\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n\n\n# # Publishes metrics to an AMQP broker\n# [[outputs.amqp]]\n#   ## Brokers to publish to.  If multiple brokers are specified a random broker\n#   ## will be selected anytime a connection is established.  This can be\n#   ## helpful for load balancing when not using a dedicated load balancer.\n#   brokers = [\"amqp://localhost:5672/influxdb\"]\n#\n#   ## Maximum messages to send over a connection.  Once this is reached, the\n#   ## connection is closed and a new connection is made.  This can be helpful for\n#   ## load balancing when not using a dedicated load balancer.\n#   # max_messages = 0\n#\n#   ## Exchange to declare and publish to.\n#   exchange = \"telegraf\"\n#\n#   ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n#   # exchange_type = \"topic\"\n#\n#   ## If true, exchange will be passively declared.\n#   # exchange_passive = false\n#\n#   ## Exchange durability can be either \"transient\" or \"durable\".\n#   # exchange_durability = \"durable\"\n#\n#   ## Additional exchange arguments.\n#   # exchange_arguments = { }\n#   # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n#\n#   ## Authentication credentials for the PLAIN auth_method.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Auth method. PLAIN and EXTERNAL are supported\n#   ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n#   ## described here: https://www.rabbitmq.com/plugins.html\n#   # auth_method = \"PLAIN\"\n#\n#   ## Metric tag to use as a routing key.\n#   ##   ie, if this tag exists, its value will be used as the routing key\n#   # routing_tag = \"host\"\n#\n#   ## Static routing key.  Used when no routing_tag is set or as a fallback\n#   ## when the tag specified in routing tag is not found.\n#   # routing_key = \"\"\n#   # routing_key = \"telegraf\"\n#\n#   ## Delivery Mode controls if a published message is persistent.\n#   ##   One of \"transient\" or \"persistent\".\n#   # delivery_mode = \"transient\"\n#\n#   ## Static headers added to each published message.\n#   # headers = { }\n#   # headers = {\"database\" = \"telegraf\", \"retention_policy\" = \"default\"}\n#\n#   ## Connection timeout.  If not provided, will default to 5s.  0s means no\n#   ## timeout (not recommended).\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## If true use batch serialization format instead of line based delimiting.\n#   ## Only applies to data formats which are not line based such as JSON.\n#   ## Recommended to set to true.\n#   # use_batch_format = false\n#\n#   ## Content encoding for message payloads, can be set to \"gzip\" to or\n#   ## \"identity\" to apply no encoding.\n#   ##\n#   ## Please note that when use_batch_format = false each amqp message contains only\n#   ## a single metric, it is recommended to use compression with batch format\n#   ## for best results.\n#   # content_encoding = \"identity\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n\n\n# # Send metrics to Azure Application Insights\n# [[outputs.application_insights]]\n#   ## Instrumentation key of the Application Insights resource.\n#   instrumentation_key = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"\n#\n#   ## Regions that require endpoint modification https://docs.microsoft.com/en-us/azure/azure-monitor/app/custom-endpoints\n#   # endpoint_url = \"https://dc.services.visualstudio.com/v2/track\"\n#\n#   ## Timeout for closing (default: 5s).\n#   # timeout = \"5s\"\n#\n#   ## Enable additional diagnostic logging.\n#   # enable_diagnostic_logging = false\n#\n#   ## Context Tag Sources add Application Insights context tags to a tag value.\n#   ##\n#   ## For list of allowed context tag keys see:\n#   ## https://github.com/microsoft/ApplicationInsights-Go/blob/master/appinsights/contracts/contexttagkeys.go\n#   # [outputs.application_insights.context_tag_sources]\n#   #   \"ai.cloud.role\" = \"kubernetes_container_name\"\n#   #   \"ai.cloud.roleInstance\" = \"kubernetes_pod_name\"\n\n\n# # Sends metrics to Azure Data Explorer\n# [[outputs.azure_data_explorer]]\n#   ## Azure Data Explorer cluster endpoint\n#   ## ex: endpoint_url = \"https://clustername.australiasoutheast.kusto.windows.net\"\n#   endpoint_url = \"\"\n#\n#   ## The Azure Data Explorer database that the metrics will be ingested into.\n#   ## The plugin will NOT generate this database automatically, it's expected that this database already exists before ingestion.\n#   ## ex: \"exampledatabase\"\n#   database = \"\"\n#\n#   ## Timeout for Azure Data Explorer operations\n#   # timeout = \"20s\"\n#\n#   ## Type of metrics grouping used when pushing to Azure Data Explorer.\n#   ## Default is \"TablePerMetric\" for one table per different metric.\n#   ## For more information, please check the plugin README.\n#   # metrics_grouping_type = \"TablePerMetric\"\n#\n#   ## Name of the single table to store all the metrics (Only needed if metrics_grouping_type is \"SingleTable\").\n#   # table_name = \"\"\n#\n#   ## Creates tables and relevant mapping if set to true(default).\n#   ## Skips table and mapping creation if set to false, this is useful for running Telegraf with the lowest possible permissions i.e. table ingestor role.\n#   # create_tables = true\n\n\n# # Send aggregate metrics to Azure Monitor\n# [[outputs.azure_monitor]]\n#   ## Timeout for HTTP writes.\n#   # timeout = \"20s\"\n#\n#   ## Set the namespace prefix, defaults to \"Telegraf/\u003cinput-name\u003e\".\n#   # namespace_prefix = \"Telegraf/\"\n#\n#   ## Azure Monitor doesn't have a string value type, so convert string\n#   ## fields to dimensions (a.k.a. tags) if enabled. Azure Monitor allows\n#   ## a maximum of 10 dimensions so Telegraf will only send the first 10\n#   ## alphanumeric dimensions.\n#   # strings_as_dimensions = false\n#\n#   ## Both region and resource_id must be set or be available via the\n#   ## Instance Metadata service on Azure Virtual Machines.\n#   #\n#   ## Azure Region to publish metrics against.\n#   ##   ex: region = \"southcentralus\"\n#   # region = \"\"\n#   #\n#   ## The Azure Resource ID against which metric will be logged, e.g.\n#   ##   ex: resource_id = \"/subscriptions/\u003csubscription_id\u003e/resourceGroups/\u003cresource_group\u003e/providers/Microsoft.Compute/virtualMachines/\u003cvm_name\u003e\"\n#   # resource_id = \"\"\n#\n#   ## Optionally, if in Azure US Government, China or other sovereign\n#   ## cloud environment, set appropriate REST endpoint for receiving\n#   ## metrics. (Note: region may be unused in this context)\n#   # endpoint_url = \"https://monitoring.core.usgovcloudapi.net\"\n\n\n# # Configuration for Google Cloud BigQuery to send entries\n# [[outputs.bigquery]]\n#   ## Credentials File\n#   credentials_file = \"/path/to/service/account/key.json\"\n#\n#   ## Google Cloud Platform Project\n#   project = \"my-gcp-project\"\n#\n#   ## The namespace for the metric descriptor\n#   dataset = \"telegraf\"\n#\n#   ## Timeout for BigQuery operations.\n#   # timeout = \"5s\"\n#\n#   ## Character to replace hyphens on Metric name\n#   # replace_hyphen_to = \"_\"\n\n\n# # Publish Telegraf metrics to a Google Cloud PubSub topic\n# [[outputs.cloud_pubsub]]\n#   ## Required. Name of Google Cloud Platform (GCP) Project that owns\n#   ## the given PubSub topic.\n#   project = \"my-project\"\n#\n#   ## Required. Name of PubSub topic to publish metrics to.\n#   topic = \"my-topic\"\n#\n#   ## Required. Data format to consume.\n#   ## Each data format has its own unique set of configuration options.\n#   ## Read more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n#   ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n#   ## Application Default Credentials, which is preferred.\n#   # credentials_file = \"path/to/my/creds.json\"\n#\n#   ## Optional. If true, will send all metrics per write in one PubSub message.\n#   # send_batched = true\n#\n#   ## The following publish_* parameters specifically configures batching\n#   ## requests made to the GCP Cloud PubSub API via the PubSub Golang library. Read\n#   ## more here: https://godoc.org/cloud.google.com/go/pubsub#PublishSettings\n#\n#   ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n#   ## when it has this many PubSub messages. If send_batched is true,\n#   ## this is ignored and treated as if it were 1.\n#   # publish_count_threshold = 1000\n#\n#   ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n#   ## when it has this many PubSub messages. If send_batched is true,\n#   ## this is ignored and treated as if it were 1\n#   # publish_byte_threshold = 1000000\n#\n#   ## Optional. Specifically configures requests made to the PubSub API.\n#   # publish_num_go_routines = 2\n#\n#   ## Optional. Specifies a timeout for requests to the PubSub API.\n#   # publish_timeout = \"30s\"\n#\n#   ## Optional. If true, published PubSub message data will be base64-encoded.\n#   # base64_data = false\n#\n#   ## Optional. PubSub attributes to add to metrics.\n#   # [outputs.cloud_pubsub.attributes]\n#   #   my_attr = \"tag_value\"\n\n\n# # Configuration for AWS CloudWatch output.\n# [[outputs.cloudwatch]]\n#   ## Amazon REGION\n#   region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Namespace for the CloudWatch MetricDatums\n#   namespace = \"InfluxData/Telegraf\"\n#\n#   ## If you have a large amount of metrics, you should consider to send statistic\n#   ## values instead of raw metrics which could not only improve performance but\n#   ## also save AWS API cost. If enable this flag, this plugin would parse the required\n#   ## CloudWatch statistic fields (count, min, max, and sum) and send them to CloudWatch.\n#   ## You could use basicstats aggregator to calculate those fields. If not all statistic\n#   ## fields are available, all fields would still be sent as raw metrics.\n#   # write_statistics = false\n#\n#   ## Enable high resolution metrics of 1 second (if not enabled, standard resolution are of 60 seconds precision)\n#   # high_resolution_metrics = false\n\n\n# # Configuration for AWS CloudWatchLogs output.\n# [[outputs.cloudwatch_logs]]\n# ## The region is the Amazon region that you wish to connect to.\n# ## Examples include but are not limited to:\n# ## - us-west-1\n# ## - us-west-2\n# ## - us-east-1\n# ## - ap-southeast-1\n# ## - ap-southeast-2\n# ## ...\n# region = \"us-east-1\"\n#\n# ## Amazon Credentials\n# ## Credentials are loaded in the following order\n# ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n# ## 2) Assumed credentials via STS if role_arn is specified\n# ## 3) explicit credentials from 'access_key' and 'secret_key'\n# ## 4) shared profile from 'profile'\n# ## 5) environment variables\n# ## 6) shared credentials file\n# ## 7) EC2 Instance Profile\n# #access_key = \"\"\n# #secret_key = \"\"\n# #token = \"\"\n# #role_arn = \"\"\n# #web_identity_token_file = \"\"\n# #role_session_name = \"\"\n# #profile = \"\"\n# #shared_credential_file = \"\"\n#\n# ## Endpoint to make request against, the correct endpoint is automatically\n# ## determined and this option should only be set if you wish to override the\n# ## default.\n# ##   ex: endpoint_url = \"http://localhost:8000\"\n# # endpoint_url = \"\"\n#\n# ## Cloud watch log group. Must be created in AWS cloudwatch logs upfront!\n# ## For example, you can specify the name of the k8s cluster here to group logs from all cluster in oine place\n# log_group = \"my-group-name\"\n#\n# ## Log stream in log group\n# ## Either log group name or reference to metric attribute, from which it can be parsed:\n# ## tag:\u003cTAG_NAME\u003e or field:\u003cFIELD_NAME\u003e. If log stream is not exist, it will be created.\n# ## Since AWS is not automatically delete logs streams with expired logs entries (i.e. empty log stream)\n# ## you need to put in place appropriate house-keeping (https://forums.aws.amazon.com/thread.jspa?threadID=178855)\n# log_stream = \"tag:location\"\n#\n# ## Source of log data - metric name\n# ## specify the name of the metric, from which the log data should be retrieved.\n# ## I.e., if you  are using docker_log plugin to stream logs from container, then\n# ## specify log_data_metric_name  = \"docker_log\"\n# log_data_metric_name  = \"docker_log\"\n#\n# ## Specify from which metric attribute the log data should be retrieved:\n# ## tag:\u003cTAG_NAME\u003e or field:\u003cFIELD_NAME\u003e.\n# ## I.e., if you  are using docker_log plugin to stream logs from container, then\n# ## specify log_data_source  = \"field:message\"\n# log_data_source  = \"field:message\"\n\n\n# # Configuration for CrateDB to send metrics to.\n# [[outputs.cratedb]]\n#   # A github.com/jackc/pgx/v4 connection string.\n#   # See https://pkg.go.dev/github.com/jackc/pgx/v4#ParseConfig\n#   url = \"postgres://user:password@localhost/schema?sslmode=disable\"\n#   # Timeout for all CrateDB queries.\n#   timeout = \"5s\"\n#   # Name of the table to store metrics in.\n#   table = \"metrics\"\n#   # If true, and the metrics table does not exist, create it automatically.\n#   table_create = true\n#   # The character(s) to replace any '.' in an object key with\n#   key_separator = \"_\"\n\n\n# # Configuration for DataDog API to send metrics to.\n# [[outputs.datadog]]\n#   ## Datadog API key\n#   apikey = \"my-secret-key\"\n#\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n#\n#   ## Write URL override; useful for debugging.\n#   # url = \"https://app.datadoghq.com/api/v1/series\"\n#\n#   ## Set http_proxy (telegraf uses the system wide proxy settings if it isn't set)\n#   # http_proxy_url = \"http://localhost:8888\"\n#\n#   ## Override the default (none) compression used to send data.\n#   ## Supports: \"zlib\", \"none\"\n#   # compression = \"none\"\n\n\n# # Send metrics to nowhere at all\n# [[outputs.discard]]\n#   # no configuration\n\n\n# # Send telegraf metrics to a Dynatrace environment\n# [[outputs.dynatrace]]\n#   ## For usage with the Dynatrace OneAgent you can omit any configuration,\n#   ## the only requirement is that the OneAgent is running on the same host.\n#   ## Only setup environment url and token if you want to monitor a Host without the OneAgent present.\n#   ##\n#   ## Your Dynatrace environment URL.\n#   ## For Dynatrace OneAgent you can leave this empty or set it to \"http://127.0.0.1:14499/metrics/ingest\" (default)\n#   ## For Dynatrace SaaS environments the URL scheme is \"https://{your-environment-id}.live.dynatrace.com/api/v2/metrics/ingest\"\n#   ## For Dynatrace Managed environments the URL scheme is \"https://{your-domain}/e/{your-environment-id}/api/v2/metrics/ingest\"\n#   url = \"\"\n#\n#   ## Your Dynatrace API token.\n#   ## Create an API token within your Dynatrace environment, by navigating to Settings \u003e Integration \u003e Dynatrace API\n#   ## The API token needs data ingest scope permission. When using OneAgent, no API token is required.\n#   api_token = \"\"\n#\n#   ## Optional prefix for metric names (e.g.: \"telegraf\")\n#   prefix = \"telegraf\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional flag for ignoring tls certificate check\n#   # insecure_skip_verify = false\n#\n#\n#   ## Connection timeout, defaults to \"5s\" if not set.\n#   timeout = \"5s\"\n#\n#   ## If you want metrics to be treated and reported as delta counters, add the metric names here\n#   additional_counters = [ ]\n#\n#   ## Optional dimensions to be added to every metric\n#   # [outputs.dynatrace.default_dimensions]\n#   # default_key = \"default value\"\n\n\n# # Configuration for Elasticsearch to send metrics to.\n# [[outputs.elasticsearch]]\n#   ## The full HTTP endpoint URL for your Elasticsearch instance\n#   ## Multiple urls can be specified as part of the same cluster,\n#   ## this means that only ONE of the urls will be written to each interval.\n#   urls = [ \"http://node1.es.example.com:9200\" ] # required.\n#   ## Elasticsearch client timeout, defaults to \"5s\" if not set.\n#   timeout = \"5s\"\n#   ## Set to true to ask Elasticsearch a list of all cluster nodes,\n#   ## thus it is not necessary to list all nodes in the urls config option.\n#   enable_sniffer = false\n#   ## Set to true to enable gzip compression\n#   enable_gzip = false\n#   ## Set the interval to check if the Elasticsearch nodes are available\n#   ## Setting to \"0s\" will disable the health check (not recommended in production)\n#   health_check_interval = \"10s\"\n#   ## HTTP basic authentication details\n#   # username = \"telegraf\"\n#   # password = \"mypassword\"\n#   ## HTTP bearer token authentication details\n#   # auth_bearer_token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9\"\n#\n#   ## Index Config\n#   ## The target index for metrics (Elasticsearch will create if it not exists).\n#   ## You can use the date specifiers below to create indexes per time frame.\n#   ## The metric timestamp will be used to decide the destination index name\n#   # %Y - year (2016)\n#   # %y - last two digits of year (00..99)\n#   # %m - month (01..12)\n#   # %d - day of month (e.g., 01)\n#   # %H - hour (00..23)\n#   # %V - week of the year (ISO week) (01..53)\n#   ## Additionally, you can specify a tag name using the notation {{tag_name}}\n#   ## which will be used as part of the index name. If the tag does not exist,\n#   ## the default tag value will be used.\n#   # index_name = \"telegraf-{{host}}-%Y.%m.%d\"\n#   # default_tag_value = \"none\"\n#   index_name = \"telegraf-%Y.%m.%d\" # required.\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Template Config\n#   ## Set to true if you want telegraf to manage its index template.\n#   ## If enabled it will create a recommended index template for telegraf indexes\n#   manage_template = true\n#   ## The template name used for telegraf indexes\n#   template_name = \"telegraf\"\n#   ## Set to true if you want telegraf to overwrite an existing template\n#   overwrite_template = false\n#   ## If set to true a unique ID hash will be sent as sha256(concat(timestamp,measurement,series-hash)) string\n#   ## it will enable data resend and update metric points avoiding duplicated metrics with diferent id's\n#   force_document_id = false\n#\n#   ## Specifies the handling of NaN and Inf values.\n#   ## This option can have the following values:\n#   ##    none    -- do not modify field-values (default); will produce an error if NaNs or infs are encountered\n#   ##    drop    -- drop fields containing NaNs or infs\n#   ##    replace -- replace with the value in \"float_replacement_value\" (default: 0.0)\n#   ##               NaNs and inf will be replaced with the given number, -inf with the negative of that number\n#   # float_handling = \"none\"\n#   # float_replacement_value = 0.0\n#\n#   ## Pipeline Config\n#   ## To use a ingest pipeline, set this to the name of the pipeline you want to use.\n#   # use_pipeline = \"my_pipeline\"\n#   ## Additionally, you can specify a tag name using the notation {{tag_name}}\n#   ## which will be used as part of the pipeline name. If the tag does not exist,\n#   ## the default pipeline will be used as the pipeline. If no default pipeline is set,\n#   ## no pipeline is used for the metric.\n#   # use_pipeline = \"{{es_pipeline}}\"\n#   # default_pipeline = \"my_pipeline\"\n\n\n# # Configuration for Event Hubs output plugin\n# [[outputs.event_hubs]]\n#   ## The full connection string to the Event Hub (required)\n#   ## The shared access key must have \"Send\" permissions on the target Event Hub.\n#   connection_string = \"Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName\"\n#\n#   ## Client timeout (defaults to 30s)\n#   # timeout = \"30s\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"json\"\n\n\n# # Send metrics to command as input over stdin\n# [[outputs.exec]]\n#   ## Command to ingest metrics via stdin.\n#   command = [\"tee\", \"-a\", \"/dev/null\"]\n#\n#   ## Timeout for command to complete.\n#   # timeout = \"5s\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n\n\n# # Run executable as long-running output plugin\n# [[outputs.execd]]\n#   ## Program to run as daemon\n#   command = [\"my-telegraf-output\", \"--some-flag\", \"value\"]\n#\n#   ## Delay before the process is restarted after an unexpected termination\n#   restart_delay = \"10s\"\n#\n#   ## Data format to export.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send telegraf metrics to file(s)\n# [[outputs.file]]\n#   ## Files to write to, \"stdout\" is a specially handled file.\n#   files = [\"stdout\", \"/tmp/metrics.out\"]\n#\n#   ## Use batch serialization format instead of line based delimiting.  The\n#   ## batch format allows for the production of non line based output formats and\n#   ## may more efficiently encode metric groups.\n#   # use_batch_format = false\n#\n#   ## The file will be rotated after the time interval specified.  When set\n#   ## to 0 no time based rotation is performed.\n#   # rotation_interval = \"0d\"\n#\n#   ## The logfile will be rotated when it becomes larger than the specified\n#   ## size.  When set to 0 no size based rotation is performed.\n#   # rotation_max_size = \"0MB\"\n#\n#   ## Maximum number of rotated archives to keep, any older logs are deleted.\n#   ## If set to -1, no archives are removed.\n#   # rotation_max_archives = 5\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Configuration for Graphite server to send metrics to\n# [[outputs.graphite]]\n#   ## TCP endpoint for your graphite instance.\n#   ## If multiple endpoints are configured, output will be load balanced.\n#   ## Only one of the endpoints will be written to with each iteration.\n#   servers = [\"localhost:2003\"]\n#   ## Prefix metrics name\n#   prefix = \"\"\n#   ## Graphite output template\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   template = \"host.tags.measurement.field\"\n#\n#   ## Enable Graphite tags support\n#   # graphite_tag_support = false\n#\n#   ## Define how metric names and tags are sanitized; options are \"strict\", or \"compatible\"\n#   ## strict - Default method, and backwards compatible with previous versionf of Telegraf\n#   ## compatible - More relaxed sanitizing when using tags, and compatible with the graphite spec\n#   # graphite_tag_sanitize_mode = \"strict\"\n#\n#   ## Character for separating metric name and field for Graphite tags\n#   # graphite_separator = \".\"\n#\n#   ## Graphite templates patterns\n#   ## 1. Template for cpu\n#   ## 2. Template for disk*\n#   ## 3. Default template\n#   # templates = [\n#   #  \"cpu tags.measurement.host.field\",\n#   #  \"disk* measurement.field\",\n#   #  \"host.measurement.tags.field\"\n#   #]\n#\n#   ## timeout in seconds for the write connection to graphite\n#   timeout = 2\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Send telegraf metrics to graylog\n# [[outputs.graylog]]\n#   ## Endpoints for your graylog instances.\n#   servers = [\"udp://127.0.0.1:12201\"]\n#\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n#\n#   ## The field to use as the GELF short_message, if unset the static string\n#   ## \"telegraf\" will be used.\n#   ##   example: short_message_field = \"message\"\n#   # short_message_field = \"\"\n#\n#   ## According to GELF payload specification, additional fields names must be prefixed\n#   ## with an underscore. Previous versions did not prefix custom field 'name' with underscore.\n#   ## Set to true for backward compatibility.\n#   # name_field_no_prefix = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Send telegraf metrics to GroundWork Monitor\n# [[outputs.groundwork]]\n#   ## URL of your groundwork instance.\n#   url = \"https://groundwork.example.com\"\n#\n#   ## Agent uuid for GroundWork API Server.\n#   agent_id = \"\"\n#\n#   ## Username and password to access GroundWork API.\n#   username = \"\"\n#   password = \"\"\n#\n#   ## Default display name for the host with services(metrics).\n#   # default_host = \"telegraf\"\n#\n#   ## Default service state.\n#   # default_service_state = \"SERVICE_OK\"\n#\n#   ## The name of the tag that contains the hostname.\n#   # resource_tag = \"host\"\n#\n#   ## The name of the tag that contains the host group name.\n#   # group_tag = \"group\"\n\n\n# # Configurable HTTP health check resource based on metrics\n# [[outputs.health]]\n#   ## Address and port to listen on.\n#   ##   ex: service_address = \"http://localhost:8080\"\n#   ##       service_address = \"unix:///var/run/telegraf-health.sock\"\n#   # service_address = \"http://:8080\"\n#\n#   ## The maximum duration for reading the entire request.\n#   # read_timeout = \"5s\"\n#   ## The maximum duration for writing the entire response.\n#   # write_timeout = \"5s\"\n#\n#   ## Username and password to accept for HTTP basic authentication.\n#   # basic_username = \"user1\"\n#   # basic_password = \"secret\"\n#\n#   ## Allowed CA certificates for client certificates.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## TLS server certificate and private key.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## One or more check sub-tables should be defined, it is also recommended to\n#   ## use metric filtering to limit the metrics that flow into this output.\n#   ##\n#   ## When using the default buffer sizes, this example will fail when the\n#   ## metric buffer is half full.\n#   ##\n#   ## namepass = [\"internal_write\"]\n#   ## tagpass = { output = [\"influxdb\"] }\n#   ##\n#   ## [[outputs.health.compares]]\n#   ##   field = \"buffer_size\"\n#   ##   lt = 5000.0\n#   ##\n#   ## [[outputs.health.contains]]\n#   ##   field = \"buffer_size\"\n\n\n# # A plugin that can transmit metrics over HTTP\n# [[outputs.http]]\n#   ## URL is the address to send metrics to\n#   url = \"http://127.0.0.1:8080/telegraf\"\n#\n#   ## Timeout for HTTP message\n#   # timeout = \"5s\"\n#\n#   ## HTTP method, one of: \"POST\" or \"PUT\"\n#   # method = \"POST\"\n#\n#   ## HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## OAuth2 Client Credentials Grant\n#   # client_id = \"clientid\"\n#   # client_secret = \"secret\"\n#   # token_url = \"https://indentityprovider/oauth2/v1/token\"\n#   # scopes = [\"urn:opc:idm:__myscopes__\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional Cookie authentication\n#   # cookie_auth_url = \"https://localhost/authMe\"\n#   # cookie_auth_method = \"POST\"\n#   # cookie_auth_username = \"username\"\n#   # cookie_auth_password = \"pa$$word\"\n#   # cookie_auth_headers = '{\"Content-Type\": \"application/json\", \"X-MY-HEADER\":\"hello\"}'\n#   # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n#   ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n#   # cookie_auth_renewal = \"5m\"\n#\n#   ## Data format to output.\n#   ## Each data format has it's own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n#\n#   ## Use batch serialization format (default) instead of line based format.\n#   ## Batch format is more efficient and should be used unless line based\n#   ## format is really needed.\n#   # use_batch_format = true\n#\n#   ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## Additional HTTP headers\n#   # [outputs.http.headers]\n#   #   # Should be set manually to \"application/json\" for json data_format\n#   #   Content-Type = \"text/plain; charset=utf-8\"\n#\n#   ## Idle (keep-alive) connection timeout.\n#   ## Maximum amount of time before idle connection is closed.\n#   ## Zero means no limit.\n#   # idle_conn_timeout = 0\n#\n#   ## Amazon Region\n#   #region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n\n\n# # Configuration for sending metrics to InfluxDB\n[[outputs.influxdb_v2]]\n  ## The URLs of the InfluxDB cluster nodes.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  urls = [\"http://ec2-35-80-237-168.us-west-2.compute.amazonaws.com:8086\"]\n\n  ## Token for authentication.\n  token = \"39477970d8cb52692dcfc689f1a2614ac074ca94e5e231489fbcaa00ccac7763\"\n\n  ## Organization is the name of the organization you wish to write to; must exist.\n  organization = \"jon\"\n\n  ## Destination bucket to write into.\n  bucket = \"telegraf\"\n\n#   ## The value of this tag will be used to determine the bucket.  If this\n#   ## tag is not set the 'bucket' option is used as the default.\n#   # bucket_tag = \"\"\n#\n#   ## If true, the bucket tag will not be added to the metric.\n#   # exclude_bucket_tag = false\n#\n#   ## Timeout for HTTP messages.\n#   # timeout = \"5s\"\n#\n#   ## Additional HTTP headers\n#   # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## HTTP Proxy override, if unset values the standard proxy environment\n#   ## variables are consulted to determine which proxy, if any, should be used.\n#   # http_proxy = \"http://corporate.proxy:3128\"\n#\n#   ## HTTP User-Agent\n#   # user_agent = \"telegraf\"\n#\n#   ## Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"gzip\"\n#\n#   ## Enable or disable uint support for writing uints influxdb 2.0.\n#   # influx_uint_support = false\n#\n#   ## Optional TLS Config for use on HTTP connections.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  insecure_skip_verify = false\n\n\n# # Configuration for sending metrics to an Instrumental project\n# [[outputs.instrumental]]\n#   ## Project API Token (required)\n#   api_token = \"API Token\" # required\n#   ## Prefix the metrics with a given name\n#   prefix = \"\"\n#   ## Stats output template (Graphite formatting)\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n#   template = \"host.tags.measurement.field\"\n#   ## Timeout in seconds to connect\n#   timeout = \"2s\"\n#   ## Display Communication to Instrumental\n#   debug = false\n\n\n# # Configuration for the Kafka server to send metrics to\n# [[outputs.kafka]]\n#   ## URLs of kafka brokers\n#   brokers = [\"localhost:9092\"]\n#   ## Kafka topic for producer messages\n#   topic = \"telegraf\"\n#\n#   ## The value of this tag will be used as the topic.  If not set the 'topic'\n#   ## option is used.\n#   # topic_tag = \"\"\n#\n#   ## If true, the 'topic_tag' will be removed from to the metric.\n#   # exclude_topic_tag = false\n#\n#   ## Optional Client id\n#   # client_id = \"Telegraf\"\n#\n#   ## Set the minimal supported Kafka version.  Setting this enables the use of new\n#   ## Kafka features and APIs.  Of particular interest, lz4 compression\n#   ## requires at least version 0.10.0.0.\n#   ##   ex: version = \"1.1.0\"\n#   # version = \"\"\n#\n#   ## Optional topic suffix configuration.\n#   ## If the section is omitted, no suffix is used.\n#   ## Following topic suffix methods are supported:\n#   ##   measurement - suffix equals to separator + measurement's name\n#   ##   tags        - suffix equals to separator + specified tags' values\n#   ##                 interleaved with separator\n#\n#   ## Suffix equals to \"_\" + measurement name\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"measurement\"\n#   #   separator = \"_\"\n#\n#   ## Suffix equals to \"__\" + measurement's \"foo\" tag value.\n#   ##   If there's no such a tag, suffix equals to an empty string\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"tags\"\n#   #   keys = [\"foo\"]\n#   #   separator = \"__\"\n#\n#   ## Suffix equals to \"_\" + measurement's \"foo\" and \"bar\"\n#   ##   tag values, separated by \"_\". If there is no such tags,\n#   ##   their values treated as empty strings.\n#   # [outputs.kafka.topic_suffix]\n#   #   method = \"tags\"\n#   #   keys = [\"foo\", \"bar\"]\n#   #   separator = \"_\"\n#\n#   ## The routing tag specifies a tagkey on the metric whose value is used as\n#   ## the message key.  The message key is used to determine which partition to\n#   ## send the message to.  This tag is prefered over the routing_key option.\n#   routing_tag = \"host\"\n#\n#   ## The routing key is set as the message key and used to determine which\n#   ## partition to send the message to.  This value is only used when no\n#   ## routing_tag is set or as a fallback when the tag specified in routing tag\n#   ## is not found.\n#   ##\n#   ## If set to \"random\", a random value will be generated for each message.\n#   ##\n#   ## When unset, no message key is added and each message is routed to a random\n#   ## partition.\n#   ##\n#   ##   ex: routing_key = \"random\"\n#   ##       routing_key = \"telegraf\"\n#   # routing_key = \"\"\n#\n#   ## Compression codec represents the various compression codecs recognized by\n#   ## Kafka in messages.\n#   ##  0 : None\n#   ##  1 : Gzip\n#   ##  2 : Snappy\n#   ##  3 : LZ4\n#   ##  4 : ZSTD\n#   # compression_codec = 0\n#\n#   ## Idempotent Writes\n#   ## If enabled, exactly one copy of each message is written.\n#   # idempotent_writes = false\n#\n#   ##  RequiredAcks is used in Produce Requests to tell the broker how many\n#   ##  replica acknowledgements it must see before responding\n#   ##   0 : the producer never waits for an acknowledgement from the broker.\n#   ##       This option provides the lowest latency but the weakest durability\n#   ##       guarantees (some data will be lost when a server fails).\n#   ##   1 : the producer gets an acknowledgement after the leader replica has\n#   ##       received the data. This option provides better durability as the\n#   ##       client waits until the server acknowledges the request as successful\n#   ##       (only messages that were written to the now-dead leader but not yet\n#   ##       replicated will be lost).\n#   ##   -1: the producer gets an acknowledgement after all in-sync replicas have\n#   ##       received the data. This option provides the best durability, we\n#   ##       guarantee that no messages will be lost as long as at least one in\n#   ##       sync replica remains.\n#   # required_acks = -1\n#\n#   ## The maximum number of times to retry sending a metric before failing\n#   ## until the next flush.\n#   # max_retry = 3\n#\n#   ## The maximum permitted size of a message. Should be set equal to or\n#   ## smaller than the broker's 'message.max.bytes'.\n#   # max_message_bytes = 1000000\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional SOCKS5 proxy to use when connecting to brokers\n#   # socks5_enabled = true\n#   # socks5_address = \"127.0.0.1:1080\"\n#   # socks5_username = \"alice\"\n#   # socks5_password = \"pass123\"\n#\n#   ## Optional SASL Config\n#   # sasl_username = \"kafka\"\n#   # sasl_password = \"secret\"\n#\n#   ## Optional SASL:\n#   ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n#   ## (defaults to PLAIN)\n#   # sasl_mechanism = \"\"\n#\n#   ## used if sasl_mechanism is GSSAPI (experimental)\n#   # sasl_gssapi_service_name = \"\"\n#   # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n#   # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n#   # sasl_gssapi_kerberos_config_path = \"/\"\n#   # sasl_gssapi_realm = \"realm\"\n#   # sasl_gssapi_key_tab_path = \"\"\n#   # sasl_gssapi_disable_pafxfast = false\n#\n#   ## used if sasl_mechanism is OAUTHBEARER (experimental)\n#   # sasl_access_token = \"\"\n#\n#   ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n#   # sasl_version = 1\n#\n#   # Disable Kafka metadata full fetch\n#   # metadata_full = false\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n\n\n# # Configuration for the AWS Kinesis output.\n# [[outputs.kinesis]]\n#   ## Amazon REGION of kinesis endpoint.\n#   region = \"ap-southeast-2\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Kinesis StreamName must exist prior to starting telegraf.\n#   streamname = \"StreamName\"\n#\n#   ## The partition key can be calculated using one of several methods:\n#   ##\n#   ## Use a static value for all writes:\n#   #  [outputs.kinesis.partition]\n#   #    method = \"static\"\n#   #    key = \"howdy\"\n#   #\n#   ## Use a random partition key on each write:\n#   #  [outputs.kinesis.partition]\n#   #    method = \"random\"\n#   #\n#   ## Use the measurement name as the partition key:\n#   #  [outputs.kinesis.partition]\n#   #    method = \"measurement\"\n#   #\n#   ## Use the value of a tag for all writes, if the tag is not set the empty\n#   ## default option will be used. When no default, defaults to \"telegraf\"\n#   #  [outputs.kinesis.partition]\n#   #    method = \"tag\"\n#   #    key = \"host\"\n#   #    default = \"mykey\"\n#\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n#\n#   ## debug will show upstream aws messages.\n#   debug = false\n\n\n# # Configuration for Librato API to send metrics to.\n# [[outputs.librato]]\n#   ## Librato API Docs\n#   ## http://dev.librato.com/v1/metrics-authentication\n#   ## Librato API user\n#   api_user = \"telegraf@influxdb.com\" # required.\n#   ## Librato API token\n#   api_token = \"my-secret-token\" # required.\n#   ## Debug\n#   # debug = false\n#   ## Connection timeout.\n#   # timeout = \"5s\"\n#   ## Output source Template (same as graphite buckets)\n#   ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n#   ## This template is used in librato's source (not metric's name)\n#   template = \"host\"\n#\n\n\n# # Send aggregate metrics to Logz.io\n# [[outputs.logzio]]\n#   ## Connection timeout, defaults to \"5s\" if not set.\n#   timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Logz.io account token\n#   token = \"your logz.io token\" # required\n#\n#   ## Use your listener URL for your Logz.io account region.\n#   # url = \"https://listener.logz.io:8071\"\n\n\n# # Send logs to Loki\n# [[outputs.loki]]\n#   ## The domain of Loki\n#   domain = \"https://loki.domain.tld\"\n#\n#   ## Endpoint to write api\n#   # endpoint = \"/loki/api/v1/push\"\n#\n#   ## Connection timeout, defaults to \"5s\" if not set.\n#   # timeout = \"5s\"\n#\n#   ## Basic auth credential\n#   # username = \"loki\"\n#   # password = \"pass\"\n#\n#   ## Additional HTTP headers\n#   # http_headers = {\"X-Scope-OrgID\" = \"1\"}\n#\n#   ## If the request must be gzip encoded\n#   # gzip_request = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n\n\n# # Sends metrics to MongoDB\n# [[outputs.mongodb]]\n#   # connection string examples for mongodb\n#   dsn = \"mongodb://localhost:27017\"\n#   # dsn = \"mongodb://mongod1:27017,mongod2:27017,mongod3:27017/admin\u0026replicaSet=myReplSet\u0026w=1\"\n#\n#   # overrides serverSelectionTimeoutMS in dsn if set\n#   # timeout = \"30s\"\n#\n#   # default authentication, optional\n#   # authentication = \"NONE\"\n#\n#   # for SCRAM-SHA-256 authentication\n#   # authentication = \"SCRAM\"\n#   # username = \"root\"\n#   # password = \"***\"\n#\n#   # for x509 certificate authentication\n#   # authentication = \"X509\"\n#   # tls_ca = \"ca.pem\"\n#   # tls_key = \"client.pem\"\n#   # # tls_key_pwd = \"changeme\" # required for encrypted tls_key\n#   # insecure_skip_verify = false\n#\n#   # database to store measurements and time series collections\n#   # database = \"telegraf\"\n#\n#   # granularity can be seconds, minutes, or hours.\n#   # configuring this value will be based on your input collection frequency.\n#   # see https://docs.mongodb.com/manual/core/timeseries-collections/#create-a-time-series-collection\n#   # granularity = \"seconds\"\n#\n#   # optionally set a TTL to automatically expire documents from the measurement collections.\n#   # ttl = \"360h\"\n\n\n# # Configuration for MQTT server to send metrics to\n# [[outputs.mqtt]]\n#   ## MQTT Brokers\n#   ## The list of brokers should only include the hostname or IP address and the\n#   ## port to the broker. This should follow the format '{host}:{port}'. For\n#   ## example, \"localhost:1883\" or \"127.0.0.1:8883\".\n#   servers = [\"localhost:1883\"]\n#\n#   ## MQTT Topic for Producer Messages\n#   ## MQTT outputs send metrics to this topic format:\n#   ## \u003ctopic_prefix\u003e/\u003chostname\u003e/\u003cpluginname\u003e/ (e.g. prefix/web01.example.com/mem)\n#   topic_prefix = \"telegraf\"\n#\n#   ## QoS policy for messages\n#   ## The mqtt QoS policy for sending messages.\n#   ## See https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.0.0/com.ibm.mq.dev.doc/q029090_.htm\n#   ##   0 = at most once\n#   ##   1 = at least once\n#   ##   2 = exactly once\n#   # qos = 2\n#\n#   ## Keep Alive\n#   ## Defines the maximum length of time that the broker and client may not\n#   ## communicate. Defaults to 0 which turns the feature off.\n#   ##\n#   ## For version v2.0.12 and later mosquitto there is a bug\n#   ## (see https://github.com/eclipse/mosquitto/issues/2117), which requires\n#   ## this to be non-zero. As a reference eclipse/paho.mqtt.golang defaults to 30.\n#   # keep_alive = 0\n#\n#   ## username and password to connect MQTT server.\n#   # username = \"telegraf\"\n#   # password = \"metricsmetricsmetricsmetrics\"\n#\n#   ## client ID\n#   ## The unique client id to connect MQTT server. If this parameter is not set\n#   ## then a random ID is generated.\n#   # client_id = \"\"\n#\n#   ## Timeout for write operations. default: 5s\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## When true, metrics will be sent in one MQTT message per flush. Otherwise,\n#   ## metrics are written one metric per MQTT message.\n#   # batch = false\n#\n#   ## When true, metric will have RETAIN flag set, making broker cache entries until someone\n#   ## actually reads it\n#   # retain = false\n#\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send telegraf measurements to NATS\n# [[outputs.nats]]\n#   ## URLs of NATS servers\n#   servers = [\"nats://localhost:4222\"]\n#\n#   ## Optional client name\n#   # name = \"\"\n#\n#   ## Optional credentials\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional NATS 2.0 and NATS NGS compatible user credentials\n#   # credentials = \"/etc/telegraf/nats.creds\"\n#\n#   ## NATS subject for producer messages\n#   subject = \"telegraf\"\n#\n#   ## Use Transport Layer Security\n#   # secure = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send metrics to New Relic metrics endpoint\n# [[outputs.newrelic]]\n#   ## The 'insights_key' parameter requires a NR license key.\n#   ## New Relic recommends you create one\n#   ## with a convenient name such as TELEGRAF_INSERT_KEY.\n#   ## reference: https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#ingest-license-key\n#   # insights_key = \"New Relic License Key Here\"\n#\n#   ## Prefix to add to add to metric name for easy identification.\n#   ## This is very useful if your metric names are ambiguous.\n#   # metric_prefix = \"\"\n#\n#   ## Timeout for writes to the New Relic API.\n#   # timeout = \"15s\"\n#\n#   ## HTTP Proxy override. If unset use values from the standard\n#   ## proxy environment variables to determine proxy, if any.\n#   # http_proxy = \"http://corporate.proxy:3128\"\n#\n#   ## Metric URL override to enable geographic location endpoints.\n#   # If not set use values from the standard\n#   # metric_url = \"https://metric-api.newrelic.com/metric/v1\"\n\n\n# # Send telegraf measurements to NSQD\n# [[outputs.nsq]]\n#   ## Location of nsqd instance listening on TCP\n#   server = \"localhost:4150\"\n#   ## NSQ topic for producer messages\n#   topic = \"telegraf\"\n#\n#   ## Data format to output.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   data_format = \"influx\"\n\n\n# # Send OpenTelemetry metrics over gRPC\n# [[outputs.opentelemetry]]\n#   ## Override the default (localhost:4317) OpenTelemetry gRPC service\n#   ## address:port\n#   # service_address = \"localhost:4317\"\n#\n#   ## Override the default (5s) request timeout\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config.\n#   ##\n#   ## Root certificates for verifying server certificates encoded in PEM format.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   ## The public and private keypairs for the client encoded in PEM format.\n#   ## May contain intermediate certificates.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS, but skip TLS chain and host verification.\n#   # insecure_skip_verify = false\n#   ## Send the specified TLS server name via SNI.\n#   # tls_server_name = \"foo.example.com\"\n#\n#   ## Override the default (gzip) compression used to send data.\n#   ## Supports: \"gzip\", \"none\"\n#   # compression = \"gzip\"\n#\n#   ## Additional OpenTelemetry resource attributes\n#   # [outputs.opentelemetry.attributes]\n#   # \"service.name\" = \"demo\"\n#\n#   ## Additional gRPC request metadata\n#   # [outputs.opentelemetry.headers]\n#   # key1 = \"value1\"\n\n\n# # Configuration for OpenTSDB server to send metrics to\n# [[outputs.opentsdb]]\n#   ## prefix for metrics keys\n#   prefix = \"my.specific.prefix.\"\n#\n#   ## DNS name of the OpenTSDB server\n#   ## Using \"opentsdb.example.com\" or \"tcp://opentsdb.example.com\" will use the\n#   ## telnet API. \"http://opentsdb.example.com\" will use the Http API.\n#   host = \"opentsdb.example.com\"\n#\n#   ## Port of the OpenTSDB server\n#   port = 4242\n#\n#   ## Number of data points to send to OpenTSDB in Http requests.\n#   ## Not used with telnet API.\n#   http_batch_size = 50\n#\n#   ## URI Path for Http requests to OpenTSDB.\n#   ## Used in cases where OpenTSDB is located behind a reverse proxy.\n#   http_path = \"/api/put\"\n#\n#   ## Debug true - Prints OpenTSDB communication\n#   debug = false\n#\n#   ## Separator separates measurement name from field\n#   separator = \"_\"\n\n\n# # Configuration for the Prometheus client to spawn\n# [[outputs.prometheus_client]]\n#   ## Address to listen on\n#   listen = \":9273\"\n#\n#   ## Metric version controls the mapping from Telegraf metrics into\n#   ## Prometheus format.  When using the prometheus input, use the same value in\n#   ## both plugins to ensure metrics are round-tripped without modification.\n#   ##\n#   ##   example: metric_version = 1;\n#   ##            metric_version = 2; recommended version\n#   # metric_version = 1\n#\n#   ## Use HTTP Basic Authentication.\n#   # basic_username = \"Foo\"\n#   # basic_password = \"Bar\"\n#\n#   ## If set, the IP Ranges which are allowed to access metrics.\n#   ##   ex: ip_range = [\"192.168.0.0/24\", \"192.168.1.0/30\"]\n#   # ip_range = []\n#\n#   ## Path to publish the metrics on.\n#   # path = \"/metrics\"\n#\n#   ## Expiration interval for each metric. 0 == no expiration\n#   # expiration_interval = \"60s\"\n#\n#   ## Collectors to enable, valid entries are \"gocollector\" and \"process\".\n#   ## If unset, both are enabled.\n#   # collectors_exclude = [\"gocollector\", \"process\"]\n#\n#   ## Send string metrics as Prometheus labels.\n#   ## Unless set to false all string metrics will be sent as labels.\n#   # string_as_label = true\n#\n#   ## If set, enable TLS with the given certificate.\n#   # tls_cert = \"/etc/ssl/telegraf.crt\"\n#   # tls_key = \"/etc/ssl/telegraf.key\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Export metric collection time.\n#   # export_timestamp = false\n\n\n# # Configuration for the Riemann server to send metrics to\n# [[outputs.riemann]]\n#   ## The full TCP or UDP URL of the Riemann server\n#   url = \"tcp://localhost:5555\"\n#\n#   ## Riemann event TTL, floating-point time in seconds.\n#   ## Defines how long that an event is considered valid for in Riemann\n#   # ttl = 30.0\n#\n#   ## Separator to use between measurement and field name in Riemann service name\n#   ## This does not have any effect if 'measurement_as_attribute' is set to 'true'\n#   separator = \"/\"\n#\n#   ## Set measurement name as Riemann attribute 'measurement', instead of prepending it to the Riemann service name\n#   # measurement_as_attribute = false\n#\n#   ## Send string metrics as Riemann event states.\n#   ## Unless enabled all string metrics will be ignored\n#   # string_as_state = false\n#\n#   ## A list of tag keys whose values get sent as Riemann tags.\n#   ## If empty, all Telegraf tag values will be sent as tags\n#   # tag_keys = [\"telegraf\",\"custom_tag\"]\n#\n#   ## Additional Riemann tags to send.\n#   # tags = [\"telegraf-output\"]\n#\n#   ## Description for Riemann event\n#   # description_text = \"metrics collected from telegraf\"\n#\n#   ## Riemann client write timeout, defaults to \"5s\" if not set.\n#   # timeout = \"5s\"\n\n\n# # Configuration for the Riemann server to send metrics to\n# [[outputs.riemann_legacy]]\n#   ## DEPRECATED: The 'riemann_legacy' plugin is deprecated in version 1.3.0, use 'outputs.riemann' instead (see https://github.com/influxdata/telegraf/issues/1878).\n#   ## URL of server\n#   url = \"localhost:5555\"\n#   ## transport protocol to use either tcp or udp\n#   transport = \"tcp\"\n#   ## separator to use between input name and field name in Riemann service name\n#   separator = \" \"\n\n\n# # Send aggregate metrics to Sensu Monitor\n# [[outputs.sensu]]\n#   ## BACKEND API URL is the Sensu Backend API root URL to send metrics to\n#   ## (protocol, host, and port only). The output plugin will automatically\n#   ## append the corresponding backend API path\n#   ## /api/core/v2/namespaces/:entity_namespace/events/:entity_name/:check_name).\n#   ##\n#   ## Backend Events API reference:\n#   ## https://docs.sensu.io/sensu-go/latest/api/events/\n#   ##\n#   ## AGENT API URL is the Sensu Agent API root URL to send metrics to\n#   ## (protocol, host, and port only). The output plugin will automatically\n#   ## append the correspeonding agent API path (/events).\n#   ##\n#   ## Agent API Events API reference:\n#   ## https://docs.sensu.io/sensu-go/latest/api/events/\n#   ##\n#   ## NOTE: if backend_api_url and agent_api_url and api_key are set, the output\n#   ## plugin will use backend_api_url. If backend_api_url and agent_api_url are\n#   ## not provided, the output plugin will default to use an agent_api_url of\n#   ## http://127.0.0.1:3031\n#   ##\n#   # backend_api_url = \"http://127.0.0.1:8080\"\n#   # agent_api_url = \"http://127.0.0.1:3031\"\n#\n#   ## API KEY is the Sensu Backend API token\n#   ## Generate a new API token via:\n#   ##\n#   ## $ sensuctl cluster-role create telegraf --verb create --resource events,entities\n#   ## $ sensuctl cluster-role-binding create telegraf --cluster-role telegraf --group telegraf\n#   ## $ sensuctl user create telegraf --group telegraf --password REDACTED\n#   ## $ sensuctl api-key grant telegraf\n#   ##\n#   ## For more information on Sensu RBAC profiles \u0026 API tokens, please visit:\n#   ## - https://docs.sensu.io/sensu-go/latest/reference/rbac/\n#   ## - https://docs.sensu.io/sensu-go/latest/reference/apikeys/\n#   ##\n#   # api_key = \"SENSU_API_KEY\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Timeout for HTTP message\n#   # timeout = \"5s\"\n#\n#   ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## Sensu Event details\n#   ##\n#   ## Below are the event details to be sent to Sensu.  The main portions of the\n#   ## event are the check, entity, and metrics specifications. For more information\n#   ## on Sensu events and its components, please visit:\n#   ## - Events - https://docs.sensu.io/sensu-go/latest/reference/events\n#   ## - Checks -  https://docs.sensu.io/sensu-go/latest/reference/checks\n#   ## - Entities - https://docs.sensu.io/sensu-go/latest/reference/entities\n#   ## - Metrics - https://docs.sensu.io/sensu-go/latest/reference/events#metrics\n#   ##\n#   ## Check specification\n#   ## The check name is the name to give the Sensu check associated with the event\n#   ## created. This maps to check.metatadata.name in the event.\n#   [outputs.sensu.check]\n#   name = \"telegraf\"\n#\n#   ## Entity specification\n#   ## Configure the entity name and namespace, if necessary. This will be part of\n#   ## the entity.metadata in the event.\n#   ##\n#   ## NOTE: if the output plugin is configured to send events to a\n#   ## backend_api_url and entity_name is not set, the value returned by\n#   ## os.Hostname() will be used; if the output plugin is configured to send\n#   ## events to an agent_api_url, entity_name and entity_namespace are not used.\n#   # [outputs.sensu.entity]\n#   #   name = \"server-01\"\n#   #   namespace = \"default\"\n#\n#   ## Metrics specification\n#   ## Configure the tags for the metrics that are sent as part of the Sensu event\n#   # [outputs.sensu.tags]\n#   #   source = \"telegraf\"\n#\n#   ## Configure the handler(s) for processing the provided metrics\n#   # [outputs.sensu.metrics]\n#   #   handlers = [\"influxdb\",\"elasticsearch\"]\n\n\n# # Send metrics and events to SignalFx\n# [[outputs.signalfx]]\n#     ## SignalFx Org Access Token\n#     access_token = \"my-secret-token\"\n#\n#     ## The SignalFx realm that your organization resides in\n#     signalfx_realm = \"us9\"  # Required if ingest_url is not set\n#\n#     ## You can optionally provide a custom ingest url instead of the\n#     ## signalfx_realm option above if you are using a gateway or proxy\n#     ## instance.  This option takes precident over signalfx_realm.\n#     ingest_url = \"https://my-custom-ingest/\"\n#\n#     ## Event typed metrics are omitted by default,\n#     ## If you require an event typed metric you must specify the\n#     ## metric name in the following list.\n#     included_event_names = [\"plugin.metric_name\"]\n\n\n# # Generic socket writer capable of handling multiple socket types.\n# [[outputs.socket_writer]]\n#   ## URL to connect to\n#   # address = \"tcp://127.0.0.1:8094\"\n#   # address = \"tcp://example.com:http\"\n#   # address = \"tcp4://127.0.0.1:8094\"\n#   # address = \"tcp6://127.0.0.1:8094\"\n#   # address = \"tcp6://[2001:db8::1]:8094\"\n#   # address = \"udp://127.0.0.1:8094\"\n#   # address = \"udp4://127.0.0.1:8094\"\n#   # address = \"udp6://127.0.0.1:8094\"\n#   # address = \"unix:///tmp/telegraf.sock\"\n#   # address = \"unixgram:///tmp/telegraf.sock\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Period between keep alive probes.\n#   ## Only applies to TCP sockets.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n#\n#   ## Content encoding for packet-based connections (i.e. UDP, unixgram).\n#   ## Can be set to \"gzip\" or to \"identity\" to apply no encoding.\n#   ##\n#   # content_encoding = \"identity\"\n#\n#   ## Data format to generate.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   # data_format = \"influx\"\n\n\n# # Send metrics to SQL Database\n# [[outputs.sql]]\n#   ## Database driver\n#   ## Valid options: mssql (Microsoft SQL Server), mysql (MySQL), pgx (Postgres),\n#   ##  sqlite (SQLite3), snowflake (snowflake.com) clickhouse (ClickHouse)\n#   # driver = \"\"\n#\n#   ## Data source name\n#   ## The format of the data source name is different for each database driver.\n#   ## See the plugin readme for details.\n#   # data_source_name = \"\"\n#\n#   ## Timestamp column name\n#   # timestamp_column = \"timestamp\"\n#\n#   ## Table creation template\n#   ## Available template variables:\n#   ##  {TABLE} - table name as a quoted identifier\n#   ##  {TABLELITERAL} - table name as a quoted string literal\n#   ##  {COLUMNS} - column definitions (list of quoted identifiers and types)\n#   # table_template = \"CREATE TABLE {TABLE}({COLUMNS})\"\n#\n#   ## Table existence check template\n#   ## Available template variables:\n#   ##  {TABLE} - tablename as a quoted identifier\n#   # table_exists_template = \"SELECT 1 FROM {TABLE} LIMIT 1\"\n#\n#   ## Initialization SQL\n#   # init_sql = \"\"\n#\n#   ## Metric type to SQL type conversion\n#   ## The values on the left are the data types Telegraf has and the values on\n#   ## the right are the data types Telegraf will use when sending to a database.\n#   ##\n#   ## The database values used must be data types the destination database\n#   ## understands. It is up to the user to ensure that the selected data type is\n#   ## available in the database they are using. Refer to your database\n#   ## documentation for what data types are available and supported.\n#   #[outputs.sql.convert]\n#   #  integer              = \"INT\"\n#   #  real                 = \"DOUBLE\"\n#   #  text                 = \"TEXT\"\n#   #  timestamp            = \"TIMESTAMP\"\n#   #  defaultvalue         = \"TEXT\"\n#   #  unsigned             = \"UNSIGNED\"\n#   #  bool                 = \"BOOL\"\n#\n#   ## This setting controls the behavior of the unsigned value. By default the\n#   ## setting will take the integer value and append the unsigned value to it. The other\n#   ## option is \"literal\", which will use the actual value the user provides to\n#   ## the unsigned option. This is useful for a database like ClickHouse where\n#   ## the unsigned value should use a value like \"uint64\".\n#   # conversion_style = \"unsigned_suffix\"\n\n\n# # Configuration for Google Cloud Stackdriver to send metrics to\n# [[outputs.stackdriver]]\n#   ## GCP Project\n#   project = \"erudite-bloom-151019\"\n#\n#   ## The namespace for the metric descriptor\n#   namespace = \"telegraf\"\n#\n#   ## Custom resource type\n#   # resource_type = \"generic_node\"\n#\n#   ## Additional resource labels\n#   # [outputs.stackdriver.resource_labels]\n#   #   node_id = \"$HOSTNAME\"\n#   #   namespace = \"myapp\"\n#   #   location = \"eu-north0\"\n\n\n# # A plugin that can transmit metrics to Sumo Logic HTTP Source\n# [[outputs.sumologic]]\n#   ## Unique URL generated for your HTTP Metrics Source.\n#   ## This is the address to send metrics to.\n#   # url = \"https://events.sumologic.net/receiver/v1/http/\u003cUniqueHTTPCollectorCode\u003e\"\n#\n#   ## Data format to be used for sending metrics.\n#   ## This will set the \"Content-Type\" header accordingly.\n#   ## Currently supported formats:\n#   ## * graphite - for Content-Type of application/vnd.sumologic.graphite\n#   ## * carbon2 - for Content-Type of application/vnd.sumologic.carbon2\n#   ## * prometheus - for Content-Type of application/vnd.sumologic.prometheus\n#   ##\n#   ## More information can be found at:\n#   ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#content-type-headers-for-metrics\n#   ##\n#   ## NOTE:\n#   ## When unset, telegraf will by default use the influx serializer which is currently unsupported\n#   ## in HTTP Source.\n#   data_format = \"carbon2\"\n#\n#   ## Timeout used for HTTP request\n#   # timeout = \"5s\"\n#\n#   ## Max HTTP request body size in bytes before compression (if applied).\n#   ## By default 1MB is recommended.\n#   ## NOTE:\n#   ## Bear in mind that in some serializer a metric even though serialized to multiple\n#   ## lines cannot be split any further so setting this very low might not work\n#   ## as expected.\n#   # max_request_body_size = 1000000\n#\n#   ## Additional, Sumo specific options.\n#   ## Full list can be found here:\n#   ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#supported-http-headers\n#\n#   ## Desired source name.\n#   ## Useful if you want to override the source name configured for the source.\n#   # source_name = \"\"\n#\n#   ## Desired host name.\n#   ## Useful if you want to override the source host configured for the source.\n#   # source_host = \"\"\n#\n#   ## Desired source category.\n#   ## Useful if you want to override the source category configured for the source.\n#   # source_category = \"\"\n#\n#   ## Comma-separated key=value list of dimensions to apply to every metric.\n#   ## Custom dimensions will allow you to query your metrics at a more granular level.\n#   # dimensions = \"\"\n\n\n# # Configuration for Syslog server to send metrics to\n# [[outputs.syslog]]\n#   ## URL to connect to\n#   ## ex: address = \"tcp://127.0.0.1:8094\"\n#   ## ex: address = \"tcp4://127.0.0.1:8094\"\n#   ## ex: address = \"tcp6://127.0.0.1:8094\"\n#   ## ex: address = \"tcp6://[2001:db8::1]:8094\"\n#   ## ex: address = \"udp://127.0.0.1:8094\"\n#   ## ex: address = \"udp4://127.0.0.1:8094\"\n#   ## ex: address = \"udp6://127.0.0.1:8094\"\n#   address = \"tcp://127.0.0.1:8094\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Period between keep alive probes.\n#   ## Only applies to TCP sockets.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n#\n#   ## The framing technique with which it is expected that messages are\n#   ## transported (default = \"octet-counting\").  Whether the messages come\n#   ## using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n#   ## or the non-transparent framing technique (RFC6587#section-3.4.2).  Must\n#   ## be one of \"octet-counting\", \"non-transparent\".\n#   # framing = \"octet-counting\"\n#\n#   ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n#   ## Must be one of \"LF\", or \"NUL\".\n#   # trailer = \"LF\"\n#\n#   ## SD-PARAMs settings\n#   ## Syslog messages can contain key/value pairs within zero or more\n#   ## structured data sections.  For each unrecognized metric tag/field a\n#   ## SD-PARAMS is created.\n#   ##\n#   ## Example:\n#   ##   [[outputs.syslog]]\n#   ##     sdparam_separator = \"_\"\n#   ##     default_sdid = \"default@32473\"\n#   ##     sdids = [\"foo@123\", \"bar@456\"]\n#   ##\n#   ##   input =\u003e xyzzy,x=y foo@123_value=42,bar@456_value2=84,something_else=1\n#   ##   output (structured data only) =\u003e [foo@123 value=42][bar@456 value2=84][default@32473 something_else=1 x=y]\n#\n#   ## SD-PARAMs separator between the sdid and tag/field key (default = \"_\")\n#   # sdparam_separator = \"_\"\n#\n#   ## Default sdid used for tags/fields that don't contain a prefix defined in\n#   ## the explicit sdids setting below If no default is specified, no SD-PARAMs\n#   ## will be used for unrecognized field.\n#   # default_sdid = \"default@32473\"\n#\n#   ## List of explicit prefixes to extract from tag/field keys and use as the\n#   ## SDID, if they match (see above example for more details):\n#   # sdids = [\"foo@123\", \"bar@456\"]\n#\n#   ## Default severity value. Severity and Facility are used to calculate the\n#   ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field\n#   ## with key \"severity_code\" is defined.  If unset, 5 (notice) is the default\n#   # default_severity_code = 5\n#\n#   ## Default facility value. Facility and Severity are used to calculate the\n#   ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field with\n#   ## key \"facility_code\" is defined.  If unset, 1 (user-level) is the default\n#   # default_facility_code = 1\n#\n#   ## Default APP-NAME value (RFC5424#section-6.2.5)\n#   ## Used when no metric tag with key \"appname\" is defined.\n#   ## If unset, \"Telegraf\" is the default\n#   # default_appname = \"Telegraf\"\n\n\n# # Configuration for Amazon Timestream output.\n# [[outputs.timestream]]\n#   ## Amazon Region\n#   region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order:\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   #access_key = \"\"\n#   #secret_key = \"\"\n#   #token = \"\"\n#   #role_arn = \"\"\n#   #web_identity_token_file = \"\"\n#   #role_session_name = \"\"\n#   #profile = \"\"\n#   #shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Timestream database where the metrics will be inserted.\n#   ## The database must exist prior to starting Telegraf.\n#   database_name = \"yourDatabaseNameHere\"\n#\n#   ## Specifies if the plugin should describe the Timestream database upon starting\n#   ## to validate if it has access necessary permissions, connection, etc., as a safety check.\n#   ## If the describe operation fails, the plugin will not start\n#   ## and therefore the Telegraf agent will not start.\n#   describe_database_on_start = false\n#\n#   ## The mapping mode specifies how Telegraf records are represented in Timestream.\n#   ## Valid values are: single-table, multi-table.\n#   ## For example, consider the following data in line protocol format:\n#   ## weather,location=us-midwest,season=summer temperature=82,humidity=71 1465839830100400200\n#   ## airquality,location=us-west no2=5,pm25=16 1465839830100400200\n#   ## where weather and airquality are the measurement names, location and season are tags,\n#   ## and temperature, humidity, no2, pm25 are fields.\n#   ## In multi-table mode:\n#   ##  - first line will be ingested to table named weather\n#   ##  - second line will be ingested to table named airquality\n#   ##  - the tags will be represented as dimensions\n#   ##  - first table (weather) will have two records:\n#   ##      one with measurement name equals to temperature,\n#   ##      another with measurement name equals to humidity\n#   ##  - second table (airquality) will have two records:\n#   ##      one with measurement name equals to no2,\n#   ##      another with measurement name equals to pm25\n#   ##  - the Timestream tables from the example will look like this:\n#   ##      TABLE \"weather\":\n#   ##        time | location | season | measure_name | measure_value::bigint\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | temperature | 82\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | humidity | 71\n#   ##      TABLE \"airquality\":\n#   ##        time | location | measure_name | measure_value::bigint\n#   ##        2016-06-13 17:43:50 | us-west | no2 | 5\n#   ##        2016-06-13 17:43:50 | us-west | pm25 | 16\n#   ## In single-table mode:\n#   ##  - the data will be ingested to a single table, which name will be valueOf(single_table_name)\n#   ##  - measurement name will stored in dimension named valueOf(single_table_dimension_name_for_telegraf_measurement_name)\n#   ##  - location and season will be represented as dimensions\n#   ##  - temperature, humidity, no2, pm25 will be represented as measurement name\n#   ##  - the Timestream table from the example will look like this:\n#   ##      Assuming:\n#   ##        - single_table_name = \"my_readings\"\n#   ##        - single_table_dimension_name_for_telegraf_measurement_name = \"namespace\"\n#   ##      TABLE \"my_readings\":\n#   ##        time | location | season | namespace | measure_name | measure_value::bigint\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | weather | temperature | 82\n#   ##        2016-06-13 17:43:50 | us-midwest | summer | weather | humidity | 71\n#   ##        2016-06-13 17:43:50 | us-west | NULL | airquality | no2 | 5\n#   ##        2016-06-13 17:43:50 | us-west | NULL | airquality | pm25 | 16\n#   ## In most cases, using multi-table mapping mode is recommended.\n#   ## However, you can consider using single-table in situations when you have thousands of measurement names.\n#   mapping_mode = \"multi-table\"\n#\n#   ## Only valid and required for mapping_mode = \"single-table\"\n#   ## Specifies the Timestream table where the metrics will be uploaded.\n#   # single_table_name = \"yourTableNameHere\"\n#\n#   ## Only valid and required for mapping_mode = \"single-table\"\n#   ## Describes what will be the Timestream dimension name for the Telegraf\n#   ## measurement name.\n#   # single_table_dimension_name_for_telegraf_measurement_name = \"namespace\"\n#\n#   ## Specifies if the plugin should create the table, if the table do not exist.\n#   ## The plugin writes the data without prior checking if the table exists.\n#   ## When the table does not exist, the error returned from Timestream will cause\n#   ## the plugin to create the table, if this parameter is set to true.\n#   create_table_if_not_exists = true\n#\n#   ## Only valid and required if create_table_if_not_exists = true\n#   ## Specifies the Timestream table magnetic store retention period in days.\n#   ## Check Timestream documentation for more details.\n#   create_table_magnetic_store_retention_period_in_days = 365\n#\n#   ## Only valid and required if create_table_if_not_exists = true\n#   ## Specifies the Timestream table memory store retention period in hours.\n#   ## Check Timestream documentation for more details.\n#   create_table_memory_store_retention_period_in_hours = 24\n#\n#   ## Only valid and optional if create_table_if_not_exists = true\n#   ## Specifies the Timestream table tags.\n#   ## Check Timestream documentation for more details\n#   # create_table_tags = { \"foo\" = \"bar\", \"environment\" = \"dev\"}\n#\n#   ## Specify the maximum number of parallel go routines to ingest/write data\n#   ## If not specified, defaulted to 1 go routines\n#   max_write_go_routines = 25\n\n\n# # Write metrics to Warp 10\n# [[outputs.warp10]]\n#   # Prefix to add to the measurement.\n#   prefix = \"telegraf.\"\n#\n#   # URL of the Warp 10 server\n#   warp_url = \"http://localhost:8080\"\n#\n#   # Write token to access your app on warp 10\n#   token = \"Token\"\n#\n#   # Warp 10 query timeout\n#   # timeout = \"15s\"\n#\n#   ## Print Warp 10 error body\n#   # print_error_body = false\n#\n#   ## Max string error size\n#   # max_string_error_size = 511\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Configuration for Wavefront server to send metrics to\n# [[outputs.wavefront]]\n#   ## Url for Wavefront Direct Ingestion. For Wavefront Proxy Ingestion, see\n#   ## the 'host' and 'port' optioins below.\n#   url = \"https://metrics.wavefront.com\"\n#\n#   ## Authentication Token for Wavefront. Only required if using Direct Ingestion\n#   #token = \"DUMMY_TOKEN\"\n#\n#   ## DNS name of the wavefront proxy server. Do not use if url is specified\n#   #host = \"wavefront.example.com\"\n#\n#   ## Port that the Wavefront proxy server listens on. Do not use if url is specified\n#   #port = 2878\n#\n#   ## prefix for metrics keys\n#   #prefix = \"my.specific.prefix.\"\n#\n#   ## whether to use \"value\" for name of simple fields. default is false\n#   #simple_fields = false\n#\n#   ## character to use between metric and field name.  default is . (dot)\n#   #metric_separator = \".\"\n#\n#   ## Convert metric name paths to use metricSeparator character\n#   ## When true will convert all _ (underscore) characters in final metric name. default is true\n#   #convert_paths = true\n#\n#   ## Use Strict rules to sanitize metric and tag names from invalid characters\n#   ## When enabled forward slash (/) and comma (,) will be accepted\n#   #use_strict = false\n#\n#   ## Use Regex to sanitize metric and tag names from invalid characters\n#   ## Regex is more thorough, but significantly slower. default is false\n#   #use_regex = false\n#\n#   ## point tags to use as the source name for Wavefront (if none found, host will be used)\n#   #source_override = [\"hostname\", \"address\", \"agent_host\", \"node_host\"]\n#\n#   ## whether to convert boolean values to numeric values, with false -\u003e 0.0 and true -\u003e 1.0. default is true\n#   #convert_bool = true\n#\n#   ## Truncate metric tags to a total of 254 characters for the tag name value. Wavefront will reject any\n#   ## data point exceeding this limit if not truncated. Defaults to 'false' to provide backwards compatibility.\n#   #truncate_tags = false\n#\n#   ## Flush the internal buffers after each batch. This effectively bypasses the background sending of metrics\n#   ## normally done by the Wavefront SDK. This can be used if you are experiencing buffer overruns. The sending\n#   ## of metrics will block for a longer time, but this will be handled gracefully by the internal buffering in\n#   ## Telegraf.\n#   #immediate_flush = true\n\n\n# # Generic WebSocket output writer.\n# [[outputs.websocket]]\n#   ## URL is the address to send metrics to. Make sure ws or wss scheme is used.\n#   url = \"ws://127.0.0.1:8080/telegraf\"\n#\n#   ## Timeouts (make sure read_timeout is larger than server ping interval or set to zero).\n#   # connect_timeout = \"30s\"\n#   # write_timeout = \"30s\"\n#   # read_timeout = \"30s\"\n#\n#   ## Optionally turn on using text data frames (binary by default).\n#   # use_text_frames = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional SOCKS5 proxy to use\n#   # socks5_enabled = true\n#   # socks5_address = \"127.0.0.1:1080\"\n#   # socks5_username = \"alice\"\n#   # socks5_password = \"pass123\"\n#\n#   ## Data format to output.\n#   ## Each data format has it's own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n#   # data_format = \"influx\"\n#\n#   ## Additional HTTP Upgrade headers\n#   # [outputs.websocket.headers]\n#   #   Authorization = \"Bearer \u003cTOKEN\u003e\"\n\n\n# # Send aggregated metrics to Yandex.Cloud Monitoring\n# [[outputs.yandex_cloud_monitoring]]\n#   ## Timeout for HTTP writes.\n#   # timeout = \"20s\"\n#\n#   ## Yandex.Cloud monitoring API endpoint. Normally should not be changed\n#   # endpoint_url = \"https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write\"\n#\n#   ## All user metrics should be sent with \"custom\" service specified. Normally should not be changed\n#   # service = \"custom\"\n\n\n###############################################################################\n#                            PROCESSOR PLUGINS                                #\n###############################################################################\n\n\n# # Attach AWS EC2 metadata to metrics\n# [[processors.aws_ec2]]\n#   ## Instance identity document tags to attach to metrics.\n#   ## For more information see:\n#   ## https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html\n#   ##\n#   ## Available tags:\n#   ## * accountId\n#   ## * architecture\n#   ## * availabilityZone\n#   ## * billingProducts\n#   ## * imageId\n#   ## * instanceId\n#   ## * instanceType\n#   ## * kernelId\n#   ## * pendingTime\n#   ## * privateIp\n#   ## * ramdiskId\n#   ## * region\n#   ## * version\n#   imds_tags = []\n#\n#   ## EC2 instance tags retrieved with DescribeTags action.\n#   ## In case tag is empty upon retrieval it's omitted when tagging metrics.\n#   ## Note that in order for this to work, role attached to EC2 instance or AWS\n#   ## credentials available from the environment must have a policy attached, that\n#   ## allows ec2:DescribeTags.\n#   ##\n#   ## For more information see:\n#   ## https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeTags.html\n#   ec2_tags = []\n#\n#   ## Timeout for http requests made by against aws ec2 metadata endpoint.\n#   timeout = \"10s\"\n#\n#   ## ordered controls whether or not the metrics need to stay in the same order\n#   ## this plugin received them in. If false, this plugin will change the order\n#   ## with requests hitting cached results moving through immediately and not\n#   ## waiting on slower lookups. This may cause issues for you if you are\n#   ## depending on the order of metrics staying the same. If so, set this to true.\n#   ## Keeping the metrics ordered may be slightly slower.\n#   ordered = false\n#\n#   ## max_parallel_calls is the maximum number of AWS API calls to be in flight\n#   ## at the same time.\n#   ## It's probably best to keep this number fairly low.\n#   max_parallel_calls = 10\n\n\n# # Clone metrics and apply modifications.\n# [[processors.clone]]\n#   ## All modifications on inputs and aggregators can be overridden:\n#   # name_override = \"new_name\"\n#   # name_prefix = \"new_name_prefix\"\n#   # name_suffix = \"new_name_suffix\"\n#\n#   ## Tags to be added (all values must be strings)\n#   # [processors.clone.tags]\n#   #   additional_tag = \"tag_value\"\n\n\n# # Convert values to another metric value type\n# [[processors.converter]]\n#   ## Tags to convert\n#   ##\n#   ## The table key determines the target type, and the array of key-values\n#   ## select the keys to convert.  The array may contain globs.\n#   ##   \u003ctarget-type\u003e = [\u003ctag-key\u003e...]\n#   [processors.converter.tags]\n#     measurement = []\n#     string = []\n#     integer = []\n#     unsigned = []\n#     boolean = []\n#     float = []\n#\n#   ## Fields to convert\n#   ##\n#   ## The table key determines the target type, and the array of key-values\n#   ## select the keys to convert.  The array may contain globs.\n#   ##   \u003ctarget-type\u003e = [\u003cfield-key\u003e...]\n#   [processors.converter.fields]\n#     measurement = []\n#     tag = []\n#     string = []\n#     integer = []\n#     unsigned = []\n#     boolean = []\n#     float = []\n\n\n# # Dates measurements, tags, and fields that pass through this filter.\n# [[processors.date]]\n# \t## New tag to create\n# \ttag_key = \"month\"\n#\n# \t## New field to create (cannot set both field_key and tag_key)\n# \t# field_key = \"month\"\n#\n# \t## Date format string, must be a representation of the Go \"reference time\"\n# \t## which is \"Mon Jan 2 15:04:05 -0700 MST 2006\".\n# \tdate_format = \"Jan\"\n#\n# \t## If destination is a field, date format can also be one of\n# \t## \"unix\", \"unix_ms\", \"unix_us\", or \"unix_ns\", which will insert an integer field.\n# \t# date_format = \"unix\"\n#\n# \t## Offset duration added to the date string when writing the new tag.\n# \t# date_offset = \"0s\"\n#\n# \t## Timezone to use when creating the tag or field using a reference time\n# \t## string.  This can be set to one of \"UTC\", \"Local\", or to a location name\n# \t## in the IANA Time Zone database.\n# \t##   example: timezone = \"America/Los_Angeles\"\n# \t# timezone = \"UTC\"\n\n\n# # Filter metrics with repeating field values\n# [[processors.dedup]]\n#   ## Maximum time to suppress output\n#   dedup_interval = \"600s\"\n\n\n# # Defaults sets default value(s) for specified fields that are not set on incoming metrics.\n# [[processors.defaults]]\n#   ## Ensures a set of fields always exists on your metric(s) with their\n#   ## respective default value.\n#   ## For any given field pair (key = default), if it's not set, a field\n#   ## is set on the metric with the specified default.\n#   ##\n#   ## A field is considered not set if it is nil on the incoming metric;\n#   ## or it is not nil but its value is an empty string or is a string\n#   ## of one or more spaces.\n#   ##   \u003ctarget-field\u003e = \u003cvalue\u003e\n#   # [processors.defaults.fields]\n#   #   field_1 = \"bar\"\n#   #   time_idle = 0\n#   #   is_error = true\n\n\n# # Map enum values according to given table.\n# [[processors.enum]]\n#   [[processors.enum.mapping]]\n#     ## Name of the field to map. Globs accepted.\n#     field = \"status\"\n#\n#     ## Name of the tag to map. Globs accepted.\n#     # tag = \"status\"\n#\n#     ## Destination tag or field to be used for the mapped value.  By default the\n#     ## source tag or field is used, overwriting the original value.\n#     dest = \"status_code\"\n#\n#     ## Default value to be used for all values not contained in the mapping\n#     ## table.  When unset, the unmodified value for the field will be used if no\n#     ## match is found.\n#     # default = 0\n#\n#     ## Table of mappings\n#     [processors.enum.mapping.value_mappings]\n#       green = 1\n#       amber = 2\n#       red = 3\n\n\n# # Run executable as long-running processor plugin\n# [[processors.execd]]\n# \t## Program to run as daemon\n# \t## eg: command = [\"/path/to/your_program\", \"arg1\", \"arg2\"]\n# \tcommand = [\"cat\"]\n#\n#   ## Delay before the process is restarted after an unexpected termination\n#   restart_delay = \"10s\"\n\n\n# # Performs file path manipulations on tags and fields\n# [[processors.filepath]]\n#   ## Treat the tag value as a path and convert it to its last element, storing the result in a new tag\n#   # [[processors.filepath.basename]]\n#   #   tag = \"path\"\n#   #   dest = \"basepath\"\n#\n#   ## Treat the field value as a path and keep all but the last element of path, typically the path's directory\n#   # [[processors.filepath.dirname]]\n#   #   field = \"path\"\n#\n#   ## Treat the tag value as a path, converting it to its the last element without its suffix\n#   # [[processors.filepath.stem]]\n#   #   tag = \"path\"\n#\n#   ## Treat the tag value as a path, converting it to the shortest path name equivalent\n#   ## to path by purely lexical processing\n#   # [[processors.filepath.clean]]\n#   #   tag = \"path\"\n#\n#   ## Treat the tag value as a path, converting it to a relative path that is lexically\n#   ## equivalent to the source path when joined to 'base_path'\n#   # [[processors.filepath.rel]]\n#   #   tag = \"path\"\n#   #   base_path = \"/var/log\"\n#\n#   ## Treat the tag value as a path, replacing each separator character in path with a '/' character. Has only\n#   ## effect on Windows\n#   # [[processors.filepath.toslash]]\n#   #   tag = \"path\"\n\n\n# # Add a tag of the network interface name looked up over SNMP by interface number\n# [[processors.ifname]]\n#   ## Name of tag holding the interface number\n#   # tag = \"ifIndex\"\n#\n#   ## Name of output tag where service name will be added\n#   # dest = \"ifName\"\n#\n#   ## Name of tag of the SNMP agent to request the interface name from\n#   # agent = \"agent\"\n#\n#   ## Timeout for each request.\n#   # timeout = \"5s\"\n#\n#   ## SNMP version; can be 1, 2, or 3.\n#   # version = 2\n#\n#   ## SNMP community string.\n#   # community = \"public\"\n#\n#   ## Number of retries to attempt.\n#   # retries = 3\n#\n#   ## The GETBULK max-repetitions parameter.\n#   # max_repetitions = 10\n#\n#   ## SNMPv3 authentication and encryption options.\n#   ##\n#   ## Security Name.\n#   # sec_name = \"myuser\"\n#   ## Authentication protocol; one of \"MD5\", \"SHA\", or \"\".\n#   # auth_protocol = \"MD5\"\n#   ## Authentication password.\n#   # auth_password = \"pass\"\n#   ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n#   # sec_level = \"authNoPriv\"\n#   ## Context Name.\n#   # context_name = \"\"\n#   ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\" or \"\".\n#   # priv_protocol = \"\"\n#   ## Privacy password used for encrypted messages.\n#   # priv_password = \"\"\n#\n#   ## max_parallel_lookups is the maximum number of SNMP requests to\n#   ## make at the same time.\n#   # max_parallel_lookups = 100\n#\n#   ## ordered controls whether or not the metrics need to stay in the\n#   ## same order this plugin received them in. If false, this plugin\n#   ## may change the order when data is cached.  If you need metrics to\n#   ## stay in order set this to true.  keeping the metrics ordered may\n#   ## be slightly slower\n#   # ordered = false\n#\n#   ## cache_ttl is the amount of time interface names are cached for a\n#   ## given agent.  After this period elapses if names are needed they\n#   ## will be retrieved again.\n#   # cache_ttl = \"8h\"\n\n\n# # Adds noise to numerical fields\n# [[processors.noise]]\n#     ## Specified the type of the random distribution.\n#     ## Can be \"laplacian\", \"gaussian\" or \"uniform\".\n#     # type = \"laplacian\n#\n#     ## Center of the distribution.\n#     ## Only used for Laplacian and Gaussian distributions.\n#     # mu = 0.0\n#\n#     ## Scale parameter for the Laplacian or Gaussian distribution\n#     # scale = 1.0\n#\n#     ## Upper and lower bound of the Uniform distribution\n#     # min = -1.0\n#     # max = 1.0\n#\n#     ## Apply the noise only to numeric fields matching the filter criteria below.\n#     ## Excludes takes precedence over includes.\n#     # include_fields = []\n#     # exclude_fields = []\n\n\n# # Apply metric modifications using override semantics.\n# [[processors.override]]\n#   ## All modifications on inputs and aggregators can be overridden:\n#   # name_override = \"new_name\"\n#   # name_prefix = \"new_name_prefix\"\n#   # name_suffix = \"new_name_suffix\"\n#\n#   ## Tags to be added (all values must be strings)\n#   # [processors.override.tags]\n#   #   additional_tag = \"tag_value\"\n\n\n# # Parse a value in a specified field/tag(s) and add the result in a new metric\n# [[processors.parser]]\n#   ## The name of the fields whose value will be parsed.\n#   parse_fields = []\n#\n#   ## If true, incoming metrics are not emitted.\n#   drop_original = false\n#\n#   ## If set to override, emitted metrics will be merged by overriding the\n#   ## original metric using the newly parsed metrics.\n#   merge = \"override\"\n#\n#   ## The dataformat to be read from files\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Rotate a single valued metric into a multi field metric\n# [[processors.pivot]]\n#   ## Tag to use for naming the new field.\n#   tag_key = \"name\"\n#   ## Field to use as the value of the new field.\n#   value_key = \"value\"\n\n\n# # Given a tag/field of a TCP or UDP port number, add a tag/field of the service name looked up in the system services file\n# [[processors.port_name]]\n# [[processors.port_name]]\n#   ## Name of tag holding the port number\n#   # tag = \"port\"\n#   ## Or name of the field holding the port number\n#   # field = \"port\"\n#\n#   ## Name of output tag or field (depending on the source) where service name will be added\n#   # dest = \"service\"\n#\n#   ## Default tcp or udp\n#   # default_protocol = \"tcp\"\n#\n#   ## Tag containing the protocol (tcp or udp, case-insensitive)\n#   # protocol_tag = \"proto\"\n#\n#   ## Field containing the protocol (tcp or udp, case-insensitive)\n#   # protocol_field = \"proto\"\n\n\n# # Print all metrics that pass through this filter.\n# [[processors.printer]]\n\n\n# # Transforms tag and field values as well as measurement, tag and field names with regex pattern\n# [[processors.regex]]\n#   ## Tag and field conversions defined in a separate sub-tables\n#   # [[processors.regex.tags]]\n#   #   ## Tag to change\n#   #   key = \"resp_code\"\n#   #   ## Regular expression to match on a tag value\n#   #   pattern = \"^(\\\\d)\\\\d\\\\d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use 1\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"1xx\"\n#\n#   # [[processors.regex.fields]]\n#   #   ## Field to change\n#   #   key = \"request\"\n#   #   ## All the power of the Go regular expressions available here\n#   #   ## For example, named subgroups\n#   #   pattern = \"^/api(?P\u003cmethod\u003e/[\\\\w/]+)\\\\S*\"\n#   #   replacement = \"method\"\n#   #   ## If result_key is present, a new field will be created\n#   #   ## instead of changing existing field\n#   #   result_key = \"method\"\n#\n#   ## Multiple conversions may be applied for one field sequentially\n#   ## Let's extract one more value\n#   # [[processors.regex.fields]]\n#   #   key = \"request\"\n#   #   pattern = \".*category=(\\\\w+).*\"\n#   #   replacement = \"1\"\n#   #   result_key = \"search_category\"\n#\n#   ## Rename metric fields\n#   # [[processors.regex.field_rename]]\n#   #   ## Regular expression to match on a field name\n#   #   pattern = \"^search_(\\\\w+)d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use 1\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"1\"\n#   #   ## If the new field name already exists, you can either \"overwrite\" the\n#   #   ## existing one with the value of the renamed field OR you can \"keep\"\n#   #   ## both the existing and source field.\n#   #   # result_key = \"keep\"\n#\n#   ## Rename metric tags\n#   # [[processors.regex.tag_rename]]\n#   #   ## Regular expression to match on a tag name\n#   #   pattern = \"^search_(\\\\w+)d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use 1\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"1\"\n#   #   ## If the new tag name already exists, you can either \"overwrite\" the\n#   #   ## existing one with the value of the renamed tag OR you can \"keep\"\n#   #   ## both the existing and source tag.\n#   #   # result_key = \"keep\"\n#\n#   ## Rename metrics\n#   # [[processors.regex.metric_rename]]\n#   #   ## Regular expression to match on an metric name\n#   #   pattern = \"^search_(\\\\w+)d$\"\n#   #   ## Matches of the pattern will be replaced with this string.  Use 1\n#   #   ## notation to use the text of the first submatch.\n#   #   replacement = \"1\"\n\n\n# # Rename measurements, tags, and fields that pass through this filter.\n# [[processors.rename]]\n\n\n# # ReverseDNS does a reverse lookup on IP addresses to retrieve the DNS name\n# [[processors.reverse_dns]]\n#   ## For optimal performance, you may want to limit which metrics are passed to this\n#   ## processor. eg:\n#   ## namepass = [\"my_metric_*\"]\n#\n#   ## cache_ttl is how long the dns entries should stay cached for.\n#   ## generally longer is better, but if you expect a large number of diverse lookups\n#   ## you'll want to consider memory use.\n#   cache_ttl = \"24h\"\n#\n#   ## lookup_timeout is how long should you wait for a single dns request to repsond.\n#   ## this is also the maximum acceptable latency for a metric travelling through\n#   ## the reverse_dns processor. After lookup_timeout is exceeded, a metric will\n#   ## be passed on unaltered.\n#   ## multiple simultaneous resolution requests for the same IP will only make a\n#   ## single rDNS request, and they will all wait for the answer for this long.\n#   lookup_timeout = \"3s\"\n#\n#   ## max_parallel_lookups is the maximum number of dns requests to be in flight\n#   ## at the same time. Requesting hitting cached values do not count against this\n#   ## total, and neither do mulptiple requests for the same IP.\n#   ## It's probably best to keep this number fairly low.\n#   max_parallel_lookups = 10\n#\n#   ## ordered controls whether or not the metrics need to stay in the same order\n#   ## this plugin received them in. If false, this plugin will change the order\n#   ## with requests hitting cached results moving through immediately and not\n#   ## waiting on slower lookups. This may cause issues for you if you are\n#   ## depending on the order of metrics staying the same. If so, set this to true.\n#   ## keeping the metrics ordered may be slightly slower.\n#   ordered = false\n#\n#   [[processors.reverse_dns.lookup]]\n#     ## get the ip from the field \"source_ip\", and put the result in the field \"source_name\"\n#     field = \"source_ip\"\n#     dest = \"source_name\"\n#\n#   [[processors.reverse_dns.lookup]]\n#     ## get the ip from the tag \"destination_ip\", and put the result in the tag\n#     ## \"destination_name\".\n#     tag = \"destination_ip\"\n#     dest = \"destination_name\"\n#\n#     ## If you would prefer destination_name to be a field instead, you can use a\n#     ## processors.converter after this one, specifying the order attribute.\n\n\n# # Add the S2 Cell ID as a tag based on latitude and longitude fields\n# [[processors.s2geo]]\n#   ## The name of the lat and lon fields containing WGS-84 latitude and\n#   ## longitude in decimal degrees.\n#   # lat_field = \"lat\"\n#   # lon_field = \"lon\"\n#\n#   ## New tag to create\n#   # tag_key = \"s2_cell_id\"\n#\n#   ## Cell level (see https://s2geometry.io/resources/s2cell_statistics.html)\n#   # cell_level = 9\n\n\n# # Process metrics using a Starlark script\n# [[processors.starlark]]\n#   ## The Starlark source can be set as a string in this configuration file, or\n#   ## by referencing a file containing the script.  Only one source or script\n#   ## should be set at once.\n#   ##\n#   ## Source of the Starlark script.\n#   source = '''\n# def apply(metric):\n# \treturn metric\n# '''\n#\n#   ## File containing a Starlark script.\n#   # script = \"/usr/local/bin/myscript.star\"\n#\n#   ## The constants of the Starlark script.\n#   # [processors.starlark.constants]\n#   #   max_size = 10\n#   #   threshold = 0.75\n#   #   default_name = \"Julia\"\n#   #   debug_mode = true\n\n\n# # Perform string processing on tags, fields, and measurements\n# [[processors.strings]]\n#   ## Convert a tag value to uppercase\n#   # [[processors.strings.uppercase]]\n#   #   tag = \"method\"\n#\n#   ## Convert a field value to lowercase and store in a new field\n#   # [[processors.strings.lowercase]]\n#   #   field = \"uri_stem\"\n#   #   dest = \"uri_stem_normalised\"\n#\n#   ## Convert a field value to titlecase\n#   # [[processors.strings.titlecase]]\n#   #   field = \"status\"\n#\n#   ## Trim leading and trailing whitespace using the default cutset\n#   # [[processors.strings.trim]]\n#   #   field = \"message\"\n#\n#   ## Trim leading characters in cutset\n#   # [[processors.strings.trim_left]]\n#   #   field = \"message\"\n#   #   cutset = \"\\t\"\n#\n#   ## Trim trailing characters in cutset\n#   # [[processors.strings.trim_right]]\n#   #   field = \"message\"\n#   #   cutset = \"\\r\\n\"\n#\n#   ## Trim the given prefix from the field\n#   # [[processors.strings.trim_prefix]]\n#   #   field = \"my_value\"\n#   #   prefix = \"my_\"\n#\n#   ## Trim the given suffix from the field\n#   # [[processors.strings.trim_suffix]]\n#   #   field = \"read_count\"\n#   #   suffix = \"_count\"\n#\n#   ## Replace all non-overlapping instances of old with new\n#   # [[processors.strings.replace]]\n#   #   measurement = \"*\"\n#   #   old = \":\"\n#   #   new = \"_\"\n#\n#   ## Trims strings based on width\n#   # [[processors.strings.left]]\n#   #   field = \"message\"\n#   #   width = 10\n#\n#   ## Decode a base64 encoded utf-8 string\n#   # [[processors.strings.base64decode]]\n#   #   field = \"message\"\n#\n#   ## Sanitize a string to ensure it is a valid utf-8 string\n#   ## Each run of invalid UTF-8 byte sequences is replaced by the replacement string, which may be empty\n#   # [[processors.strings.valid_utf8]]\n#   #   field = \"message\"\n#   #   replacement = \"\"\n\n\n# # Restricts the number of tags that can pass through this filter and chooses which tags to preserve when over the limit.\n# [[processors.tag_limit]]\n#   ## Maximum number of tags to preserve\n#   limit = 10\n#\n#   ## List of tags to preferentially preserve\n#   keep = [\"foo\", \"bar\", \"baz\"]\n\n\n# # Uses a Go template to create a new tag\n# [[processors.template]]\n#   ## Tag to set with the output of the template.\n#   tag = \"topic\"\n#\n#   ## Go template used to create the tag value.  In order to ease TOML\n#   ## escaping requirements, you may wish to use single quotes around the\n#   ## template string.\n#   template = '{{ .Tag \"hostname\" }}.{{ .Tag \"level\" }}'\n\n\n# # Print all metrics that pass through this filter.\n# [[processors.topk]]\n#   ## How many seconds between aggregations\n#   # period = 10\n#\n#   ## How many top metrics to return\n#   # k = 10\n#\n#   ## Over which tags should the aggregation be done. Globs can be specified, in\n#   ## which case any tag matching the glob will aggregated over. If set to an\n#   ## empty list is no aggregation over tags is done\n#   # group_by = ['*']\n#\n#   ## Over which fields are the top k are calculated\n#   # fields = [\"value\"]\n#\n#   ## What aggregation to use. Options: sum, mean, min, max\n#   # aggregation = \"mean\"\n#\n#   ## Instead of the top k largest metrics, return the bottom k lowest metrics\n#   # bottomk = false\n#\n#   ## The plugin assigns each metric a GroupBy tag generated from its name and\n#   ## tags. If this setting is different than \"\" the plugin will add a\n#   ## tag (which name will be the value of this setting) to each metric with\n#   ## the value of the calculated GroupBy tag. Useful for debugging\n#   # add_groupby_tag = \"\"\n#\n#   ## These settings provide a way to know the position of each metric in\n#   ## the top k. The 'add_rank_field' setting allows to specify for which\n#   ## fields the position is required. If the list is non empty, then a field\n#   ## will be added to each and every metric for each string present in this\n#   ## setting. This field will contain the ranking of the group that\n#   ## the metric belonged to when aggregated over that field.\n#   ## The name of the field will be set to the name of the aggregation field,\n#   ## suffixed with the string '_topk_rank'\n#   # add_rank_fields = []\n#\n#   ## These settings provide a way to know what values the plugin is generating\n#   ## when aggregating metrics. The 'add_aggregate_field' setting allows to\n#   ## specify for which fields the final aggregation value is required. If the\n#   ## list is non empty, then a field will be added to each every metric for\n#   ## each field present in this setting. This field will contain\n#   ## the computed aggregation for the group that the metric belonged to when\n#   ## aggregated over that field.\n#   ## The name of the field will be set to the name of the aggregation field,\n#   ## suffixed with the string '_topk_aggregate'\n#   # add_aggregate_fields = []\n\n\n# # Rotate multi field metric into several single field metrics\n# [[processors.unpivot]]\n#   ## Tag to use for the name.\n#   tag_key = \"name\"\n#   ## Field to use for the name of the value.\n#   value_key = \"value\"\n\n\n###############################################################################\n#                            AGGREGATOR PLUGINS                               #\n###############################################################################\n\n\n# # Keep the aggregate basicstats of each metric passing through.\n# [[aggregators.basicstats]]\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## Configures which basic stats to push as fields\n#   # stats = [\"count\", \"min\", \"max\", \"mean\", \"stdev\", \"s2\", \"sum\"]\n\n\n# # Calculates a derivative for every field.\n# [[aggregators.derivative]]\n# \t## The period in which to flush the aggregator.\n# \tperiod = \"30s\"\n# \t##\n# \t## If true, the original metric will be dropped by the\n# \t## aggregator and will not get sent to the output plugins.\n# \tdrop_original = false\n# \t##\n# \t## This aggregator will estimate a derivative for each field, which is\n# \t## contained in both the first and last metric of the aggregation interval.\n# \t## Without further configuration the derivative will be calculated with\n# \t## respect to the time difference between these two measurements in seconds.\n# \t## The formula applied is for every field:\n# \t##\n# \t##               value_last - value_first\n# \t## derivative = --------------------------\n# \t##              time_difference_in_seconds\n# \t##\n# \t## The resulting derivative will be named *fieldname_rate*. The suffix\n# \t## \"_rate\" can be configured by the *suffix* parameter. When using a\n# \t## derivation variable you can include its name for more clarity.\n# \t# suffix = \"_rate\"\n# \t##\n# \t## As an abstraction the derivative can be calculated not only by the time\n# \t## difference but by the difference of a field, which is contained in the\n# \t## measurement. This field is assumed to be monotonously increasing. This\n# \t## feature is used by specifying a *variable*.\n# \t## Make sure the specified variable is not filtered and exists in the metrics\n# \t## passed to this aggregator!\n# \t# variable = \"\"\n# \t##\n# \t## When using a field as the derivation parameter the name of that field will\n# \t## be used for the resulting derivative, e.g. *fieldname_by_parameter*.\n# \t##\n# \t## Note, that the calculation is based on the actual timestamp of the\n# \t## measurements. When there is only one measurement during that period, the\n# \t## measurement will be rolled over to the next period. The maximum number of\n# \t## such roll-overs can be configured with a default of 10.\n# \t# max_roll_over = 10\n# \t##\n\n\n# # Report the final metric of a series\n# [[aggregators.final]]\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## The time that a series is not updated until considering it final.\n#   series_timeout = \"5m\"\n\n\n# # Create aggregate histograms.\n# [[aggregators.histogram]]\n#   ## The period in which to flush the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## If true, the histogram will be reset on flush instead\n#   ## of accumulating the results.\n#   reset = false\n#\n#   ## Whether bucket values should be accumulated. If set to false, \"gt\" tag will be added.\n#   ## Defaults to true.\n#   cumulative = true\n#\n#   ## Expiration interval for each histogram. The histogram will be expired if\n#   ## there are no changes in any buckets for this time interval. 0 == no expiration.\n#   # expiration_interval = \"0m\"\n#\n#   ## If true, aggregated histogram are pushed to output only if it was updated since\n#   ## previous push. Defaults to false.\n#   # push_only_on_update = false\n#\n#   ## Example config that aggregates all fields of the metric.\n#   # [[aggregators.histogram.config]]\n#   #   ## Right borders of buckets (with +Inf implicitly added).\n#   #   buckets = [0.0, 15.6, 34.5, 49.1, 71.5, 80.5, 94.5, 100.0]\n#   #   ## The name of metric.\n#   #   measurement_name = \"cpu\"\n#\n#   ## Example config that aggregates only specific fields of the metric.\n#   # [[aggregators.histogram.config]]\n#   #   ## Right borders of buckets (with +Inf implicitly added).\n#   #   buckets = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n#   #   ## The name of metric.\n#   #   measurement_name = \"diskio\"\n#   #   ## The concrete fields of metric\n#   #   fields = [\"io_time\", \"read_time\", \"write_time\"]\n\n\n# # Merge metrics into multifield metrics by series key\n# [[aggregators.merge]]\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = true\n\n\n# # Keep the aggregate min/max of each metric passing through.\n# [[aggregators.minmax]]\n#   ## General Aggregator Arguments:\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n\n\n# # Keep the aggregate quantiles of each metric passing through.\n# [[aggregators.quantile]]\n#   ## General Aggregator Arguments:\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#\n#   ## Quantiles to output in the range [0,1]\n#   # quantiles = [0.25, 0.5, 0.75]\n#\n#   ## Type of aggregation algorithm\n#   ## Supported are:\n#   ##  \"t-digest\" -- approximation using centroids, can cope with large number of samples\n#   ##  \"exact R7\" -- exact computation also used by Excel or NumPy (Hyndman \u0026 Fan 1996 R7)\n#   ##  \"exact R8\" -- exact computation (Hyndman \u0026 Fan 1996 R8)\n#   ## NOTE: Do not use \"exact\" algorithms with large number of samples\n#   ##       to not impair performance or memory consumption!\n#   # algorithm = \"t-digest\"\n#\n#   ## Compression for approximation (t-digest). The value needs to be\n#   ## greater or equal to 1.0. Smaller values will result in more\n#   ## performance but less accuracy.\n#   # compression = 100.0\n\n\n# # Aggregate metrics using a Starlark script\n# [[aggregators.starlark]]\n#   ## The Starlark source can be set as a string in this configuration file, or\n#   ## by referencing a file containing the script.  Only one source or script\n#   ## should be set at once.\n#   ##\n#   ## Source of the Starlark script.\n#   source = '''\n# state = {}\n#\n# def add(metric):\n#   state[\"last\"] = metric\n#\n# def push():\n#   return state.get(\"last\")\n#\n# def reset():\n#   state.clear()\n# '''\n#\n#   ## File containing a Starlark script.\n#   # script = \"/usr/local/bin/myscript.star\"\n#\n#   ## The constants of the Starlark script.\n#   # [aggregators.starlark.constants]\n#   #   max_size = 10\n#   #   threshold = 0.75\n#   #   default_name = \"Julia\"\n#   #   debug_mode = true\n\n\n# # Count the occurrence of values in fields.\n# [[aggregators.valuecounter]]\n#   ## General Aggregator Arguments:\n#   ## The period on which to flush \u0026 clear the aggregator.\n#   period = \"30s\"\n#   ## If true, the original metric will be dropped by the\n#   ## aggregator and will not get sent to the output plugins.\n#   drop_original = false\n#   ## The fields for which the values will be counted\n#   fields = []\n\n\n###############################################################################\n#                            INPUT PLUGINS                                    #\n###############################################################################\n\n\n# Read metrics about cpu usage\n[[inputs.cpu]]\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states\n  report_active = false\n\n\n# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n\n# Read metrics about disk IO by device\n[[inputs.diskio]]\n  ## By default, telegraf will gather stats for all devices including\n  ## disk partitions.\n  ## Setting devices will restrict the stats to the specified devices.\n  # devices = [\"sda\", \"sdb\", \"vd*\"]\n  ## Uncomment the following line if you need disk serial numbers.\n  # skip_serial_number = false\n  #\n  ## On systems which support it, device metadata can be added in the form of\n  ## tags.\n  ## Currently only Linux is supported via udev properties. You can view\n  ## available properties for a device by running:\n  ## 'udevadm info -q property -n /dev/sda'\n  ## Note: Most, but not all, udev properties can be accessed this way. Properties\n  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.\n  # device_tags = [\"ID_FS_TYPE\", \"ID_FS_USAGE\"]\n  #\n  ## Using the same metadata source as device_tags, you can also customize the\n  ## name of the device via templates.\n  ## The 'name_templates' parameter is a list of templates to try and apply to\n\n  ## present for the device is used as the device name tag.\n  ## The typical use case is for LVM volumes, to get the VG/LV name instead of\n  ## the near-meaningless DM-0 name.\n  # name_templates = [\"$ID_FS_LABEL\",\"$DM_VG_NAME/$DM_LV_NAME\"]\n\n\n# Get kernel statistics from /proc/stat\n[[inputs.kernel]]\n  # no configuration\n\n\n# Read metrics about memory usage\n[[inputs.mem]]\n  # no configuration\n\n\n# Get the number of processes and group them by status\n[[inputs.processes]]\n  # no configuration\n\n\n# Read metrics about swap memory usage\n[[inputs.swap]]\n  # no configuration\n\n\n# Read metrics about system load \u0026 uptime\n[[inputs.system]]\n  ## Uncomment to remove deprecated metrics.\n  # fielddrop = [\"uptime_format\"]\n\n\n# # Gather ActiveMQ metrics\n# [[inputs.activemq]]\n#   ## ActiveMQ WebConsole URL\n#   url = \"http://127.0.0.1:8161\"\n#\n#   ## Credentials for basic HTTP authentication\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## Required ActiveMQ webadmin root path\n#   # webadmin = \"admin\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read stats from aerospike server(s)\n# [[inputs.aerospike]]\n#   ## Aerospike servers to connect to (with port)\n#   ## This plugin will query all namespaces the aerospike\n#   ## server has configured and get stats for them.\n#   servers = [\"localhost:3000\"]\n#\n#   # username = \"telegraf\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # enable_tls = false\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   # tls_name = \"tlsname\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n#\n#   # Feature Options\n#   # Add namespace variable to limit the namespaces executed on\n#   # Leave blank to do all\n#   # disable_query_namespaces = true # default false\n#   # namespaces = [\"namespace1\", \"namespace2\"]\n#\n#   # Enable set level telemetry\n#   # query_sets = true # default: false\n#   # Add namespace set combinations to limit sets executed on\n#   # Leave blank to do all sets\n#   # sets = [\"namespace1/set1\", \"namespace1/set2\", \"namespace3\"]\n#\n#   # Histograms\n#   # enable_ttl_histogram = true # default: false\n#   # enable_object_size_linear_histogram = true # default: false\n#\n#   # by default, aerospike produces a 100 bucket histogram\n#   # this is not great for most graphing tools, this will allow\n#   # the ability to squash this to a smaller number of buckets\n#   # To have a balanced histogram, the number of buckets chosen\n#   # should divide evenly into 100.\n#   # num_histogram_buckets = 100 # default: 10\n\n\n# # Query statistics from AMD Graphics cards using rocm-smi binary\n# [[inputs.amd_rocm_smi]]\n# ## Optional: path to rocm-smi binary, defaults to $PATH via exec.LookPath\n# # bin_path = \"/opt/rocm/bin/rocm-smi\"\n#\n# ## Optional: timeout for GPU polling\n# # timeout = \"5s\"\n\n\n# # Read Apache status information (mod_status)\n# [[inputs.apache]]\n#   ## An array of URLs to gather from, must be directed at the machine\n#   ## readable version of the mod_status page including the auto query string.\n#   ## Default is \"http://localhost/server-status?auto\".\n#   urls = [\"http://localhost/server-status?auto\"]\n#\n#   ## Credentials for basic HTTP authentication.\n#   # username = \"myuser\"\n#   # password = \"mypassword\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Monitor APC UPSes connected to apcupsd\n# [[inputs.apcupsd]]\n#   # A list of running apcupsd server to connect to.\n#   # If not provided will default to tcp://127.0.0.1:3551\n#   servers = [\"tcp://127.0.0.1:3551\"]\n#\n#   ## Timeout for dialing server.\n#   timeout = \"5s\"\n\n\n# # Gather metrics from Apache Aurora schedulers\n# [[inputs.aurora]]\n#   ## Schedulers are the base addresses of your Aurora Schedulers\n#   schedulers = [\"http://127.0.0.1:8081\"]\n#\n#   ## Set of role types to collect metrics from.\n#   ##\n#   ## The scheduler roles are checked each interval by contacting the\n#   ## scheduler nodes; zookeeper is not contacted.\n#   # roles = [\"leader\", \"follower\"]\n#\n#   ## Timeout is the max time for total network operations.\n#   # timeout = \"5s\"\n#\n#   ## Username and password are sent using HTTP Basic Auth.\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Gather Azure Storage Queue metrics\n# [[inputs.azure_storage_queue]]\n#   ## Required Azure Storage Account name\n#   account_name = \"mystorageaccount\"\n#\n#   ## Required Azure Storage Account access key\n#   account_key = \"storageaccountaccesskey\"\n#\n#   ## Set to false to disable peeking age of oldest message (executes faster)\n#   # peek_oldest_message_age = true\n\n\n# # Read metrics of bcache from stats_total and dirty_data\n# [[inputs.bcache]]\n#   ## Bcache sets path\n#   ## If not specified, then default is:\n#   bcachePath = \"/sys/fs/bcache\"\n#\n#   ## By default, Telegraf gather stats for all bcache devices\n#   ## Setting devices will restrict the stats to the specified\n#   ## bcache devices.\n#   bcacheDevs = [\"bcache0\"]\n\n\n# # Collects Beanstalkd server and tubes stats\n# [[inputs.beanstalkd]]\n#   ## Server to collect data from\n#   server = \"localhost:11300\"\n#\n#   ## List of tubes to gather stats about.\n#   ## If no tubes specified then data gathered for each tube on server reported by list-tubes command\n#   tubes = [\"notifications\"]\n\n\n# # Read metrics exposed by Beat\n# [[inputs.beat]]\n#   ## An URL from which to read Beat-formatted JSON\n#   ## Default is \"http://127.0.0.1:5066\".\n#   url = \"http://127.0.0.1:5066\"\n#\n#   ## Enable collection of the listed stats\n#   ## An empty list means collect all. Available options are currently\n#   ## \"beat\", \"libbeat\", \"system\" and \"filebeat\".\n#   # include = [\"beat\", \"libbeat\", \"filebeat\"]\n#\n#   ## HTTP method\n#   # method = \"GET\"\n#\n#   ## Optional HTTP headers\n#   # headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## Override HTTP \"Host\" header\n#   # host_header = \"logstash.example.com\"\n#\n#   ## Timeout for HTTP requests\n#   # timeout = \"5s\"\n#\n#   ## Optional HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read BIND nameserver XML statistics\n# [[inputs.bind]]\n#   ## An array of BIND XML statistics URI to gather stats.\n#   ## Default is \"http://localhost:8053/xml/v3\".\n#   # urls = [\"http://localhost:8053/xml/v3\"]\n#   # gather_memory_contexts = false\n#   # gather_views = false\n#\n#   ## Timeout for http requests made by bind nameserver\n#   # timeout = \"4s\"\n\n\n# # Collect bond interface status, slaves statuses and failures count\n# [[inputs.bond]]\n#   ## Sets 'proc' directory path\n#   ## If not specified, then default is /proc\n#   # host_proc = \"/proc\"\n#\n#   ## Sets 'sys' directory path\n#   ## If not specified, then default is /sys\n#   # host_sys = \"/sys\"\n#\n#   ## By default, telegraf gather stats for all bond interfaces\n#   ## Setting interfaces will restrict the stats to the specified\n#   ## bond interfaces.\n#   # bond_interfaces = [\"bond0\"]\n#\n#   ## Tries to collect additional bond details from /sys/class/net/{bond}\n#   ## currently only useful for LACP (mode 4) bonds\n#   # collect_sys_details = false\n#\n\n\n# # Collect Kafka topics and consumers status from Burrow HTTP API.\n# [[inputs.burrow]]\n#   ## Burrow API endpoints in format \"schema://host:port\".\n#   ## Default is \"http://localhost:8000\".\n#   servers = [\"http://localhost:8000\"]\n#\n#   ## Override Burrow API prefix.\n#   ## Useful when Burrow is behind reverse-proxy.\n#   # api_prefix = \"/v3/kafka\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Limit per-server concurrent connections.\n#   ## Useful in case of large number of topics or consumer groups.\n#   # concurrent_connections = 20\n#\n#   ## Filter clusters, default is no filtering.\n#   ## Values can be specified as glob patterns.\n#   # clusters_include = []\n#   # clusters_exclude = []\n#\n#   ## Filter consumer groups, default is no filtering.\n#   ## Values can be specified as glob patterns.\n#   # groups_include = []\n#   # groups_exclude = []\n#\n#   ## Filter topics, default is no filtering.\n#   ## Values can be specified as glob patterns.\n#   # topics_include = []\n#   # topics_exclude = []\n#\n#   ## Credentials for basic HTTP authentication.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional SSL config\n#   # ssl_ca = \"/etc/telegraf/ca.pem\"\n#   # ssl_cert = \"/etc/telegraf/cert.pem\"\n#   # ssl_key = \"/etc/telegraf/key.pem\"\n#   # insecure_skip_verify = false\n\n\n# # Collects performance metrics from the MON, OSD, MDS and RGW nodes in a Ceph storage cluster.\n# [[inputs.ceph]]\n#   ## This is the recommended interval to poll.  Too frequent and you will lose\n#   ## data points due to timeouts during rebalancing and recovery\n#   interval = '1m'\n#\n#   ## All configuration values are optional, defaults are shown below\n#\n#   ## location of ceph binary\n#   ceph_binary = \"/usr/bin/ceph\"\n#\n#   ## directory in which to look for socket files\n#   socket_dir = \"/var/run/ceph\"\n#\n#   ## prefix of MON and OSD socket files, used to determine socket type\n#   mon_prefix = \"ceph-mon\"\n#   osd_prefix = \"ceph-osd\"\n#   mds_prefix = \"ceph-mds\"\n#   rgw_prefix = \"ceph-client\"\n#\n#   ## suffix used to identify socket files\n#   socket_suffix = \"asok\"\n#\n#   ## Ceph user to authenticate as, ceph will search for the corresponding keyring\n#   ## e.g. client.admin.keyring in /etc/ceph, or the explicit path defined in the\n#   ## client section of ceph.conf for example:\n#   ##\n#   ##     [client.telegraf]\n#   ##         keyring = /etc/ceph/client.telegraf.keyring\n#   ##\n#   ## Consult the ceph documentation for more detail on keyring generation.\n#   ceph_user = \"client.admin\"\n#\n#   ## Ceph configuration to use to locate the cluster\n#   ceph_config = \"/etc/ceph/ceph.conf\"\n#\n#   ## Whether to gather statistics via the admin socket\n#   gather_admin_socket_stats = true\n#\n#   ## Whether to gather statistics via ceph commands, requires ceph_user and ceph_config\n#   ## to be specified\n#   gather_cluster_stats = false\n\n\n# # Read specific statistics per cgroup\n# [[inputs.cgroup]]\n#   ## Directories in which to look for files, globs are supported.\n#   ## Consider restricting paths to the set of cgroups you really\n#   ## want to monitor if you have a large number of cgroups, to avoid\n#   ## any cardinality issues.\n#   # paths = [\n#   #   \"/sys/fs/cgroup/memory\",\n#   #   \"/sys/fs/cgroup/memory/child1\",\n#   #   \"/sys/fs/cgroup/memory/child2/*\",\n#   # ]\n#   ## cgroup stat fields, as file names, globs are supported.\n#   ## these file names are appended to each path from above.\n#   # files = [\"memory.*usage*\", \"memory.limit_in_bytes\"]\n\n\n# # Get standard chrony metrics, requires chronyc executable.\n# [[inputs.chrony]]\n#   ## If true, chronyc tries to perform a DNS lookup for the time server.\n#   # dns_lookup = false\n\n\n# # Pull Metric Statistics from Amazon CloudWatch\n# [[inputs.cloudwatch]]\n#   ## Amazon Region\n#   region = \"us-east-1\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   # access_key = \"\"\n#   # secret_key = \"\"\n#   # token = \"\"\n#   # role_arn = \"\"\n#   # web_identity_token_file = \"\"\n#   # role_session_name = \"\"\n#   # profile = \"\"\n#   # shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n#   # http_proxy_url = \"http://localhost:8888\"\n#\n#   # The minimum period for Cloudwatch metrics is 1 minute (60s). However not all\n#   # metrics are made available to the 1 minute period. Some are collected at\n#   # 3 minute, 5 minute, or larger intervals. See https://aws.amazon.com/cloudwatch/faqs/#monitoring.\n#   # Note that if a period is configured that is smaller than the minimum for a\n#   # particular metric, that metric will not be returned by the Cloudwatch API\n#   # and will not be collected by Telegraf.\n#   #\n#   ## Requested CloudWatch aggregation Period (required - must be a multiple of 60s)\n#   period = \"5m\"\n#\n#   ## Collection Delay (required - must account for metrics availability via CloudWatch API)\n#   delay = \"5m\"\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n#   ## gaps or overlap in pulled data\n#   interval = \"5m\"\n#\n#   ## Recommended if \"delay\" and \"period\" are both within 3 hours of request time. Invalid values will be ignored.\n#   ## Recently Active feature will only poll for CloudWatch ListMetrics values that occurred within the last 3 Hours.\n#   ## If enabled, it will reduce total API usage of the CloudWatch ListMetrics API and require less memory to retain.\n#   ## Do not enable if \"period\" or \"delay\" is longer than 3 hours, as it will not return data more than 3 hours old.\n#   ## See https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_ListMetrics.html\n#   #recently_active = \"PT3H\"\n#\n#   ## Configure the TTL for the internal cache of metrics.\n#   # cache_ttl = \"1h\"\n#\n#   ## Metric Statistic Namespaces (required)\n#   namespaces = [\"AWS/ELB\"]\n#   # A single metric statistic namespace that will be appended to namespaces on startup\n#   # namespace = \"AWS/ELB\"\n#\n#   ## Maximum requests per second. Note that the global default AWS rate limit is\n#   ## 50 reqs/sec, so if you define multiple namespaces, these should add up to a\n#   ## maximum of 50.\n#   ## See http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_limits.html\n#   # ratelimit = 25\n#\n#   ## Timeout for http requests made by the cloudwatch client.\n#   # timeout = \"5s\"\n#\n#   ## Namespace-wide statistic filters. These allow fewer queries to be made to\n#   ## cloudwatch.\n#   # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n#   # statistic_exclude = []\n#\n#   ## Metrics to Pull\n#   ## Defaults to all Metrics in Namespace if nothing is provided\n#   ## Refreshes Namespace available metrics every 1h\n#   #[[inputs.cloudwatch.metrics]]\n#   #  names = [\"Latency\", \"RequestCount\"]\n#   #\n#   #  ## Statistic filters for Metric.  These allow for retrieving specific\n#   #  ## statistics for an individual metric.\n#   #  # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n#   #  # statistic_exclude = []\n#   #\n#   #  ## Dimension filters for Metric.  All dimensions defined for the metric names\n#   #  ## must be specified in order to retrieve the metric statistics.\n#   #  ## 'value' has wildcard / 'glob' matching support such as 'p-*'.\n#   #  [[inputs.cloudwatch.metrics.dimensions]]\n#   #    name = \"LoadBalancerName\"\n#   #    value = \"p-example\"\n\n\n# # Collects conntrack stats from the configured directories and files.\n# [[inputs.conntrack]]\n#    ## The following defaults would work with multiple versions of conntrack.\n#    ## Note the nf_ and ip_ filename prefixes are mutually exclusive across\n#    ## kernel versions, as are the directory locations.\n#\n#    ## Superset of filenames to look for within the conntrack dirs.\n#    ## Missing files will be ignored.\n#    files = [\"ip_conntrack_count\",\"ip_conntrack_max\",\n#             \"nf_conntrack_count\",\"nf_conntrack_max\"]\n#\n#    ## Directories to search within for the conntrack files above.\n#    ## Missing directories will be ignored.\n#    dirs = [\"/proc/sys/net/ipv4/netfilter\",\"/proc/sys/net/netfilter\"]\n\n\n# # Gather health check statuses from services registered in Consul\n# [[inputs.consul]]\n#   ## Consul server address\n#   # address = \"localhost:8500\"\n#\n#   ## URI scheme for the Consul server, one of \"http\", \"https\"\n#   # scheme = \"http\"\n#\n#   ## Metric version controls the mapping from Consul metrics into\n#   ## Telegraf metrics.\n#   ##\n#   ##   example: metric_version = 1; deprecated in 1.15\n#   ##            metric_version = 2; recommended version\n#   # metric_version = 1\n#\n#   ## ACL token used in every request\n#   # token = \"\"\n#\n#   ## HTTP Basic Authentication username and password.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Data center to query the health checks from\n#   # datacenter = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n#\n#   ## Consul checks' tag splitting\n#   # When tags are formatted like \"key:value\" with \":\" as a delimiter then\n#   # they will be splitted and reported as proper key:value in Telegraf\n#   # tag_delimiter = \":\"\n\n\n# # Read metrics from the Consul API\n# [[inputs.consul_metrics]]\n#   ## URL for the Consul agent\n#   # url = \"http://127.0.0.1:8500\"\n#\n#   ## Use auth token for authorization.\n#   ## Only one of the options can be set. Leave empty to not use any token.\n#   # token_file = \"/path/to/auth/token\"\n#   ## OR\n#   # token = \"a1234567-40c7-9048-7bae-378687048181\"\n#\n#   ## Set timeout (default 5 seconds)\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n\n\n# # Read per-node and per-bucket metrics from Couchbase\n# [[inputs.couchbase]]\n#   ## specify servers via a url matching:\n#   ##  [protocol://][:password]@address[:port]\n#   ##  e.g.\n#   ##    http://couchbase-0.example.com/\n#   ##    http://admin:secret@couchbase-0.example.com:8091/\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no protocol is specified, HTTP is used.\n#   ## If no port is specified, 8091 is used.\n#   servers = [\"http://localhost:8091\"]\n#\n#   ## Filter bucket fields to include only here.\n#   # bucket_stats_included = [\"quota_percent_used\", \"ops_per_sec\", \"disk_fetches\", \"item_count\", \"disk_used\", \"data_used\", \"mem_used\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification (defaults to false)\n#   ## If set to false, tls_cert and tls_key are required\n#   # insecure_skip_verify = false\n\n\n# # Read CouchDB Stats from one or more servers\n# [[inputs.couchdb]]\n#   ## Works with CouchDB stats endpoints out of the box\n#   ## Multiple Hosts from which to read CouchDB stats:\n#   hosts = [\"http://localhost:8086/_stats\"]\n#\n#   ## Use HTTP Basic Authentication.\n#   # basic_username = \"telegraf\"\n#   # basic_password = \"p@ssw0rd\"\n\n\n# # Fetch metrics from a CSGO SRCDS\n# [[inputs.csgo]]\n#   ## Specify servers using the following format:\n#   ##    servers = [\n#   ##      [\"ip1:port1\", \"rcon_password1\"],\n#   ##      [\"ip2:port2\", \"rcon_password2\"],\n#   ##    ]\n#   #\n#   ## If no servers are specified, no data will be collected\n#   servers = []\n\n\n# # Input plugin for DC/OS metrics\n# [[inputs.dcos]]\n#   ## The DC/OS cluster URL.\n#   cluster_url = \"https://dcos-ee-master-1\"\n#\n#   ## The ID of the service account.\n#   service_account_id = \"telegraf\"\n#   ## The private key file for the service account.\n#   service_account_private_key = \"/etc/telegraf/telegraf-sa-key.pem\"\n#\n#   ## Path containing login token.  If set, will read on every gather.\n#   # token_file = \"/home/dcos/.dcos/token\"\n#\n#   ## In all filter options if both include and exclude are empty all items\n#   ## will be collected.  Arrays may contain glob patterns.\n#   ##\n#   ## Node IDs to collect metrics from.  If a node is excluded, no metrics will\n#   ## be collected for its containers or apps.\n#   # node_include = []\n#   # node_exclude = []\n#   ## Container IDs to collect container metrics from.\n#   # container_include = []\n#   # container_exclude = []\n#   ## Container IDs to collect app metrics from.\n#   # app_include = []\n#   # app_exclude = []\n#\n#   ## Maximum concurrent connections to the cluster.\n#   # max_connections = 10\n#   ## Maximum time to receive a response from cluster.\n#   # response_timeout = \"20s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n#\n#   ## Recommended filtering to reduce series cardinality.\n#   # [inputs.dcos.tagdrop]\n#   #   path = [\"/var/lib/mesos/slave/slaves/*\"]\n\n\n# # Read metrics from one or many disque servers\n# [[inputs.disque]]\n#   ## An array of URI to gather stats about. Specify an ip or hostname\n#   ## with optional port and password.\n#   ## ie disque://localhost, disque://10.10.3.33:18832, 10.0.0.1:10000, etc.\n#   ## If no servers are specified, then localhost is used as the host.\n#   servers = [\"localhost\"]\n\n\n# # Provide a native collection for dmsetup based statistics for dm-cache\n# [[inputs.dmcache]]\n#   ## Whether to report per-device stats or not\n#   per_device = true\n\n\n# # Query given DNS server and gives statistics\n# [[inputs.dns_query]]\n#   ## servers to query\n#   servers = [\"8.8.8.8\"]\n#\n#   ## Network is the network protocol name.\n#   # network = \"udp\"\n#\n#   ## Domains or subdomains to query.\n#   # domains = [\".\"]\n#\n#   ## Query record type.\n#   ## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n#   # record_type = \"A\"\n#\n#   ## Dns server port.\n#   # port = 53\n#\n#   ## Query timeout in seconds.\n#   # timeout = 2\n\n\n# # Read metrics about docker containers\n [[inputs.docker]]\n#   ## Docker Endpoint\n#   ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n#   ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n   endpoint = \"unix:///var/run/docker.sock\"\n#\n#   ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)\n#   gather_services = false\n#\n#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n#   source_tag = false\n#\n#   ## Containers to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all containers\n#   container_name_include = []\n#   container_name_exclude = []\n#\n#   ## Container states to include and exclude. Globs accepted.\n#   ## When empty only containers in the \"running\" state will be captured.\n#   ## example: container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n#   ## example: container_state_exclude = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n   container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n#   # container_state_exclude = []\n#\n#   ## Timeout for docker list, info, and stats commands\n   timeout = \"10s\"\n#\n#   ## Specifies for which classes a per-device metric should be issued\n#   ## Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...)\n#   ## Please note that this setting has no effect if 'perdevice' is set to 'true'\n#   # perdevice_include = [\"cpu\"]\n#\n#   ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice' values.\n#   ## Possible values are 'cpu', 'blkio' and 'network'\n#   ## Total 'cpu' is reported directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin.\n#   ## Please note that this setting has no effect if 'total' is set to 'false'\n#   # total_include = [\"cpu\", \"blkio\", \"network\"]\n#\n#   ## Which environment variables should we use as a tag\n#   ##tag_env = [\"JAVA_HOME\", \"HEAP_SIZE\"]\n#\n#   ## docker labels to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all labels as tags\n#   docker_label_include = []\n#   docker_label_exclude = []\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read statistics from one or many dovecot servers\n# [[inputs.dovecot]]\n#   ## specify dovecot servers via an address:port list\n#   ##  e.g.\n#   ##    localhost:24242\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   servers = [\"localhost:24242\"]\n#\n#   ## Type is one of \"user\", \"domain\", \"ip\", or \"global\"\n#   type = \"global\"\n#\n#   ## Wildcard matches like \"*.com\". An empty string \"\" is same as \"*\"\n#   ## If type = \"ip\" filters should be \u003cIP/network\u003e\n#   filters = [\"\"]\n\n\n# # Reads metrics from DPDK applications using v2 telemetry interface.\n# [[inputs.dpdk]]\n#   ## Path to DPDK telemetry socket. This shall point to v2 version of DPDK telemetry interface.\n#   # socket_path = \"/var/run/dpdk/rte/dpdk_telemetry.v2\"\n#\n#   ## Duration that defines how long the connected socket client will wait for a response before terminating connection.\n#   ## This includes both writing to and reading from socket. Since it's local socket access\n#   ## to a fast packet processing application, the timeout should be sufficient for most users.\n#   ## Setting the value to 0 disables the timeout (not recommended)\n#   # socket_access_timeout = \"200ms\"\n#\n#   ## Enables telemetry data collection for selected device types.\n#   ## Adding \"ethdev\" enables collection of telemetry from DPDK NICs (stats, xstats, link_status).\n#   ## Adding \"rawdev\" enables collection of telemetry from DPDK Raw Devices (xstats).\n#   # device_types = [\"ethdev\"]\n#\n#   ## List of custom, application-specific telemetry commands to query\n#   ## The list of available commands depend on the application deployed. Applications can register their own commands\n#   ##   via telemetry library API http://doc.dpdk.org/guides/prog_guide/telemetry_lib.html#registering-commands\n#   ## For e.g. L3 Forwarding with Power Management Sample Application this could be:\n#   ##   additional_commands = [\"/l3fwd-power/stats\"]\n#   # additional_commands = []\n#\n#   ## Allows turning off collecting data for individual \"ethdev\" commands.\n#   ## Remove \"/ethdev/link_status\" from list to start getting link status metrics.\n#   [inputs.dpdk.ethdev]\n#     exclude_commands = [\"/ethdev/link_status\"]\n#\n#   ## When running multiple instances of the plugin it's recommended to add a unique tag to each instance to identify\n#   ## metrics exposed by an instance of DPDK application. This is useful when multiple DPDK apps run on a single host.\n#   ##  [inputs.dpdk.tags]\n#   ##    dpdk_instance = \"my-fwd-app\"\n\n\n# # Read metrics about docker containers from Fargate/ECS v2, v3 meta endpoints.\n# [[inputs.ecs]]\n#   ## ECS metadata url.\n#   ## Metadata v2 API is used if set explicitly. Otherwise,\n#   ## v3 metadata endpoint API is used if available.\n#   # endpoint_url = \"\"\n#\n#   ## Containers to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all containers\n#   # container_name_include = []\n#   # container_name_exclude = []\n#\n#   ## Container states to include and exclude. Globs accepted.\n#   ## When empty only containers in the \"RUNNING\" state will be captured.\n#   ## Possible values are \"NONE\", \"PULLED\", \"CREATED\", \"RUNNING\",\n#   ## \"RESOURCES_PROVISIONED\", \"STOPPED\".\n#   # container_status_include = []\n#   # container_status_exclude = []\n#\n#   ## ecs labels to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all labels as tags\n#   ecs_label_include = [ \"com.amazonaws.ecs.*\" ]\n#   ecs_label_exclude = []\n#\n#   ## Timeout for queries.\n#   # timeout = \"5s\"\n\n\n# # Read stats from one or more Elasticsearch servers or clusters\n# [[inputs.elasticsearch]]\n#   ## specify a list of one or more Elasticsearch servers\n#   # you can add username and password to your url to use basic authentication:\n#   # servers = [\"http://user:pass@localhost:9200\"]\n#   servers = [\"http://localhost:9200\"]\n#\n#   ## Timeout for HTTP requests to the elastic search server(s)\n#   http_timeout = \"5s\"\n#\n#   ## When local is true (the default), the node will read only its own stats.\n#   ## Set local to false when you want to read the node stats from all nodes\n#   ## of the cluster.\n#   local = true\n#\n#   ## Set cluster_health to true when you want to also obtain cluster health stats\n#   cluster_health = false\n#\n#   ## Adjust cluster_health_level when you want to also obtain detailed health stats\n#   ## The options are\n#   ##  - indices (default)\n#   ##  - cluster\n#   # cluster_health_level = \"indices\"\n#\n#   ## Set cluster_stats to true when you want to also obtain cluster stats.\n#   cluster_stats = false\n#\n#   ## Only gather cluster_stats from the master node. To work this require local = true\n#   cluster_stats_only_from_master = true\n#\n#   ## Indices to collect; can be one or more indices names or _all\n#   ## Use of wildcards is allowed. Use a wildcard at the end to retrieve index names that end with a changing value, like a date.\n#   indices_include = [\"_all\"]\n#\n#   ## One of \"shards\", \"cluster\", \"indices\"\n#   indices_level = \"shards\"\n#\n#   ## node_stats is a list of sub-stats that you want to have gathered. Valid options\n#   ## are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\", \"fs\", \"transport\", \"http\",\n#   ## \"breaker\". Per default, all stats are gathered.\n#   # node_stats = [\"jvm\", \"http\"]\n#\n#   ## HTTP Basic Authentication username and password.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Sets the number of most recent indices to return for indices that are configured with a date-stamped suffix.\n#   ## Each 'indices_include' entry ending with a wildcard (*) or glob matching pattern will group together all indices that match it, and sort them\n#   ## by the date or number after the wildcard. Metrics then are gathered for only the 'num_most_recent_indices' amount of most recent indices.\n#   # num_most_recent_indices = 0\n\n\n# # Derive metrics from aggregating Elasticsearch query results\n# [[inputs.elasticsearch_query]]\n#   ## The full HTTP endpoint URL for your Elasticsearch instance\n#   ## Multiple urls can be specified as part of the same cluster,\n#   ## this means that only ONE of the urls will be written to each interval.\n#   urls = [ \"http://node1.es.example.com:9200\" ] # required.\n#\n#   ## Elasticsearch client timeout, defaults to \"5s\".\n#   # timeout = \"5s\"\n#\n#   ## Set to true to ask Elasticsearch a list of all cluster nodes,\n#   ## thus it is not necessary to list all nodes in the urls config option\n#   # enable_sniffer = false\n#\n#   ## Set the interval to check if the Elasticsearch nodes are available\n#   ## This option is only used if enable_sniffer is also set (0s to disable it)\n#   # health_check_interval = \"10s\"\n#\n#   ## HTTP basic authentication details (eg. when using x-pack)\n#   # username = \"telegraf\"\n#   # password = \"mypassword\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   [[inputs.elasticsearch_query.aggregation]]\n#     ## measurement name for the results of the aggregation query\n#     measurement_name = \"measurement\"\n#\n#     ## Elasticsearch indexes to query (accept wildcards).\n#     index = \"index-*\"\n#\n#     ## The date/time field in the Elasticsearch index (mandatory).\n#     date_field = \"@timestamp\"\n#\n#     ## If the field used for the date/time field in Elasticsearch is also using\n#     ## a custom date/time format it may be required to provide the format to\n#     ## correctly parse the field.\n#     ##\n#     ## If using one of the built in elasticsearch formats this is not required.\n#     # date_field_custom_format = \"\"\n#\n#     ## Time window to query (eg. \"1m\" to query documents from last minute).\n#     ## Normally should be set to same as collection interval\n#     query_period = \"1m\"\n#\n#     ## Lucene query to filter results\n#     # filter_query = \"*\"\n#\n#     ## Fields to aggregate values (must be numeric fields)\n#     # metric_fields = [\"metric\"]\n#\n#     ## Aggregation function to use on the metric fields\n#     ## Must be set if 'metric_fields' is set\n#     ## Valid values are: avg, sum, min, max, sum\n#     # metric_function = \"avg\"\n#\n#     ## Fields to be used as tags\n#     ## Must be text, non-analyzed fields. Metric aggregations are performed per tag\n#     # tags = [\"field.keyword\", \"field2.keyword\"]\n#\n#     ## Set to true to not ignore documents when the tag(s) above are missing\n#     # include_missing_tag = false\n#\n#     ## String value of the tag when the tag does not exist\n#     ## Used when include_missing_tag is true\n#     # missing_tag_value = \"null\"\n\n\n# # Returns ethtool statistics for given interfaces\n# [[inputs.ethtool]]\n#   ## List of interfaces to pull metrics for\n#   # interface_include = [\"eth0\"]\n#\n#   ## List of interfaces to ignore when pulling metrics.\n#   # interface_exclude = [\"eth1\"]\n#\n#   ## Some drivers declare statistics with extra whitespace, different spacing,\n#   ## and mix cases. This list, when enabled, can be used to clean the keys.\n#   ## Here are the current possible normalizations:\n#   ##  * snakecase: converts fooBarBaz to foo_bar_baz\n#   ##  * trim: removes leading and trailing whitespace\n#   ##  * lower: changes all capitalized letters to lowercase\n#   ##  * underscore: replaces spaces with underscores\n#   # normalize_keys = [\"snakecase\", \"trim\", \"lower\", \"underscore\"]\n\n\n# # Read metrics from one or more commands that can output to stdout\n# [[inputs.exec]]\n#   ## Commands array\n#   commands = [\n#     \"/tmp/test.sh\",\n#     \"/usr/bin/mycollector --foo=bar\",\n#     \"/tmp/collect_*.sh\"\n#   ]\n#\n#   ## Timeout for each command to complete.\n#   timeout = \"5s\"\n#\n#   ## measurement name suffix (for separating different commands)\n#   name_suffix = \"_mycollector\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read metrics from fail2ban.\n# [[inputs.fail2ban]]\n#   ## Use sudo to run fail2ban-client\n#   use_sudo = false\n\n\n# # Read devices value(s) from a Fibaro controller\n# [[inputs.fibaro]]\n#   ## Required Fibaro controller address/hostname.\n#   ## Note: at the time of writing this plugin, Fibaro only implemented http - no https available\n#   url = \"http://\u003ccontroller\u003e:80\"\n#\n#   ## Required credentials to access the API (http://\u003ccontroller/api/\u003ccomponent\u003e)\n#   username = \"\u003cusername\u003e\"\n#   password = \"\u003cpassword\u003e\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n\n\n# # Parse a complete file each interval\n# [[inputs.file]]\n#   ## Files to parse each interval.  Accept standard unix glob matching rules,\n#   ## as well as ** to match recursive files and directories.\n#   files = [\"/tmp/metrics.out\"]\n#\n#\n#   ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n#   ## to disable. Cautious when file name variation is high, this can increase the cardinality\n#   ## significantly. Read more about cardinality here:\n#   ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n#   # file_tag = \"\"\n#   #\n#\n#   ## Character encoding to use when interpreting the file contents.  Invalid\n#   ## characters are replaced using the unicode replacement character.  When set\n#   ## to the empty string the data is not decoded to text.\n#   ##   ex: character_encoding = \"utf-8\"\n#   ##       character_encoding = \"utf-16le\"\n#   ##       character_encoding = \"utf-16be\"\n#   ##       character_encoding = \"\"\n#   # character_encoding = \"\"\n#\n#   ## The dataformat to be read from files\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Count files in a directory\n# [[inputs.filecount]]\n#   ## Directories to gather stats about.\n#   ## This accept standard unit glob matching rules, but with the addition of\n#   ## ** as a \"super asterisk\". ie:\n#   ##   /var/log/**    -\u003e recursively find all directories in /var/log and count files in each directories\n#   ##   /var/log/*/*   -\u003e find all directories with a parent dir in /var/log and count files in each directories\n#   ##   /var/log       -\u003e count all files in /var/log and all of its subdirectories\n#   directories = [\"/var/cache/apt/archives\"]\n#\n#   ## Only count files that match the name pattern. Defaults to \"*\".\n#   name = \"*.deb\"\n#\n#   ## Count files in subdirectories. Defaults to true.\n#   recursive = false\n#\n#   ## Only count regular files. Defaults to true.\n#   regular_only = true\n#\n#   ## Follow all symlinks while walking the directory tree. Defaults to false.\n#   follow_symlinks = false\n#\n#   ## Only count files that are at least this size. If size is\n#   ## a negative number, only count files that are smaller than the\n#   ## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n#   ## Without quotes and units, interpreted as size in bytes.\n#   size = \"0B\"\n#\n#   ## Only count files that have not been touched for at least this\n#   ## duration. If mtime is negative, only count files that have been\n#   ## touched in this duration. Defaults to \"0s\".\n#   mtime = \"0s\"\n\n\n# # Read stats about given file(s)\n# [[inputs.filestat]]\n#   ## Files to gather stats about.\n#   ## These accept standard unix glob matching rules, but with the addition of\n#   ## ** as a \"super asterisk\". ie:\n#   ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n#   ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n#   ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n#   ##\n#   ## See https://github.com/gobwas/glob for more examples\n#   ##\n#   files = [\"/var/log/**.log\"]\n#\n#   ## If true, read the entire file and calculate an md5 checksum.\n#   md5 = false\n\n\n# # Read real time temps from fireboard.io servers\n# [[inputs.fireboard]]\n#   ## Specify auth token for your account\n#   auth_token = \"invalidAuthToken\"\n#   ## You can override the fireboard server URL if necessary\n#   # url = https://fireboard.io/api/v1/devices.json\n#   ## You can set a different http_timeout if you need to\n#   ## You should set a string using an number and time indicator\n#   ## for example \"12s\" for 12 seconds.\n#   # http_timeout = \"4s\"\n\n\n# # Read metrics exposed by fluentd in_monitor plugin\n# [[inputs.fluentd]]\n#   ## This plugin reads information exposed by fluentd (using /api/plugins.json endpoint).\n#   ##\n#   ## Endpoint:\n#   ## - only one URI is allowed\n#   ## - https is not supported\n#   endpoint = \"http://localhost:24220/api/plugins.json\"\n#\n#   ## Define which plugins have to be excluded (based on \"type\" field - e.g. monitor_agent)\n#   exclude = [\n# \t  \"monitor_agent\",\n# \t  \"dummy\",\n#   ]\n\n\n# # Gather repository information from GitHub hosted repositories.\n# [[inputs.github]]\n#   ## List of repositories to monitor.\n#   repositories = [\n# \t  \"influxdata/telegraf\",\n# \t  \"influxdata/influxdb\"\n#   ]\n#\n#   ## Github API access token.  Unauthenticated requests are limited to 60 per hour.\n#   # access_token = \"\"\n#\n#   ## Github API enterprise url. Github Enterprise accounts must specify their base url.\n#   # enterprise_base_url = \"\"\n#\n#   ## Timeout for HTTP requests.\n#   # http_timeout = \"5s\"\n#\n#   ## List of additional fields to query.\n# \t## NOTE: Getting those fields might involve issuing additional API-calls, so please\n# \t##       make sure you do not exceed the rate-limit of GitHub.\n# \t##\n# \t## Available fields are:\n# \t## \t- pull-requests\t\t\t-- number of open and closed pull requests (2 API-calls per repository)\n#   # additional_fields = []\n\n\n# # Read flattened metrics from one or more GrayLog HTTP endpoints\n# [[inputs.graylog]]\n#   ## API endpoint, currently supported API:\n#   ##\n#   ##   - multiple  (e.g. http://\u003chost\u003e:9000/api/system/metrics/multiple)\n#   ##   - namespace (e.g. http://\u003chost\u003e:9000/api/system/metrics/namespace/{namespace})\n#   ##\n#   ## For namespace endpoint, the metrics array will be ignored for that call.\n#   ## Endpoint can contain namespace and multiple type calls.\n#   ##\n#   ## Please check http://[graylog-server-ip]:9000/api/api-browser for full list\n#   ## of endpoints\n#   servers = [\n#     \"http://[graylog-server-ip]:9000/api/system/metrics/multiple\",\n#   ]\n#\n#   ## Set timeout (default 5 seconds)\n#   # timeout = \"5s\"\n#\n#   ## Metrics list\n#   ## List of metrics can be found on Graylog webservice documentation.\n#   ## Or by hitting the web service api at:\n#   ##   http://[graylog-host]:9000/api/system/metrics\n#   metrics = [\n#     \"jvm.cl.loaded\",\n#     \"jvm.memory.pools.Metaspace.committed\"\n#   ]\n#\n#   ## Username and password\n#   username = \"\"\n#   password = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics of haproxy, via socket or csv stats page\n# [[inputs.haproxy]]\n#   ## An array of address to gather stats about. Specify an ip on hostname\n#   ## with optional port. ie localhost, 10.10.3.33:1936, etc.\n#   ## Make sure you specify the complete path to the stats endpoint\n#   ## including the protocol, ie http://10.10.3.33:1936/haproxy?stats\n#\n#   ## If no servers are specified, then default to 127.0.0.1:1936/haproxy?stats\n#   servers = [\"http://myhaproxy.com:1936/haproxy?stats\"]\n#\n#   ## Credentials for basic HTTP authentication\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## You can also use local socket with standard wildcard globbing.\n#   ## Server address not starting with 'http' will be treated as a possible\n#   ## socket, so both examples below are valid.\n#   # servers = [\"socket:/run/haproxy/admin.sock\", \"/run/haproxy/*.sock\"]\n#\n#   ## By default, some of the fields are renamed from what haproxy calls them.\n#   ## Setting this option to true results in the plugin keeping the original\n#   ## field names.\n#   # keep_field_names = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Monitor disks' temperatures using hddtemp\n# [[inputs.hddtemp]]\n#   ## By default, telegraf gathers temps data from all disks detected by the\n#   ## hddtemp.\n#   ##\n#   ## Only collect temps from the selected disks.\n#   ##\n#   ## A * as the device name will return the temperature values of all disks.\n#   ##\n#   # address = \"127.0.0.1:7634\"\n#   # devices = [\"sda\", \"*\"]\n\n\n# # Read formatted metrics from one or more HTTP endpoints\n# [[inputs.http]]\n#   ## One or more URLs from which to read formatted metrics\n#   urls = [\n#     \"http://localhost/metrics\"\n#   ]\n#\n#   ## HTTP method\n#   # method = \"GET\"\n#\n#   ## Optional HTTP headers\n#   # headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## Optional file with Bearer token\n#   ## file content is added as an Authorization header\n#   # bearer_token = \"/path/to/file\"\n#\n#   ## Optional HTTP Basic Auth Credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## HTTP entity-body to send with POST/PUT requests.\n#   # body = \"\"\n#\n#   ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n#   ## compress body or \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## HTTP Proxy support\n#   # http_proxy_url = \"\"\n#\n#   ## OAuth2 Client Credentials Grant\n#   # client_id = \"clientid\"\n#   # client_secret = \"secret\"\n#   # token_url = \"https://indentityprovider/oauth2/v1/token\"\n#   # scopes = [\"urn:opc:idm:__myscopes__\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional Cookie authentication\n#   # cookie_auth_url = \"https://localhost/authMe\"\n#   # cookie_auth_method = \"POST\"\n#   # cookie_auth_username = \"username\"\n#   # cookie_auth_password = \"pa$$word\"\n#   # cookie_auth_headers = '{\"Content-Type\": \"application/json\", \"X-MY-HEADER\":\"hello\"}'\n#   # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n#   ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n#   # cookie_auth_renewal = \"5m\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n#\n#   ## List of success status codes\n#   # success_status_codes = [200]\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   # data_format = \"influx\"\n\n\n# # HTTP/HTTPS request given an address a method and a timeout\n# [[inputs.http_response]]\n#   ## List of urls to query.\n#   # urls = [\"http://localhost\"]\n#\n#   ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n#   # http_proxy = \"http://localhost:8888\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## HTTP Request Method\n#   # method = \"GET\"\n#\n#   ## Whether to follow redirects from the server (defaults to false)\n#   # follow_redirects = false\n#\n#   ## Optional file with Bearer token\n#   ## file content is added as an Authorization header\n#   # bearer_token = \"/path/to/file\"\n#\n#   ## Optional HTTP Basic Auth Credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional HTTP Request Body\n#   # body = '''\n#   # {'fake':'data'}\n#   # '''\n#\n#   ## Optional name of the field that will contain the body of the response.\n#   ## By default it is set to an empty String indicating that the body's content won't be added\n#   # response_body_field = ''\n#\n#   ## Maximum allowed HTTP response body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   ## If the response body size exceeds this limit a \"body_read_error\" will be raised\n#   # response_body_max_size = \"32MiB\"\n#\n#   ## Optional substring or regex match in body of the response (case sensitive)\n#   # response_string_match = \"\\\"service_status\\\": \\\"up\\\"\"\n#   # response_string_match = \"ok\"\n#   # response_string_match = \"\\\".*_status\\\".?:.?\\\"up\\\"\"\n#\n#   ## Expected response status code.\n#   ## The status code of the response is compared to this value. If they match, the field\n#   ## \"response_status_code_match\" will be 1, otherwise it will be 0. If the\n#   ## expected status code is 0, the check is disabled and the field won't be added.\n#   # response_status_code = 0\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## HTTP Request Headers (all values must be strings)\n#   # [inputs.http_response.headers]\n#   #   Host = \"github.com\"\n#\n#   ## Optional setting to map response http headers into tags\n#   ## If the http header is not present on the request, no corresponding tag will be added\n#   ## If multiple instances of the http header are present, only the first value will be used\n#   # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n#\n#   ## Interface to use when dialing an address\n#   # interface = \"eth0\"\n\n\n# # Read flattened metrics from one or more JSON HTTP endpoints\n# [[inputs.httpjson]]\n#   ## DEPRECATED: The 'httpjson' plugin is deprecated in version 1.6.0, use 'inputs.http' instead.\n#   ## NOTE This plugin only reads numerical measurements, strings and booleans\n#   ## will be ignored.\n#\n#   ## URL of each server in the service's cluster\n#   servers = [\n#     \"http://localhost:9999/stats/\",\n#     \"http://localhost:9998/stats/\",\n#   ]\n#   ## Set response_timeout (default 5 seconds)\n#   response_timeout = \"5s\"\n#\n#   ## HTTP method to use: GET or POST (case-sensitive)\n#   method = \"GET\"\n#\n#   ## List of tag names to extract from top-level of JSON server response\n#   # tag_keys = [\n#   #   \"my_tag_1\",\n#   #   \"my_tag_2\"\n#   # ]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## HTTP parameters (all values must be strings).  For \"GET\" requests, data\n#   ## will be included in the query.  For \"POST\" requests, data will be included\n#   ## in the request body as \"x-www-form-urlencoded\".\n#   # [inputs.httpjson.parameters]\n#   #   event_type = \"cpu_spike\"\n#   #   threshold = \"0.75\"\n#\n#   ## HTTP Headers (all values must be strings)\n#   # [inputs.httpjson.headers]\n#   #   X-Auth-Token = \"my-xauth-token\"\n#   #   apiVersion = \"v1\"\n\n\n# # Gather Icinga2 status\n# [[inputs.icinga2]]\n#   ## Required Icinga2 server address\n#   # server = \"https://localhost:5665\"\n#\n#   ## Required Icinga2 object type (\"services\" or \"hosts\")\n#   # object_type = \"services\"\n#\n#   ## Credentials for basic HTTP authentication\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## Maximum time to receive response.\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Gets counters from all InfiniBand cards and ports installed\n# [[inputs.infiniband]]\n#   # no configuration\n\n\n# # Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints\n# [[inputs.influxdb]]\n#   ## Works with InfluxDB debug endpoints out of the box,\n#   ## but other services can use this format too.\n#   ## See the influxdb plugin's README for more details.\n#\n#   ## Multiple URLs from which to read InfluxDB-formatted JSON\n#   ## Default is \"http://localhost:8086/debug/vars\".\n#   urls = [\n#     \"http://localhost:8086/debug/vars\"\n#   ]\n#\n#   ## Username and password to send using HTTP Basic Authentication.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## http request \u0026 header timeout\n#   timeout = \"5s\"\n\n\n# # Intel PowerStat plugin enables monitoring of platform metrics (power, TDP) and Core metrics like temperature, power and utilization.\n# [[inputs.intel_powerstat]]\n#   ## All global metrics are always collected by Intel PowerStat plugin.\n#   ## User can choose which per-CPU metrics are monitored by the plugin in cpu_metrics array.\n#   ## Empty array means no per-CPU specific metrics will be collected by the plugin - in this case only platform level\n#   ## telemetry will be exposed by Intel PowerStat plugin.\n#   ## Supported options:\n#   ## \"cpu_frequency\", \"cpu_busy_frequency\", \"cpu_temperature\", \"cpu_c1_state_residency\", \"cpu_c6_state_residency\", \"cpu_busy_cycles\"\n#   # cpu_metrics = []\n\n\n# # Collect statistics about itself\n# [[inputs.internal]]\n#   ## If true, collect telegraf memory stats.\n#   # collect_memstats = true\n\n\n# # Monitors internet speed using speedtest.net service\n# [[inputs.internet_speed]]\n#   ## Sets if runs file download test\n#   # enable_file_download = false\n#\n#   ## Caches the closest server location\n#   # cache = false\n\n\n# # This plugin gathers interrupts data from /proc/interrupts and /proc/softirqs.\n# [[inputs.interrupts]]\n#   ## When set to true, cpu metrics are tagged with the cpu.  Otherwise cpu is\n#   ## stored as a field.\n#   ##\n#   ## The default is false for backwards compatibility, and will be changed to\n#   ## true in a future version.  It is recommended to set to true on new\n#   ## deployments.\n#   # cpu_as_tag = false\n#\n#   ## To filter which IRQs to collect, make use of tagpass / tagdrop, i.e.\n#   # [inputs.interrupts.tagdrop]\n#   #   irq = [ \"NET_RX\", \"TASKLET\" ]\n\n\n# # Read metrics from the bare metal servers via IPMI\n# [[inputs.ipmi_sensor]]\n#   ## optionally specify the path to the ipmitool executable\n#   # path = \"/usr/bin/ipmitool\"\n#   ##\n#   ## Setting 'use_sudo' to true will make use of sudo to run ipmitool.\n#   ## Sudo must be configured to allow the telegraf user to run ipmitool\n#   ## without a password.\n#   # use_sudo = false\n#   ##\n#   ## optionally force session privilege level. Can be CALLBACK, USER, OPERATOR, ADMINISTRATOR\n#   # privilege = \"ADMINISTRATOR\"\n#   ##\n#   ## optionally specify one or more servers via a url matching\n#   ##  [username[:password]@][protocol[(address)]]\n#   ##  e.g.\n#   ##    root:passwd@lan(127.0.0.1)\n#   ##\n#   ## if no servers are specified, local machine sensor stats will be queried\n#   ##\n#   # servers = [\"USERID:PASSW0RD@lan(192.168.1.1)\"]\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'timeout' to avoid\n#   ## gaps or overlap in pulled data\n#   interval = \"30s\"\n#\n#   ## Timeout for the ipmitool command to complete\n#   timeout = \"20s\"\n#\n#   ## Schema Version: (Optional, defaults to version 1)\n#   metric_version = 2\n#\n#   ## Optionally provide the hex key for the IMPI connection.\n#   # hex_key = \"\"\n#\n#   ## If ipmitool should use a cache\n#   ## for me ipmitool runs about 2 to 10 times faster with cache enabled on HP G10 servers (when using ubuntu20.04)\n#   ## the cache file may not work well for you if some sensors come up late\n#   # use_cache = false\n#\n#   ## Path to the ipmitools cache file (defaults to OS temp dir)\n#   ## The provided path must exist and must be writable\n#   # cache_path = \"\"\n\n\n# # Gather packets and bytes counters from Linux ipsets\n# [[inputs.ipset]]\n#   ## By default, we only show sets which have already matched at least 1 packet.\n#   ## set include_unmatched_sets = true to gather them all.\n#   include_unmatched_sets = false\n#   ## Adjust your sudo settings appropriately if using this option (\"sudo ipset save\")\n#   use_sudo = false\n#   ## The default timeout of 1s for ipset execution can be overridden here:\n#   # timeout = \"1s\"\n\n\n# # Gather packets and bytes throughput from iptables\n# [[inputs.iptables]]\n#   ## iptables require root access on most systems.\n#   ## Setting 'use_sudo' to true will make use of sudo to run iptables.\n#   ## Users must configure sudo to allow telegraf user to run iptables with no password.\n#   ## iptables can be restricted to only list command \"iptables -nvL\".\n#   use_sudo = false\n#   ## Setting 'use_lock' to true runs iptables with the \"-w\" option.\n#   ## Adjust your sudo settings appropriately if using this option (\"iptables -w 5 -nvl\")\n#   use_lock = false\n#   ## Define an alternate executable, such as \"ip6tables\". Default is \"iptables\".\n#   # binary = \"ip6tables\"\n#   ## defines the table to monitor:\n#   table = \"filter\"\n#   ## defines the chains to monitor.\n#   ## NOTE: iptables rules without a comment will not be monitored.\n#   ## Read the plugin documentation for more information.\n#   chains = [ \"INPUT\" ]\n\n\n# # Collect virtual and real server stats from Linux IPVS\n# [[inputs.ipvs]]\n#   # no configuration\n\n\n# # Read jobs and cluster metrics from Jenkins instances\n# [[inputs.jenkins]]\n#   ## The Jenkins URL in the format \"schema://host:port\"\n#   url = \"http://my-jenkins-instance:8080\"\n#   # username = \"admin\"\n#   # password = \"admin\"\n#\n#   ## Set response_timeout\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use SSL but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional Max Job Build Age filter\n#   ## Default 1 hour, ignore builds older than max_build_age\n#   # max_build_age = \"1h\"\n#\n#   ## Optional Sub Job Depth filter\n#   ## Jenkins can have unlimited layer of sub jobs\n#   ## This config will limit the layers of pulling, default value 0 means\n#   ## unlimited pulling until no more sub jobs\n#   # max_subjob_depth = 0\n#\n#   ## Optional Sub Job Per Layer\n#   ## In workflow-multibranch-plugin, each branch will be created as a sub job.\n#   ## This config will limit to call only the lasted branches in each layer,\n#   ## empty will use default value 10\n#   # max_subjob_per_layer = 10\n#\n#   ## Jobs to include or exclude from gathering\n#   ## When using both lists, job_exclude has priority.\n#   ## Wildcards are supported: [ \"jobA/*\", \"jobB/subjob1/*\"]\n#   # job_include = [ \"*\" ]\n#   # job_exclude = [ ]\n#\n#   ## Nodes to include or exclude from gathering\n#   ## When using both lists, node_exclude has priority.\n#   # node_include = [ \"*\" ]\n#   # node_exclude = [ ]\n#\n#   ## Worker pool for jenkins plugin only\n#   ## Empty this field will use default value 5\n#   # max_connections = 5\n\n\n# # Read JMX metrics through Jolokia\n# [[inputs.jolokia]]\n#   ## DEPRECATED: The 'jolokia' plugin is deprecated in version 1.5.0, use 'inputs.jolokia2' instead.\n#   # DEPRECATED: the jolokia plugin has been deprecated in favor of the\n#   # jolokia2 plugin\n#   # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n#\n#   ## This is the context root used to compose the jolokia url\n#   ## NOTE that Jolokia requires a trailing slash at the end of the context root\n#   ## NOTE that your jolokia security policy must allow for POST requests.\n#   context = \"/jolokia/\"\n#\n#   ## This specifies the mode used\n#   # mode = \"proxy\"\n#   #\n#   ## When in proxy mode this section is used to specify further\n#   ## proxy address configurations.\n#   ## Remember to change host address to fit your environment.\n#   # [inputs.jolokia.proxy]\n#   #   host = \"127.0.0.1\"\n#   #   port = \"8080\"\n#\n#   ## Optional http timeouts\n#   ##\n#   ## response_header_timeout, if non-zero, specifies the amount of time to wait\n#   ## for a server's response headers after fully writing the request.\n#   # response_header_timeout = \"3s\"\n#   ##\n#   ## client_timeout specifies a time limit for requests made by this client.\n#   ## Includes connection time, any redirects, and reading the response body.\n#   # client_timeout = \"4s\"\n#\n#   ## Attribute delimiter\n#   ##\n#   ## When multiple attributes are returned for a single\n#   ## [inputs.jolokia.metrics], the field name is a concatenation of the metric\n#   ## name, and the attribute name, separated by the given delimiter.\n#   # delimiter = \"_\"\n#\n#   ## List of servers exposing jolokia read service\n#   [[inputs.jolokia.servers]]\n#     name = \"as-server-01\"\n#     host = \"127.0.0.1\"\n#     port = \"8080\"\n#     # username = \"myuser\"\n#     # password = \"mypassword\"\n#\n#   ## List of metrics collected on above servers\n#   ## Each metric consists in a name, a jmx path and either\n#   ## a pass or drop slice attribute.\n#   ## This collect all heap memory usage metrics.\n#   [[inputs.jolokia.metrics]]\n#     name = \"heap_memory_usage\"\n#     mbean  = \"java.lang:type=Memory\"\n#     attribute = \"HeapMemoryUsage\"\n#\n#   ## This collect thread counts metrics.\n#   [[inputs.jolokia.metrics]]\n#     name = \"thread_count\"\n#     mbean  = \"java.lang:type=Threading\"\n#     attribute = \"TotalStartedThreadCount,ThreadCount,DaemonThreadCount,PeakThreadCount\"\n#\n#   ## This collect number of class loaded/unloaded counts metrics.\n#   [[inputs.jolokia.metrics]]\n#     name = \"class_count\"\n#     mbean  = \"java.lang:type=ClassLoading\"\n#     attribute = \"LoadedClassCount,UnloadedClassCount,TotalLoadedClassCount\"\n\n\n# # Read JMX metrics from a Jolokia REST agent endpoint\n# [[inputs.jolokia2_agent]]\n#   # default_tag_prefix      = \"\"\n#   # default_field_prefix    = \"\"\n#   # default_field_separator = \".\"\n#\n#   # Add agents URLs to query\n#   urls = [\"http://localhost:8080/jolokia\"]\n#   # username = \"\"\n#   # password = \"\"\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS config\n#   # tls_ca   = \"/var/private/ca.pem\"\n#   # tls_cert = \"/var/private/client.pem\"\n#   # tls_key  = \"/var/private/client-key.pem\"\n#   # insecure_skip_verify = false\n#\n#   ## Add metrics to read\n#   [[inputs.jolokia2_agent.metric]]\n#     name  = \"java_runtime\"\n#     mbean = \"java.lang:type=Runtime\"\n#     paths = [\"Uptime\"]\n\n\n# # Read JMX metrics from a Jolokia REST proxy endpoint\n# [[inputs.jolokia2_proxy]]\n#   # default_tag_prefix      = \"\"\n#   # default_field_prefix    = \"\"\n#   # default_field_separator = \".\"\n#\n#   ## Proxy agent\n#   url = \"http://localhost:8080/jolokia\"\n#   # username = \"\"\n#   # password = \"\"\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS config\n#   # tls_ca   = \"/var/private/ca.pem\"\n#   # tls_cert = \"/var/private/client.pem\"\n#   # tls_key  = \"/var/private/client-key.pem\"\n#   # insecure_skip_verify = false\n#\n#   ## Add proxy targets to query\n#   # default_target_username = \"\"\n#   # default_target_password = \"\"\n#   [[inputs.jolokia2_proxy.target]]\n#     url = \"service:jmx:rmi:///jndi/rmi://targethost:9999/jmxrmi\"\n#     # username = \"\"\n#     # password = \"\"\n#\n#   ## Add metrics to read\n#   [[inputs.jolokia2_proxy.metric]]\n#     name  = \"java_runtime\"\n#     mbean = \"java.lang:type=Runtime\"\n#     paths = [\"Uptime\"]\n\n\n# # Read Kapacitor-formatted JSON metrics from one or more HTTP endpoints\n# [[inputs.kapacitor]]\n#   ## Multiple URLs from which to read Kapacitor-formatted JSON\n#   ## Default is \"http://localhost:9092/kapacitor/v1/debug/vars\".\n#   urls = [\n#     \"http://localhost:9092/kapacitor/v1/debug/vars\"\n#   ]\n#\n#   ## Time limit for http requests\n#   timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Get kernel statistics from /proc/vmstat\n# [[inputs.kernel_vmstat]]\n#   # no configuration\n\n\n# # Read status information from one or more Kibana servers\n# [[inputs.kibana]]\n#   ## Specify a list of one or more Kibana servers\n#   servers = [\"http://localhost:5601\"]\n#\n#   ## Timeout for HTTP requests\n#   timeout = \"5s\"\n#\n#   ## HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from the Kubernetes api\n# [[inputs.kube_inventory]]\n#   ## URL for the Kubernetes API\n#   url = \"https://127.0.0.1\"\n#\n#   ## Namespace to use. Set to \"\" to use all namespaces.\n#   # namespace = \"default\"\n#\n#   ## Use bearer token for authorization. ('bearer_token' takes priority)\n#   ## If both of these are empty, we'll use the default serviceaccount:\n#   ## at: /run/secrets/kubernetes.io/serviceaccount/token\n#   # bearer_token = \"/path/to/bearer/token\"\n#   ## OR\n#   # bearer_token_string = \"abc_123\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional Resources to exclude from gathering\n#   ## Leave them with blank with try to gather everything available.\n#   ## Values can be - \"daemonsets\", deployments\", \"endpoints\", \"ingress\", \"nodes\",\n#   ## \"persistentvolumes\", \"persistentvolumeclaims\", \"pods\", \"services\", \"statefulsets\"\n#   # resource_exclude = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n#\n#   ## Optional Resources to include when gathering\n#   ## Overrides resource_exclude if both set.\n#   # resource_include = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n#\n#   ## selectors to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all selectors as tags\n#   ## selector_exclude overrides selector_include if both set.\n#   # selector_include = []\n#   # selector_exclude = [\"*\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/path/to/cafile\"\n#   # tls_cert = \"/path/to/certfile\"\n#   # tls_key = \"/path/to/keyfile\"\n#   # tls_server_name = \"kubernetes.example.com\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from the kubernetes kubelet api\n# [[inputs.kubernetes]]\n#   ## URL for the kubelet\n#   url = \"http://127.0.0.1:10255\"\n#\n#   ## Use bearer token for authorization. ('bearer_token' takes priority)\n#   ## If both of these are empty, we'll use the default serviceaccount:\n#   ## at: /run/secrets/kubernetes.io/serviceaccount/token\n#   # bearer_token = \"/path/to/bearer/token\"\n#   ## OR\n#   # bearer_token_string = \"abc_123\"\n#\n#   ## Pod labels to be added as tags.  An empty array for both include and\n#   ## exclude will include all labels.\n#   # label_include = []\n#   # label_exclude = [\"*\"]\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from a LeoFS Server via SNMP\n# [[inputs.leofs]]\n#   ## An array of URLs of the form:\n#   ##   host [ \":\" port]\n#   servers = [\"127.0.0.1:4020\"]\n\n\n# # Provides Linux sysctl fs metrics\n# [[inputs.linux_sysctl_fs]]\n#   # no configuration\n\n\n# # Read metrics exposed by Logstash\n# [[inputs.logstash]]\n#   ## The URL of the exposed Logstash API endpoint.\n#   url = \"http://127.0.0.1:9600\"\n#\n#   ## Use Logstash 5 single pipeline API, set to true when monitoring\n#   ## Logstash 5.\n#   # single_pipeline = false\n#\n#   ## Enable optional collection components.  Can contain\n#   ## \"pipelines\", \"process\", and \"jvm\".\n#   # collect = [\"pipelines\", \"process\", \"jvm\"]\n#\n#   ## Timeout for HTTP requests.\n#   # timeout = \"5s\"\n#\n#   ## Optional HTTP Basic Auth credentials.\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config.\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Use TLS but skip chain \u0026 host verification.\n#   # insecure_skip_verify = false\n#\n#   ## Optional HTTP headers.\n#   # [inputs.logstash.headers]\n#   #   \"X-Special-Header\" = \"Special-Value\"\n\n\n# # Read metrics from local Lustre service on OST, MDS\n# [[inputs.lustre2]]\n#   ## An array of /proc globs to search for Lustre stats\n#   ## If not specified, the default will work on Lustre 2.5.x\n#   ##\n#   # ost_procfiles = [\n#   #   \"/proc/fs/lustre/obdfilter/*/stats\",\n#   #   \"/proc/fs/lustre/osd-ldiskfs/*/stats\",\n#   #   \"/proc/fs/lustre/obdfilter/*/job_stats\",\n#   # ]\n#   # mds_procfiles = [\n#   #   \"/proc/fs/lustre/mdt/*/md_stats\",\n#   #   \"/proc/fs/lustre/mdt/*/job_stats\",\n#   # ]\n\n\n# # Read metrics about LVM physical volumes, volume groups, logical volumes.\n# [[inputs.lvm]]\n# ## Use sudo to run LVM commands\n# use_sudo = false\n\n\n# # Gathers metrics from the /3.0/reports MailChimp API\n# [[inputs.mailchimp]]\n#   ## MailChimp API key\n#   ## get from https://admin.mailchimp.com/account/api/\n#   api_key = \"\" # required\n#   ## Reports for campaigns sent more than days_old ago will not be collected.\n#   ## 0 means collect all.\n#   days_old = 0\n#   ## Campaign ID to get, if empty gets all campaigns, this option overrides days_old\n#   # campaign_id = \"\"\n\n\n# # Retrieves information on a specific host in a MarkLogic Cluster\n# [[inputs.marklogic]]\n#   ## Base URL of the MarkLogic HTTP Server.\n#   url = \"http://localhost:8002\"\n#\n#   ## List of specific hostnames to retrieve information. At least (1) required.\n#   # hosts = [\"hostname1\", \"hostname2\"]\n#\n#   ## Using HTTP Basic Authentication. Management API requires 'manage-user' role privileges\n#   # username = \"myuser\"\n#   # password = \"mypassword\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from one or many mcrouter servers\n# [[inputs.mcrouter]]\n#   ## An array of address to gather stats about. Specify an ip or hostname\n#   ## with port. ie tcp://localhost:11211, tcp://10.0.0.1:11211, etc.\n# \tservers = [\"tcp://localhost:11211\", \"unix:///var/run/mcrouter.sock\"]\n#\n# \t## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n#   # timeout = \"5s\"\n\n\n# # Get md array statistics from /proc/mdstat\n# [[inputs.mdstat]]\n# \t## Sets file path\n# \t## If not specified, then default is /proc/mdstat\n# \t# file_name = \"/proc/mdstat\"\n\n\n# # Read metrics from one or many memcached servers\n# [[inputs.memcached]]\n#   ## An array of address to gather stats about. Specify an ip on hostname\n#   ## with optional port. ie localhost, 10.0.0.1:11211, etc.\n#   servers = [\"localhost:11211\"]\n#   # unix_sockets = [\"/var/run/memcached.sock\"]\n#\n#   ## Optional TLS Config\n#   # enable_tls = true\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Telegraf plugin for gathering metrics from N Mesos masters\n# [[inputs.mesos]]\n#   ## Timeout, in ms.\n#   timeout = 100\n#\n#   ## A list of Mesos masters.\n#   masters = [\"http://localhost:5050\"]\n#\n#   ## Master metrics groups to be collected, by default, all enabled.\n#   master_collections = [\n#     \"resources\",\n#     \"master\",\n#     \"system\",\n#     \"agents\",\n#     \"frameworks\",\n#     \"framework_offers\",\n#     \"tasks\",\n#     \"messages\",\n#     \"evqueue\",\n#     \"registrar\",\n#     \"allocator\",\n#   ]\n#\n#   ## A list of Mesos slaves, default is []\n#   # slaves = []\n#\n#   ## Slave metrics groups to be collected, by default, all enabled.\n#   # slave_collections = [\n#   #   \"resources\",\n#   #   \"agent\",\n#   #   \"system\",\n#   #   \"executors\",\n#   #   \"tasks\",\n#   #   \"messages\",\n#   # ]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Collects scores from a Minecraft server's scoreboard using the RCON protocol\n# [[inputs.minecraft]]\n#   ## Address of the Minecraft server.\n#   # server = \"localhost\"\n#\n#   ## Server RCON Port.\n#   # port = \"25575\"\n#\n#   ## Server RCON Password.\n#   password = \"\"\n#\n#   ## Uncomment to remove deprecated metric components.\n#   # tagdrop = [\"server\"]\n\n\n# # Generate metrics for test and demonstration purposes\n# [[inputs.mock]]\n#   ## Set the metric name to use for reporting\n#   metric_name = \"mock\"\n#\n#   ## Optional string key-value pairs of tags to add to all metrics\n#   # [inputs.mock.tags]\n#   # \"key\" = \"value\"\n#\n#   ## One or more mock data fields *must* be defined.\n#   ##\n#   ## [[inputs.mock.random]]\n#   ##   name = \"rand\"\n#   ##   min = 1.0\n#   ##   max = 6.0\n#   ## [[inputs.mock.sine_wave]]\n#   ##   name = \"wave\"\n#   ##   amplitude = 1.0\n#   ##   period = 0.5\n#   ## [[inputs.mock.step]]\n#   ##   name = \"plus_one\"\n#   ##   start = 0.0\n#   ##   step = 1.0\n#   ## [[inputs.mock.stock]]\n#   ##   name = \"abc\"\n#   ##   price = 50.00\n#   ##   volatility = 0.2\n\n\n# # Retrieve data from MODBUS slave devices\n# [[inputs.modbus]]\n#   ## Connection Configuration\n#   ##\n#   ## The plugin supports connections to PLCs via MODBUS/TCP, RTU over TCP, ASCII over TCP or\n#   ## via serial line communication in binary (RTU) or readable (ASCII) encoding\n#   ##\n#   ## Device name\n#   name = \"Device\"\n#\n#   ## Slave ID - addresses a MODBUS device on the bus\n#   ## Range: 0 - 255 [0 = broadcast; 248 - 255 = reserved]\n#   slave_id = 1\n#\n#   ## Timeout for each request\n#   timeout = \"1s\"\n#\n#   ## Maximum number of retries and the time to wait between retries\n#   ## when a slave-device is busy.\n#   # busy_retries = 0\n#   # busy_retries_wait = \"100ms\"\n#\n#   # TCP - connect via Modbus/TCP\n#   controller = \"tcp://localhost:502\"\n#\n#   ## Serial (RS485; RS232)\n#   # controller = \"file:///dev/ttyUSB0\"\n#   # baud_rate = 9600\n#   # data_bits = 8\n#   # parity = \"N\"\n#   # stop_bits = 1\n#\n#   ## Trace the connection to the modbus device as debug messages\n#   ## Note: You have to enable telegraf's debug mode to see those messages!\n#   # debug_connection = false\n#\n#   ## For Modbus over TCP you can choose between \"TCP\", \"RTUoverTCP\" and \"ASCIIoverTCP\"\n#   ## default behaviour is \"TCP\" if the controller is TCP\n#   ## For Serial you can choose between \"RTU\" and \"ASCII\"\n#   # transmission_mode = \"RTU\"\n#\n# \t## Define the configuration schema\n#   ##  |---register -- define fields per register type in the original style (only supports one slave ID)\n#   ##  |---request  -- define fields on a requests base\n#   configuration_type = \"register\"\n#\n#   ## Per register definition\n#   ##\n#\n#   ## Digital Variables, Discrete Inputs and Coils\n#   ## measurement - the (optional) measurement name, defaults to \"modbus\"\n#   ## name        - the variable name\n#   ## address     - variable address\n#\n#   discrete_inputs = [\n#     { name = \"start\",          address = [0]},\n#     { name = \"stop\",           address = [1]},\n#     { name = \"reset\",          address = [2]},\n#     { name = \"emergency_stop\", address = [3]},\n#   ]\n#   coils = [\n#     { name = \"motor1_run\",     address = [0]},\n#     { name = \"motor1_jog\",     address = [1]},\n#     { name = \"motor1_stop\",    address = [2]},\n#   ]\n#\n#   ## Analog Variables, Input Registers and Holding Registers\n#   ## measurement - the (optional) measurement name, defaults to \"modbus\"\n#   ## name        - the variable name\n#   ## byte_order  - the ordering of bytes\n#   ##  |---AB, ABCD   - Big Endian\n#   ##  |---BA, DCBA   - Little Endian\n#   ##  |---BADC       - Mid-Big Endian\n#   ##  |---CDAB       - Mid-Little Endian\n#   ## data_type  - INT16, UINT16, INT32, UINT32, INT64, UINT64,\n#   ##              FLOAT32-IEEE, FLOAT64-IEEE (the IEEE 754 binary representation)\n#   ##              FLOAT32, FIXED, UFIXED (fixed-point representation on input)\n#   ## scale      - the final numeric variable representation\n#   ## address    - variable address\n#\n#   holding_registers = [\n#     { name = \"power_factor\", byte_order = \"AB\",   data_type = \"FIXED\", scale=0.01,  address = [8]},\n#     { name = \"voltage\",      byte_order = \"AB\",   data_type = \"FIXED\", scale=0.1,   address = [0]},\n#     { name = \"energy\",       byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [5,6]},\n#     { name = \"current\",      byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [1,2]},\n#     { name = \"frequency\",    byte_order = \"AB\",   data_type = \"UFIXED\", scale=0.1,  address = [7]},\n#     { name = \"power\",        byte_order = \"ABCD\", data_type = \"UFIXED\", scale=0.1,  address = [3,4]},\n#   ]\n#   input_registers = [\n#     { name = \"tank_level\",   byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [0]},\n#     { name = \"tank_ph\",      byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [1]},\n#     { name = \"pump1_speed\",  byte_order = \"ABCD\", data_type = \"INT32\",   scale=1.0,     address = [3,4]},\n#   ]\n#\n#\n#   ## Per request definition\n#   ##\n#\n#   ## Define a request sent to the device\n#   ## Multiple of those requests can be defined. Data will be collated into metrics at the end of data collection.\n#   # [[inputs.modbus.request]]\n#     ## ID of the modbus slave device to query.\n#     ## If you need to query multiple slave-devices, create several \"request\" definitions.\n#     # slave_id = 0\n#\n#     ## Byte order of the data.\n#     ##  |---ABCD or MSW-BE -- Big Endian (Motorola)\n#     ##  |---DCBA or LSW-LE -- Little Endian (Intel)\n#     ##  |---BADC or MSW-LE -- Big Endian with byte swap\n#     ##  |---CDAB or LSW-BE -- Little Endian with byte swap\n#     # byte_order = \"ABCD\"\n#\n#     ## Type of the register for the request\n#     ## Can be \"coil\", \"discrete\", \"holding\" or \"input\"\n#     # register = \"holding\"\n#\n#     ## Name of the measurement.\n#     ## Can be overriden by the individual field definitions. Defaults to \"modbus\"\n#     # measurement = \"modbus\"\n#\n#     ## Field definitions\n#     ## Analog Variables, Input Registers and Holding Registers\n#     ## address        - address of the register to query. For coil and discrete inputs this is the bit address.\n#     ## name *1        - field name\n#     ## type *1,2      - type of the modbus field, can be INT16, UINT16, INT32, UINT32, INT64, UINT64 and\n#     ##                  FLOAT32, FLOAT64 (IEEE 754 binary representation)\n#     ## scale *1,2     - (optional) factor to scale the variable with\n#     ## output *1,2    - (optional) type of resulting field, can be INT64, UINT64 or FLOAT64. Defaults to FLOAT64 if\n#     ##                  \"scale\" is provided and to the input \"type\" class otherwise (i.e. INT* -\u003e INT64, etc).\n#     ## measurement *1 - (optional) measurement name, defaults to the setting of the request\n#     ## omit           - (optional) omit this field. Useful to leave out single values when querying many registers\n#     ##                  with a single request. Defaults to \"false\".\n#     ##\n#     ## *1: Those fields are ignored if field is omitted (\"omit\"=true)\n#     ##\n#     ## *2: Thise fields are ignored for both \"coil\" and \"discrete\"-input type of registers. For those register types\n#     ##     the fields are output as zero or one in UINT64 format by default.\n#\n#     ## Coil / discrete input example\n#     # fields = [\n#     #   { address=0, name=\"motor1_run\"},\n#     #   { address=1, name=\"jog\", measurement=\"motor\"},\n#     #   { address=2, name=\"motor1_stop\", omit=true},\n#     #   { address=3, name=\"motor1_overheating\"},\n#     # ]\n#\n#     ## Per-request tags\n#     ## These tags take precedence over predefined tags.\n#     # [[inputs.modbus.request.tags]]\n#     #\t  name = \"value\"\n#\n#     ## Holding / input example\n#     ## All of those examples will result in FLOAT64 field outputs\n#     # fields = [\n#     #   { address=0, name=\"voltage\",      type=\"INT16\",   scale=0.1   },\n#     #   { address=1, name=\"current\",      type=\"INT32\",   scale=0.001 },\n#     #   { address=3, name=\"power\",        type=\"UINT32\",  omit=true   },\n#     #   { address=5, name=\"energy\",       type=\"FLOAT32\", scale=0.001, measurement=\"W\" },\n#     #   { address=7, name=\"frequency\",    type=\"UINT32\",  scale=0.1   },\n#     #   { address=8, name=\"power_factor\", type=\"INT64\",   scale=0.01  },\n#     # ]\n#\n#     ## Holding / input example with type conversions\n#     # fields = [\n#     #   { address=0, name=\"rpm\",         type=\"INT16\"                   },  # will result in INT64 field\n#     #   { address=1, name=\"temperature\", type=\"INT16\", scale=0.1        },  # will result in FLOAT64 field\n#     #   { address=2, name=\"force\",       type=\"INT32\", output=\"FLOAT64\" },  # will result in FLOAT64 field\n#     #   { address=4, name=\"hours\",       type=\"UINT32\"                  },  # will result in UIN64 field\n#     # ]\n#\n#     ## Per-request tags\n# \t\t## These tags take precedence over predefined tags.\n#     # [[inputs.modbus.request.tags]]\n#     #\t  name = \"value\"\n#\n#\n#\n#   ## Enable workarounds required by some devices to work correctly\n#   # [inputs.modbus.workarounds]\n#     ## Pause between read requests sent to the device. This might be necessary for (slow) serial devices.\n#     # pause_between_requests = \"0ms\"\n#     ## Close the connection after every gather cycle. Usually the plugin closes the connection after a certain\n#     ## idle-timeout, however, if you query a device with limited simultaneous connectivity (e.g. serial devices)\n#     ## from multiple instances you might want to only stay connected during gather and disconnect afterwards.\n#     # close_connection_after_gather = false\n\n\n# # Read metrics from one or many MongoDB servers\n# [[inputs.mongodb]]\n#   ## An array of URLs of the form:\n#   ##   \"mongodb://\" [user \":\" pass \"@\"] host [ \":\" port]\n#   ## For example:\n#   ##   mongodb://user:auth_key@10.10.3.30:27017,\n#   ##   mongodb://10.10.3.33:18832,\n#   servers = [\"mongodb://127.0.0.1:27017?connect=direct\"]\n#\n#   ## When true, collect cluster status\n#   ## Note that the query that counts jumbo chunks triggers a COLLSCAN, which\n#   ## may have an impact on performance.\n#   # gather_cluster_status = true\n#\n#   ## When true, collect per database stats\n#   # gather_perdb_stats = false\n#\n#   ## When true, collect per collection stats\n#   # gather_col_stats = false\n#\n#   ## When true, collect usage statistics for each collection\n#   ## (insert, update, queries, remove, getmore, commands etc...).\n#   # gather_top_stat = false\n#\n#   ## List of db where collections stats are collected\n#   ## If empty, all db are concerned\n#   # col_stats_dbs = [\"local\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics and status information about processes managed by Monit\n# [[inputs.monit]]\n#   ## Monit HTTPD address\n#   address = \"http://127.0.0.1:2812\"\n#\n#   ## Username and Password for Monit\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Aggregates the contents of multiple files into a single point\n# [[inputs.multifile]]\n#   ## Base directory where telegraf will look for files.\n#   ## Omit this option to use absolute paths.\n#   base_dir = \"/sys/bus/i2c/devices/1-0076/iio:device0\"\n#\n#   ## If true, Telegraf discard all data when a single file can't be read.\n#   ## Else, Telegraf omits the field generated from this file.\n#   # fail_early = true\n#\n#   ## Files to parse each interval.\n#   [[inputs.multifile.file]]\n#     file = \"in_pressure_input\"\n#     dest = \"pressure\"\n#     conversion = \"float\"\n#   [[inputs.multifile.file]]\n#     file = \"in_temp_input\"\n#     dest = \"temperature\"\n#     conversion = \"float(3)\"\n#   [[inputs.multifile.file]]\n#     file = \"in_humidityrelative_input\"\n#     dest = \"humidityrelative\"\n#     conversion = \"float(3)\"\n\n\n# # Read metrics from one or many mysql servers\n# [[inputs.mysql]]\n#   ## specify servers via a url matching:\n#   ##  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify|custom]]\n#   ##  see https://github.com/go-sql-driver/mysql#dsn-data-source-name\n#   ##  e.g.\n#   ##    servers = [\"user:passwd@tcp(127.0.0.1:3306)/?tls=false\"]\n#   ##    servers = [\"user@tcp(127.0.0.1:3306)/?tls=false\"]\n#   #\n#   ## If no servers are specified, then localhost is used as the host.\n#   servers = [\"tcp(127.0.0.1:3306)/\"]\n#\n#   ## Selects the metric output format.\n#   ##\n#   ## This option exists to maintain backwards compatibility, if you have\n#   ## existing metrics do not set or change this value until you are ready to\n#   ## migrate to the new format.\n#   ##\n#   ## If you do not have existing metrics from this plugin set to the latest\n#   ## version.\n#   ##\n#   ## Telegraf \u003e=1.6: metric_version = 2\n#   ##           \u003c1.6: metric_version = 1 (or unset)\n#   metric_version = 2\n#\n#   ## if the list is empty, then metrics are gathered from all database tables\n#   # table_schema_databases = []\n#\n#   ## gather metrics from INFORMATION_SCHEMA.TABLES for databases provided above list\n#   # gather_table_schema = false\n#\n#   ## gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST\n#   # gather_process_list = false\n#\n#   ## gather user statistics from INFORMATION_SCHEMA.USER_STATISTICS\n#   # gather_user_statistics = false\n#\n#   ## gather auto_increment columns and max values from information schema\n#   # gather_info_schema_auto_inc = false\n#\n#   ## gather metrics from INFORMATION_SCHEMA.INNODB_METRICS\n#   # gather_innodb_metrics = false\n#\n#   ## gather metrics from SHOW SLAVE STATUS command output\n#   # gather_slave_status = false\n#\n#   ## gather metrics from all channels from SHOW SLAVE STATUS command output\n#   # gather_all_slave_channels = false\n#\n#   ## use MariaDB dialect for all channels SHOW SLAVE STATUS\n#   # mariadb_dialect = false\n#\n#   ## gather metrics from SHOW BINARY LOGS command output\n#   # gather_binary_logs = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.GLOBAL_VARIABLES\n#   # gather_global_variables = true\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE\n#   # gather_table_io_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS\n#   # gather_table_lock_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE\n#   # gather_index_io_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS\n#   # gather_event_waits = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME\n#   # gather_file_events_stats = false\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST\n#   # gather_perf_events_statements = false\n#\n#   ## the limits for metrics form perf_events_statements\n#   # perf_events_statements_digest_text_limit = 120\n#   # perf_events_statements_limit = 250\n#   # perf_events_statements_time_limit = 86400\n#\n#   ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_ACCOUNT_BY_EVENT_NAME\n#   # gather_perf_sum_per_acc_per_event         = false\n#\n#   ## list of events to be gathered for gather_perf_sum_per_acc_per_event\n#   ## in case of empty list all events will be gathered\n#   # perf_summary_events                       = []\n#\n#   ## Some queries we may want to run less often (such as SHOW GLOBAL VARIABLES)\n#   ##   example: interval_slow = \"30m\"\n#   # interval_slow = \"\"\n#\n#   ## Optional TLS Config (will be used if tls=custom parameter specified in server uri)\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Provides metrics about the state of a NATS server\n# [[inputs.nats]]\n#   ## The address of the monitoring endpoint of the NATS server\n#   server = \"http://localhost:8222\"\n#\n#   ## Maximum time to receive response\n#   # response_timeout = \"5s\"\n\n\n# # Neptune Apex data collector\n# [[inputs.neptune_apex]]\n#   ## The Neptune Apex plugin reads the publicly available status.xml data from a local Apex.\n#   ## Measurements will be logged under \"apex\".\n#\n#   ## The base URL of the local Apex(es). If you specify more than one server, they will\n#   ## be differentiated by the \"source\" tag.\n#   servers = [\n#     \"http://apex.local\",\n#   ]\n#\n#   ## The response_timeout specifies how long to wait for a reply from the Apex.\n#   #response_timeout = \"5s\"\n\n\n# # Read metrics about network interface usage\n# [[inputs.net]]\n#   ## By default, telegraf gathers stats from any up interface (excluding loopback)\n#   ## Setting interfaces will tell it to gather these explicit interfaces,\n#   ## regardless of status.\n#   ##\n#   # interfaces = [\"eth0\"]\n#   ##\n#   ## On linux systems telegraf also collects protocol stats.\n#   ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.\n#   ##\n#   # ignore_protocol_stats = false\n#   ##\n\n\n# # Collect response time of a TCP or UDP connection\n# [[inputs.net_response]]\n#   ## Protocol, must be \"tcp\" or \"udp\"\n#   ## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n#   ## a send/expect string pair (see below).\n#   protocol = \"tcp\"\n#   ## Server address (default localhost)\n#   address = \"localhost:80\"\n#\n#   ## Set timeout\n#   # timeout = \"1s\"\n#\n#   ## Set read timeout (only used if expecting a response)\n#   # read_timeout = \"1s\"\n#\n#   ## The following options are required for UDP checks. For TCP, they are\n#   ## optional. The plugin will send the given string to the server and then\n#   ## expect to receive the given 'expect' string back.\n#   ## string sent to the server\n#   # send = \"ssh\"\n#   ## expected string in answer\n#   # expect = \"ssh\"\n#\n#   ## Uncomment to remove deprecated fields\n#   # fielddrop = [\"result_type\", \"string_found\"]\n\n\n# # Read TCP metrics such as established, time wait and sockets counts.\n# [[inputs.netstat]]\n#   # no configuration\n\n\n# # Read per-mount NFS client metrics from /proc/self/mountstats\n# [[inputs.nfsclient]]\n#   ## Read more low-level metrics (optional, defaults to false)\n#   # fullstat = false\n#\n#   ## List of mounts to explictly include or exclude (optional)\n#   ## The pattern (Go regexp) is matched against the mount point (not the\n#   ## device being mounted).  If include_mounts is set, all mounts are ignored\n#   ## unless present in the list. If a mount is listed in both include_mounts\n#   ## and exclude_mounts, it is excluded.  Go regexp patterns can be used.\n#   # include_mounts = []\n#   # exclude_mounts = []\n#\n#   ## List of operations to include or exclude from collecting.  This applies\n#   ## only when fullstat=true.  Symantics are similar to {include,exclude}_mounts:\n#   ## the default is to collect everything; when include_operations is set, only\n#   ## those OPs are collected; when exclude_operations is set, all are collected\n#   ## except those listed.  If include and exclude are set, the OP is excluded.\n#   ## See /proc/self/mountstats for a list of valid operations; note that\n#   ## NFSv3 and NFSv4 have different lists.  While it is not possible to\n#   ## have different include/exclude lists for NFSv3/4, unused elements\n#   ## in the list should be okay.  It is possible to have different lists\n#   ## for different mountpoints:  use mulitple [[input.nfsclient]] stanzas,\n#   ## with their own lists.  See \"include_mounts\" above, and be careful of\n#   ## duplicate metrics.\n#   # include_operations = []\n#   # exclude_operations = []\n\n\n# # Read Nginx's basic status information (ngx_http_stub_status_module)\n# [[inputs.nginx]]\n#   # An array of Nginx stub_status URI to gather stats.\n#   urls = [\"http://localhost/server_status\"]\n#\n#   ## Optional TLS Config\n#   tls_ca = \"/etc/telegraf/ca.pem\"\n#   tls_cert = \"/etc/telegraf/cert.cer\"\n#   tls_key = \"/etc/telegraf/key.key\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   insecure_skip_verify = false\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n\n\n# # Read Nginx Plus' full status information (ngx_http_status_module)\n# [[inputs.nginx_plus]]\n#   ## An array of ngx_http_status_module or status URI to gather stats.\n#   urls = [\"http://localhost/status\"]\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read Nginx Plus Api documentation\n# [[inputs.nginx_plus_api]]\n#   ## An array of API URI to gather stats.\n#   urls = [\"http://localhost/api\"]\n#\n#   # Nginx API version, default: 3\n#   # api_version = 3\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read Nginx virtual host traffic status module information (nginx-module-sts)\n# [[inputs.nginx_sts]]\n#   ## An array of ngx_http_status_module or status URI to gather stats.\n#   urls = [\"http://localhost/status\"]\n#\n#   ## HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read nginx_upstream_check module status information (https://github.com/yaoweibin/nginx_upstream_check_module)\n# [[inputs.nginx_upstream_check]]\n#   ## An URL where Nginx Upstream check module is enabled\n#   ## It should be set to return a JSON formatted response\n#   url = \"http://127.0.0.1/status?format=json\"\n#\n#   ## HTTP method\n#   # method = \"GET\"\n#\n#   ## Optional HTTP headers\n#   # headers = {\"X-Special-Header\" = \"Special-Value\"}\n#\n#   ## Override HTTP \"Host\" header\n#   # host_header = \"check.example.com\"\n#\n#   ## Timeout for HTTP requests\n#   timeout = \"5s\"\n#\n#   ## Optional HTTP Basic Auth credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read Nginx virtual host traffic status module information (nginx-module-vts)\n# [[inputs.nginx_vts]]\n#   ## An array of ngx_http_status_module or status URI to gather stats.\n#   urls = [\"http://localhost/status\"]\n#\n#   ## HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from the Nomad API\n# [[inputs.nomad]]\n#   ## URL for the Nomad agent\n#   # url = \"http://127.0.0.1:4646\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n\n\n# # A plugin to collect stats from the NSD authoritative DNS name server\n# [[inputs.nsd]]\n#   ## Address of server to connect to, optionally ':port'. Defaults to the\n#   ## address in the nsd config file.\n#   server = \"127.0.0.1:8953\"\n#\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   # use_sudo = false\n#\n#   ## The default location of the nsd-control binary can be overridden with:\n#   # binary = \"/usr/sbin/nsd-control\"\n#\n#   ## The default location of the nsd config file can be overridden with:\n#   # config_file = \"/etc/nsd/nsd.conf\"\n#\n#   ## The default timeout of 1s can be overridden with:\n#   # timeout = \"1s\"\n\n\n# # Read NSQ topic and channel statistics.\n# [[inputs.nsq]]\n#   ## An array of NSQD HTTP API endpoints\n#   endpoints  = [\"http://localhost:4151\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Collect kernel snmp counters and network interface statistics\n# [[inputs.nstat]]\n#   ## file paths for proc files. If empty default paths will be used:\n#   ##    /proc/net/netstat, /proc/net/snmp, /proc/net/snmp6\n#   ## These can also be overridden with env variables, see README.\n#   proc_net_netstat = \"/proc/net/netstat\"\n#   proc_net_snmp = \"/proc/net/snmp\"\n#   proc_net_snmp6 = \"/proc/net/snmp6\"\n#   ## dump metrics with 0 values too\n#   dump_zeros       = true\n\n\n# # Get standard NTP query metrics, requires ntpq executable.\n# [[inputs.ntpq]]\n#   ## If false, set the -n ntpq flag. Can reduce metric gather time.\n#   dns_lookup = true\n\n\n# # Pulls statistics from nvidia GPUs attached to the host\n# [[inputs.nvidia_smi]]\n#   ## Optional: path to nvidia-smi binary, defaults \"/usr/bin/nvidia-smi\"\n#   ## We will first try to locate the nvidia-smi binary with the explicitly specified value (or default value),\n#   ## if it is not found, we will try to locate it on PATH(exec.LookPath), if it is still not found, an error will be returned\n#   # bin_path = \"/usr/bin/nvidia-smi\"\n#\n#   ## Optional: timeout for GPU polling\n#   # timeout = \"5s\"\n\n\n# # Retrieve data from OPCUA devices\n# [[inputs.opcua]]\n#   ## Metric name\n#   # name = \"opcua\"\n#   #\n#   ## OPC UA Endpoint URL\n#   # endpoint = \"opc.tcp://localhost:4840\"\n#   #\n#   ## Maximum time allowed to establish a connect to the endpoint.\n#   # connect_timeout = \"10s\"\n#   #\n#   ## Maximum time allowed for a request over the estabilished connection.\n#   # request_timeout = \"5s\"\n#   #\n#   ## Security policy, one of \"None\", \"Basic128Rsa15\", \"Basic256\",\n#   ## \"Basic256Sha256\", or \"auto\"\n#   # security_policy = \"auto\"\n#   #\n#   ## Security mode, one of \"None\", \"Sign\", \"SignAndEncrypt\", or \"auto\"\n#   # security_mode = \"auto\"\n#   #\n#   ## Path to cert.pem. Required when security mode or policy isn't \"None\".\n#   ## If cert path is not supplied, self-signed cert and key will be generated.\n#   # certificate = \"/etc/telegraf/cert.pem\"\n#   #\n#   ## Path to private key.pem. Required when security mode or policy isn't \"None\".\n#   ## If key path is not supplied, self-signed cert and key will be generated.\n#   # private_key = \"/etc/telegraf/key.pem\"\n#   #\n#   ## Authentication Method, one of \"Certificate\", \"UserName\", or \"Anonymous\".  To\n#   ## authenticate using a specific ID, select 'Certificate' or 'UserName'\n#   # auth_method = \"Anonymous\"\n#   #\n#   ## Username. Required for auth_method = \"UserName\"\n#   # username = \"\"\n#   #\n#   ## Password. Required for auth_method = \"UserName\"\n#   # password = \"\"\n#   #\n#   ## Option to select the metric timestamp to use. Valid options are:\n#   ##     \"gather\" -- uses the time of receiving the data in telegraf\n#   ##     \"server\" -- uses the timestamp provided by the server\n#   ##     \"source\" -- uses the timestamp provided by the source\n#   # timestamp = \"gather\"\n#   #\n#   ## Node ID configuration\n#   ## name              - field name to use in the output\n#   ## namespace         - OPC UA namespace of the node (integer value 0 thru 3)\n#   ## identifier_type   - OPC UA ID type (s=string, i=numeric, g=guid, b=opaque)\n#   ## identifier        - OPC UA ID (tag as shown in opcua browser)\n#   ## Example:\n#   ## {name=\"ProductUri\", namespace=\"0\", identifier_type=\"i\", identifier=\"2262\"}\n#   # nodes = [\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #]\n#   #\n#   ## Node Group\n#   ## Sets defaults for OPC UA namespace and ID type so they aren't required in\n#   ## every node.  A group can also have a metric name that overrides the main\n#   ## plugin metric name.\n#   ##\n#   ## Multiple node groups are allowed\n#   #[[inputs.opcua.group]]\n#   ## Group Metric name. Overrides the top level name.  If unset, the\n#   ## top level name is used.\n#   # name =\n#   #\n#   ## Group default namespace. If a node in the group doesn't set its\n#   ## namespace, this is used.\n#   # namespace =\n#   #\n#   ## Group default identifier type. If a node in the group doesn't set its\n#   ## namespace, this is used.\n#   # identifier_type =\n#   #\n#   ## Node ID Configuration.  Array of nodes with the same settings as above.\n#   # nodes = [\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n#   #]\n#\n#   ## Enable workarounds required by some devices to work correctly\n#   # [inputs.opcua.workarounds]\n#     ## Set additional valid status codes, StatusOK (0x0) is always considered valid\n#     # additional_valid_status_codes = [\"0xC0\"]\n\n\n# # OpenLDAP cn=Monitor plugin\n# [[inputs.openldap]]\n#   host = \"localhost\"\n#   port = 389\n#\n#   # ldaps, starttls, or no encryption. default is an empty string, disabling all encryption.\n#   # note that port will likely need to be changed to 636 for ldaps\n#   # valid options: \"\" | \"starttls\" | \"ldaps\"\n#   tls = \"\"\n#\n#   # skip peer certificate verification. Default is false.\n#   insecure_skip_verify = false\n#\n#   # Path to PEM-encoded Root certificate to use to verify server certificate\n#   tls_ca = \"/etc/ssl/certs.pem\"\n#\n#   # dn/password to bind with. If bind_dn is empty, an anonymous bind is performed.\n#   bind_dn = \"\"\n#   bind_password = \"\"\n#\n#   # Reverse metric names so they sort more naturally. Recommended.\n#   # This defaults to false if unset, but is set to true when generating a new config\n#   reverse_metric_names = true\n\n\n# # Get standard NTP query metrics from OpenNTPD.\n# [[inputs.openntpd]]\n#   ## Run ntpctl binary with sudo.\n#   # use_sudo = false\n#\n#   ## Location of the ntpctl binary.\n#   # binary = \"/usr/sbin/ntpctl\"\n#\n#   ## Maximum time the ntpctl binary is allowed to run.\n#   # timeout = \"5ms\"\n\n\n# # A plugin to collect stats from Opensmtpd - a validating, recursive, and caching DNS resolver \n# [[inputs.opensmtpd]]\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   #use_sudo = false\n#\n#   ## The default location of the smtpctl binary can be overridden with:\n#   binary = \"/usr/sbin/smtpctl\"\n#\n#   ## The default timeout of 1000ms can be overridden with (in milliseconds):\n#   timeout = 1000\n\n\n# # Collects performance metrics from OpenStack services\n# [[inputs.openstack]]\n#   ## The recommended interval to poll is '30m'\n#\n#   ## The identity endpoint to authenticate against and get the service catalog from.\n#   authentication_endpoint = \"https://my.openstack.cloud:5000\"\n#\n#   ## The domain to authenticate against when using a V3 identity endpoint.\n#   # domain = \"default\"\n#\n#   ## The project to authenticate as.\n#   # project = \"admin\"\n#\n#   ## User authentication credentials. Must have admin rights.\n#   username = \"admin\"\n#   password = \"password\"\n#\n#   ## Available services are:\n#   ## \"agents\", \"aggregates\", \"flavors\", \"hypervisors\", \"networks\", \"nova_services\",\n#   ## \"ports\", \"projects\", \"servers\", \"services\", \"stacks\", \"storage_pools\", \"subnets\", \"volumes\"\n#   # enabled_services = [\"services\", \"projects\", \"hypervisors\", \"flavors\", \"networks\", \"volumes\"]\n#\n#   ## Collect Server Diagnostics\n#   # server_diagnotics = false\n#\n#   ## output secrets (such as adminPass(for server) and UserID(for volume)).\n#   # output_secrets = false\n#\n#   ## Amount of time allowed to complete the HTTP(s) request.\n#   # timeout = \"5s\"\n#\n#   ## HTTP Proxy support\n#   # http_proxy_url = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Options for tags received from Openstack\n#   # tag_prefix = \"openstack_tag_\"\n#   # tag_value = \"true\"\n#\n#   ## Timestamp format for timestamp data recieved from Openstack.\n#   ## If false format is unix nanoseconds.\n#   # human_readable_timestamps = false\n#\n#   ## Measure Openstack call duration\n#   # measure_openstack_requests = false\n\n\n# # Read current weather and forecasts data from openweathermap.org\n# [[inputs.openweathermap]]\n#   ## OpenWeatherMap API key.\n#   app_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n#\n#   ## City ID's to collect weather data from.\n#   city_id = [\"5391959\"]\n#\n#   ## Language of the description field. Can be one of \"ar\", \"bg\",\n#   ## \"ca\", \"cz\", \"de\", \"el\", \"en\", \"fa\", \"fi\", \"fr\", \"gl\", \"hr\", \"hu\",\n#   ## \"it\", \"ja\", \"kr\", \"la\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\",\n#   ## \"se\", \"sk\", \"sl\", \"es\", \"tr\", \"ua\", \"vi\", \"zh_cn\", \"zh_tw\"\n#   # lang = \"en\"\n#\n#   ## APIs to fetch; can contain \"weather\" or \"forecast\".\n#   fetch = [\"weather\", \"forecast\"]\n#\n#   ## OpenWeatherMap base URL\n#   # base_url = \"https://api.openweathermap.org/\"\n#\n#   ## Timeout for HTTP response.\n#   # response_timeout = \"5s\"\n#\n#   ## Preferred unit system for temperature and wind speed. Can be one of\n#   ## \"metric\", \"imperial\", or \"standard\".\n#   # units = \"metric\"\n#\n#   ## Query interval; OpenWeatherMap updates their weather data every 10\n#   ## minutes.\n#   interval = \"10m\"\n\n\n# # Read metrics of passenger using passenger-status\n# [[inputs.passenger]]\n#   ## Path of passenger-status.\n#   ##\n#   ## Plugin gather metric via parsing XML output of passenger-status\n#   ## More information about the tool:\n#   ##   https://www.phusionpassenger.com/library/admin/apache/overall_status_report.html\n#   ##\n#   ## If no path is specified, then the plugin simply execute passenger-status\n#   ## hopefully it can be found in your PATH\n#   command = \"passenger-status -v --show=xml\"\n\n\n# # Gather counters from PF\n# [[inputs.pf]]\n#   ## PF require root access on most systems.\n#   ## Setting 'use_sudo' to true will make use of sudo to run pfctl.\n#   ## Users must configure sudo to allow telegraf user to run pfctl with no password.\n#   ## pfctl can be restricted to only list command \"pfctl -s info\".\n#   use_sudo = false\n\n\n# # Read metrics of phpfpm, via HTTP status page or socket\n# [[inputs.phpfpm]]\n#   ## An array of addresses to gather stats about. Specify an ip or hostname\n#   ## with optional port and path\n#   ##\n#   ## Plugin can be configured in three modes (either can be used):\n#   ##   - http: the URL must start with http:// or https://, ie:\n#   ##       \"http://localhost/status\"\n#   ##       \"http://192.168.130.1/status?full\"\n#   ##\n#   ##   - unixsocket: path to fpm socket, ie:\n#   ##       \"/var/run/php5-fpm.sock\"\n#   ##      or using a custom fpm status path:\n#   ##       \"/var/run/php5-fpm.sock:fpm-custom-status-path\"\n#   ##\n#   ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n#   ##       \"fcgi://10.0.0.12:9000/status\"\n#   ##       \"cgi://10.0.10.12:9001/status\"\n#   ##\n#   ## Example of multiple gathering from local socket and remote host\n#   ## urls = [\"http://192.168.1.20/status\", \"/tmp/fpm.sock\"]\n#   urls = [\"http://localhost/status\"]\n#\n#   ## Duration allowed to complete HTTP requests.\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Ping given url(s) and return statistics\n# [[inputs.ping]]\n#   ## Hosts to send ping packets to.\n#   urls = [\"example.org\"]\n#\n#   ## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n#   ## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n#   ## the plugin will send pings directly.\n#   ##\n#   ## While the default is \"exec\" for backwards compatibility, new deployments\n#   ## are encouraged to use the \"native\" method for improved compatibility and\n#   ## performance.\n#   # method = \"exec\"\n#\n#   ## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n#   ## option of the ping command.\n#   # count = 1\n#\n#   ## Time to wait between sending ping packets in seconds.  Operates like the\n#   ## \"-i\" option of the ping command.\n#   # ping_interval = 1.0\n#\n#   ## If set, the time to wait for a ping response in seconds.  Operates like\n#   ## the \"-W\" option of the ping command.\n#   # timeout = 1.0\n#\n#   ## If set, the total ping deadline, in seconds.  Operates like the -w option\n#   ## of the ping command.\n#   # deadline = 10\n#\n#   ## Interface or source address to send ping from.  Operates like the -I or -S\n#   ## option of the ping command.\n#   # interface = \"\"\n#\n#   ## Percentiles to calculate. This only works with the native method.\n#   # percentiles = [50, 95, 99]\n#\n#   ## Specify the ping executable binary.\n#   # binary = \"ping\"\n#\n#   ## Arguments for ping command. When arguments is not empty, the command from\n#   ## the binary option will be used and other options (ping_interval, timeout,\n#   ## etc) will be ignored.\n#   # arguments = [\"-c\", \"3\"]\n#\n#   ## Use only IPv6 addresses when resolving a hostname.\n#   # ipv6 = false\n#\n#   ## Number of data bytes to be sent. Corresponds to the \"-s\"\n#   ## option of the ping command. This only works with the native method.\n#   # size = 56\n\n\n# # Measure postfix queue statistics\n# [[inputs.postfix]]\n#   ## Postfix queue directory. If not provided, telegraf will try to use\n#   ## 'postconf -h queue_directory' to determine it.\n#   # queue_directory = \"/var/spool/postfix\"\n\n\n# # Read metrics from one or many PowerDNS servers\n# [[inputs.powerdns]]\n#   ## An array of sockets to gather stats about.\n#   ## Specify a path to unix socket.\n#   unix_sockets = [\"/var/run/pdns.controlsocket\"]\n\n\n# # Read metrics from one or many PowerDNS Recursor servers\n# [[inputs.powerdns_recursor]]\n#   ## Path to the Recursor control socket.\n#   unix_sockets = [\"/var/run/pdns_recursor.controlsocket\"]\n#\n#   ## Directory to create receive socket.  This default is likely not writable,\n#   ## please reference the full plugin documentation for a recommended setup.\n#   # socket_dir = \"/var/run/\"\n#   ## Socket permissions for the receive socket.\n#   # socket_mode = \"0666\"\n\n\n# # Monitor process cpu and memory usage\n# [[inputs.procstat]]\n#   ## PID file to monitor process\n#   pid_file = \"/var/run/nginx.pid\"\n#   ## executable name (ie, pgrep \u003cexe\u003e)\n#   # exe = \"nginx\"\n#   ## pattern as argument for pgrep (ie, pgrep -f \u003cpattern\u003e)\n#   # pattern = \"nginx\"\n#   ## user as argument for pgrep (ie, pgrep -u \u003cuser\u003e)\n#   # user = \"nginx\"\n#   ## Systemd unit name, supports globs when include_systemd_children is set to true\n#   # systemd_unit = \"nginx.service\"\n#   # include_systemd_children = false\n#   ## CGroup name or path, supports globs\n#   # cgroup = \"systemd/system.slice/nginx.service\"\n#\n#   ## Windows service name\n#   # win_service = \"\"\n#\n#   ## override for process_name\n#   ## This is optional; default is sourced from /proc/\u003cpid\u003e/status\n#   # process_name = \"bar\"\n#\n#   ## Field name prefix\n#   # prefix = \"\"\n#\n#   ## When true add the full cmdline as a tag.\n#   # cmdline_tag = false\n#\n#   ## Mode to use when calculating CPU usage. Can be one of 'solaris' or 'irix'.\n#   # mode = \"irix\"\n#\n#   ## Add the PID as a tag instead of as a field.  When collecting multiple\n#   ## processes with otherwise matching tags this setting should be enabled to\n#   ## ensure each process has a unique identity.\n#   ##\n#   ## Enabling this option may result in a large number of series, especially\n#   ## when processes have a short lifetime.\n#   # pid_tag = false\n#\n#   ## Method to use when finding process IDs.  Can be one of 'pgrep', or\n#   ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while\n#   ## the native finder performs the search directly in a manor dependent on the\n#   ## platform.  Default is 'pgrep'\n#   # pid_finder = \"pgrep\"\n\n\n# # Provides metrics from Proxmox nodes (Proxmox Virtual Environment \u003e 6.2).\n# [[inputs.proxmox]]\n#   ## API connection configuration. The API token was introduced in Proxmox v6.2. Required permissions for user and token: PVEAuditor role on /.\n#   base_url = \"https://localhost:8006/api2/json\"\n#   api_token = \"USER@REALM!TOKENID=UUID\"\n#   ## Node name, defaults to OS hostname\n#   # node_name = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   insecure_skip_verify = false\n#\n#   # HTTP response timeout (default: 5s)\n#   response_timeout = \"5s\"\n\n\n# # Reads last_run_summary.yaml file and converts to measurements\n# [[inputs.puppetagent]]\n#   ## Location of puppet last run summary file\n#   location = \"/var/lib/puppet/state/last_run_summary.yaml\"\n\n\n# # Reads metrics from RabbitMQ servers via the Management Plugin\n# [[inputs.rabbitmq]]\n#   ## Management Plugin url. (default: http://localhost:15672)\n#   # url = \"http://localhost:15672\"\n#   ## Credentials\n#   # username = \"guest\"\n#   # password = \"guest\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Optional request timeouts\n#   ##\n#   ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n#   ## for a server's response headers after fully writing the request.\n#   # header_timeout = \"3s\"\n#   ##\n#   ## client_timeout specifies a time limit for requests made by this client.\n#   ## Includes connection time, any redirects, and reading the response body.\n#   # client_timeout = \"4s\"\n#\n#   ## A list of nodes to gather as the rabbitmq_node measurement. If not\n#   ## specified, metrics for all nodes are gathered.\n#   # nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n#\n#   ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n#   ## specified, metrics for all exchanges are gathered.\n#   # exchanges = [\"telegraf\"]\n#\n#   ## Metrics to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all metrics\n#   ## Currently the following metrics are supported: \"exchange\", \"federation\", \"node\", \"overview\", \"queue\"\n#   # metric_include = []\n#   # metric_exclude = []\n#\n#   ## Queues to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all queues\n#   queue_name_include = []\n#   queue_name_exclude = []\n#\n#   ## Federation upstreams include and exclude when gathering the rabbitmq_federation measurement.\n#   ## If neither are specified, metrics for all federation upstreams are gathered.\n#   ## Federation link metrics will only be gathered for queues and exchanges\n#   ## whose non-federation metrics will be collected (e.g a queue excluded\n#   ## by the 'queue_name_exclude' option will also be excluded from federation).\n#   ## Globs accepted.\n#   # federation_upstream_include = [\"dataCentre-*\"]\n#   # federation_upstream_exclude = []\n\n\n# # Read raindrops stats (raindrops - real-time stats for preforking Rack servers)\n# [[inputs.raindrops]]\n#   ## An array of raindrops middleware URI to gather stats.\n#   urls = [\"http://localhost:8080/_raindrops\"]\n\n\n# # Reads metrics from RavenDB servers via the Monitoring Endpoints\n# [[inputs.ravendb]]\n#   ## Node URL and port that RavenDB is listening on. By default,\n#   ## attempts to connect securely over HTTPS, however, if the user\n#   ## is running a local unsecure development cluster users can use\n#   ## HTTP via a URL like \"http://localhost:8080\"\n#   url = \"https://localhost:4433\"\n#\n#   ## RavenDB X509 client certificate setup\n#   # tls_cert = \"/etc/telegraf/raven.crt\"\n#   # tls_key = \"/etc/telegraf/raven.key\"\n#\n#   ## Optional request timeout\n#   ##\n#   ## Timeout, specifies the amount of time to wait\n#   ## for a server's response headers after fully writing the request and\n#   ## time limit for requests made by this client\n#   # timeout = \"5s\"\n#\n#   ## List of statistics which are collected\n#   # At least one is required\n#   # Allowed values: server, databases, indexes, collections\n#   #\n#   # stats_include = [\"server\", \"databases\", \"indexes\", \"collections\"]\n#\n#   ## List of db where database stats are collected\n#   ## If empty, all db are concerned\n#   # db_stats_dbs = []\n#\n#   ## List of db where index status are collected\n#   ## If empty, all indexes from all db are concerned\n#   # index_stats_dbs = []\n#\n#   ## List of db where collection status are collected\n#   ## If empty, all collections from all db are concerned\n#   # collection_stats_dbs = []\n\n\n# # Read CPU, Fans, Powersupply and Voltage metrics of hardware server through redfish APIs\n# [[inputs.redfish]]\n#   ## Server url\n#   address = \"https://127.0.0.1:5000\"\n#\n#   ## Username, Password for hardware server\n#   username = \"root\"\n#   password = \"password123456\"\n#\n#   ## ComputerSystemId\n#   computer_system_id=\"2M220100SL\"\n#\n#   ## Amount of time allowed to complete the HTTP request\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from one or many redis servers\n# [[inputs.redis]]\n#   ## specify servers via a url matching:\n#   ##  [protocol://][:password]@address[:port]\n#   ##  e.g.\n#   ##    tcp://localhost:6379\n#   ##    tcp://:password@192.168.99.100\n#   ##    unix:///var/run/redis.sock\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no port is specified, 6379 is used\n#   servers = [\"tcp://localhost:6379\"]\n#\n#   ## Optional. Specify redis commands to retrieve values\n#   # [[inputs.redis.commands]]\n#   #   # The command to run where each argument is a separate element\n#   #   command = [\"get\", \"sample-key\"]\n#   #   # The field to store the result in\n#   #   field = \"sample-key-value\"\n#   #   # The type of the result\n#   #   # Can be \"string\", \"integer\", or \"float\"\n#   #   type = \"string\"\n#\n#   ## specify server password\n#   # password = \"s#cr@t%\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Read metrics from one or many redis-sentinel servers\n# [[inputs.redis_sentinel]]\n#   ## specify servers via a url matching:\n#   ##  [protocol://][:password]@address[:port]\n#   ##  e.g.\n#   ##    tcp://localhost:26379\n#   ##    tcp://:password@192.168.99.100\n#   ##    unix:///var/run/redis-sentinel.sock\n#   ##\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no port is specified, 26379 is used\n#   # servers = [\"tcp://localhost:26379\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n# # Read metrics from one or many RethinkDB servers\n# [[inputs.rethinkdb]]\n#   ## An array of URI to gather stats about. Specify an ip or hostname\n#   ## with optional port add password. ie,\n#   ##   rethinkdb://user:auth_key@10.10.3.30:28105,\n#   ##   rethinkdb://10.10.3.33:18832,\n#   ##   10.0.0.1:10000, etc.\n#   servers = [\"127.0.0.1:28015\"]\n#   ##\n#   ## If you use actual rethinkdb of \u003e 2.3.0 with username/password authorization,\n#   ## protocol have to be named \"rethinkdb2\" - it will use 1_0 H.\n#   # servers = [\"rethinkdb2://username:password@127.0.0.1:28015\"]\n#   ##\n#   ## If you use older versions of rethinkdb (\u003c2.2) with auth_key, protocol\n#   ## have to be named \"rethinkdb\".\n#   # servers = [\"rethinkdb://username:auth_key@127.0.0.1:28015\"]\n\n\n# # Read metrics one or many Riak servers\n# [[inputs.riak]]\n#   # Specify a list of one or more riak http servers\n#   servers = [\"http://localhost:8098\"]\n\n\n# # Read API usage and limits for a Salesforce organisation\n# [[inputs.salesforce]]\n#   ## specify your credentials\n#   ##\n#   username = \"your_username\"\n#   password = \"your_password\"\n#   ##\n#   ## (optional) security token\n#   # security_token = \"your_security_token\"\n#   ##\n#   ## (optional) environment type (sandbox or production)\n#   ## default is: production\n#   ##\n#   # environment = \"production\"\n#   ##\n#   ## (optional) API version (default: \"39.0\")\n#   ##\n#   # version = \"39.0\"\n\n\n# # Monitor sensors, requires lm-sensors package\n# [[inputs.sensors]]\n#   ## Remove numbers from field names.\n#   ## If true, a field name like 'temp1_input' will be changed to 'temp_input'.\n#   # remove_numbers = true\n#\n#   ## Timeout is the maximum amount of time that the sensors command can run.\n#   # timeout = \"5s\"\n\n\n# # Read metrics from storage devices supporting S.M.A.R.T.\n# [[inputs.smart]]\n#   ## Optionally specify the path to the smartctl executable\n#   # path_smartctl = \"/usr/bin/smartctl\"\n#\n#   ## Optionally specify the path to the nvme-cli executable\n#   # path_nvme = \"/usr/bin/nvme\"\n#\n#   ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n#   ## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n#   ## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n#   # enable_extensions = [\"auto-on\"]\n#\n#   ## On most platforms used cli utilities requires root access.\n#   ## Setting 'use_sudo' to true will make use of sudo to run smartctl or nvme-cli.\n#   ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli\n#   ## without a password.\n#   # use_sudo = false\n#\n#   ## Skip checking disks in this power mode. Defaults to\n#   ## \"standby\" to not wake up disks that have stopped rotating.\n#   ## See --nocheck in the man pages for smartctl.\n#   ## smartctl version 5.41 and 5.42 have faulty detection of\n#   ## power mode and might require changing this value to\n#   ## \"never\" depending on your disks.\n#   # nocheck = \"standby\"\n#\n#   ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n#   ## information from each drive into the 'smart_attribute' measurement.\n#   # attributes = false\n#\n#   ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n#   # excludes = [ \"/dev/pass6\" ]\n#\n#   ## Optionally specify devices and device type, if unset\n#   ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n#   ## and all found will be included except for the excluded in excludes.\n#   # devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n#\n#   ## Timeout for the cli command to complete.\n#   # timeout = \"30s\"\n#\n#   ## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n#   ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n#   ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n#   ## SMART data - one individual array drive at the time. In such case please set this configuration option\n#   ## to \"sequential\" to get readings for all drives.\n#   ## valid options: concurrent, sequential\n#   # read_method = \"concurrent\"\n\n\n# # Retrieves SNMP values from remote agents\n# [[inputs.snmp]]\n#   ## Agent addresses to retrieve values from.\n#   ##   format:  agents = [\"\u003cscheme://\u003e\u003chostname\u003e:\u003cport\u003e\"]\n#   ##   scheme:  optional, either udp, udp4, udp6, tcp, tcp4, tcp6.\n#   ##            default is udp\n#   ##   port:    optional\n#   ##   example: agents = [\"udp://127.0.0.1:161\"]\n#   ##            agents = [\"tcp://127.0.0.1:161\"]\n#   ##            agents = [\"udp4://v4only-snmp-agent\"]\n#   agents = [\"udp://127.0.0.1:161\"]\n#\n#   ## Timeout for each request.\n#   # timeout = \"5s\"\n#\n#   ## SNMP version; can be 1, 2, or 3.\n#   # version = 2\n#\n#   ## Path to mib files\n#   # path = [\"/usr/share/snmp/mibs\"]\n#\n#   ## Agent host tag; the tag used to reference the source host\n#   # agent_host_tag = \"agent_host\"\n#\n#   ## SNMP community string.\n#   # community = \"public\"\n#\n#   ## Number of retries to attempt.\n#   # retries = 3\n#\n#   ## The GETBULK max-repetitions parameter.\n#   # max_repetitions = 10\n#\n#   ## SNMPv3 authentication and encryption options.\n#   ##\n#   ## Security Name.\n#   # sec_name = \"myuser\"\n#   ## Authentication protocol; one of \"MD5\", \"SHA\", \"SHA224\", \"SHA256\", \"SHA384\", \"SHA512\" or \"\".\n#   # auth_protocol = \"MD5\"\n#   ## Authentication password.\n#   # auth_password = \"pass\"\n#   ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n#   # sec_level = \"authNoPriv\"\n#   ## Context Name.\n#   # context_name = \"\"\n#   ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\" or \"\".\n#   # priv_protocol = \"\"\n#   ## Privacy password used for encrypted messages.\n#   # priv_password = \"\"\n#\n#   ## Add fields and tables defining the variables you wish to collect.  This\n#   ## example collects the system uptime and interface variables.  Reference the\n#   ## full plugin documentation for configuration details.\n\n\n# # DEPRECATED! PLEASE USE inputs.snmp INSTEAD.\n# [[inputs.snmp_legacy]]\n#   ## DEPRECATED: The 'snmp_legacy' plugin is deprecated in version 1.0.0, use 'inputs.snmp' instead.\n#   ## Use 'oids.txt' file to translate oids to names\n#   ## To generate 'oids.txt' you need to run:\n#   ##   snmptranslate -m all -Tz -On | sed -e 's/\"//g' \u003e /tmp/oids.txt\n#   ## Or if you have an other MIB folder with custom MIBs\n#   ##   snmptranslate -M /mycustommibfolder -Tz -On -m all | sed -e 's/\"//g' \u003e oids.txt\n#   snmptranslate_file = \"/tmp/oids.txt\"\n#   [[inputs.snmp.host]]\n#     address = \"192.168.2.2:161\"\n#     # SNMP community\n#     community = \"public\" # default public\n#     # SNMP version (1, 2 or 3)\n#     # Version 3 not supported yet\n#     version = 2 # default 2\n#     # SNMP response timeout\n#     timeout = 2.0 # default 2.0\n#     # SNMP request retries\n#     retries = 2 # default 2\n#     # Which get/bulk do you want to collect for this host\n#     collect = [\"mybulk\", \"sysservices\", \"sysdescr\"]\n#     # Simple list of OIDs to get, in addition to \"collect\"\n#     get_oids = []\n#\n#   [[inputs.snmp.host]]\n#     address = \"192.168.2.3:161\"\n#     community = \"public\"\n#     version = 2\n#     timeout = 2.0\n#     retries = 2\n#     collect = [\"mybulk\"]\n#     get_oids = [\n#         \"ifNumber\",\n#         \".1.3.6.1.2.1.1.3.0\",\n#     ]\n#\n#   [[inputs.snmp.get]]\n#     name = \"ifnumber\"\n#     oid = \"ifNumber\"\n#\n#   [[inputs.snmp.get]]\n#     name = \"interface_speed\"\n#     oid = \"ifSpeed\"\n#     instance = \"0\"\n#\n#   [[inputs.snmp.get]]\n#     name = \"sysuptime\"\n#     oid = \".1.3.6.1.2.1.1.3.0\"\n#     unit = \"second\"\n#\n#   [[inputs.snmp.bulk]]\n#     name = \"mybulk\"\n#     max_repetition = 127\n#     oid = \".1.3.6.1.2.1.1\"\n#\n#   [[inputs.snmp.bulk]]\n#     name = \"ifoutoctets\"\n#     max_repetition = 127\n#     oid = \"ifOutOctets\"\n#\n#   [[inputs.snmp.host]]\n#     address = \"192.168.2.13:161\"\n#     #address = \"127.0.0.1:161\"\n#     community = \"public\"\n#     version = 2\n#     timeout = 2.0\n#     retries = 2\n#     #collect = [\"mybulk\", \"sysservices\", \"sysdescr\", \"systype\"]\n#     collect = [\"sysuptime\" ]\n#     [[inputs.snmp.host.table]]\n#       name = \"iftable3\"\n#       include_instances = [\"enp5s0\", \"eth1\"]\n#\n#   # SNMP TABLEs\n#   # table without mapping neither subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable1\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#\n#   # table without mapping but with subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable2\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#     sub_tables = [\".1.3.6.1.2.1.2.2.1.13\"]\n#\n#   # table with mapping but without subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable3\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#     # if empty. get all instances\n#     mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n#     # if empty, get all subtables\n#\n#   # table with both mapping and subtables\n#   [[inputs.snmp.table]]\n#     name = \"iftable4\"\n#     oid = \".1.3.6.1.2.1.31.1.1.1\"\n#     # if empty get all instances\n#     mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n#     # if empty get all subtables\n#     # sub_tables could be not \"real subtables\"\n#     sub_tables=[\".1.3.6.1.2.1.2.2.1.13\", \"bytes_recv\", \"bytes_send\"]\n\n\n# # Gather indicators from established connections, using iproute2's `ss` command.\n# [[inputs.socketstat]]\n#   ## ss can display information about tcp, udp, raw, unix, packet, dccp and sctp sockets\n#   ## List of protocol types to collect\n#   # protocols = [ \"tcp\", \"udp\" ]\n#   ## The default timeout of 1s for ss execution can be overridden here:\n#   # timeout = \"1s\"\n\n\n# # Read stats from one or more Solr servers or cores\n# [[inputs.solr]]\n#   ## specify a list of one or more Solr servers\n#   servers = [\"http://localhost:8983\"]\n#\n#   ## specify a list of one or more Solr cores (default - all)\n#   # cores = [\"main\"]\n#\n#   ## Optional HTTP Basic Auth Credentials\n#   # username = \"username\"\n#   # password = \"pa$$word\"\n\n\n# # Gather timeseries from Google Cloud Platform v3 monitoring API\n# [[inputs.stackdriver]]\n#   ## GCP Project\n#   project = \"erudite-bloom-151019\"\n#\n#   ## Include timeseries that start with the given metric type.\n#   metric_type_prefix_include = [\n#     \"compute.googleapis.com/\",\n#   ]\n#\n#   ## Exclude timeseries that start with the given metric type.\n#   # metric_type_prefix_exclude = []\n#\n#   ## Many metrics are updated once per minute; it is recommended to override\n#   ## the agent level interval with a value of 1m or greater.\n#   interval = \"1m\"\n#\n#   ## Maximum number of API calls to make per second.  The quota for accounts\n#   ## varies, it can be viewed on the API dashboard:\n#   ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n#   # rate_limit = 14\n#\n#   ## The delay and window options control the number of points selected on\n#   ## each gather.  When set, metrics are gathered between:\n#   ##   start: now() - delay - window\n#   ##   end:   now() - delay\n#   #\n#   ## Collection delay; if set too low metrics may not yet be available.\n#   # delay = \"5m\"\n#   #\n#   ## If unset, the window will start at 1m and be updated dynamically to span\n#   ## the time between calls (approximately the length of the plugin interval).\n#   # window = \"1m\"\n#\n#   ## TTL for cached list of metric types.  This is the maximum amount of time\n#   ## it may take to discover new metrics.\n#   # cache_ttl = \"1h\"\n#\n#   ## If true, raw bucket counts are collected for distribution value types.\n#   ## For a more lightweight collection, you may wish to disable and use\n#   ## distribution_aggregation_aligners instead.\n#   # gather_raw_distribution_buckets = true\n#\n#   ## Aggregate functions to be used for metrics whose value type is\n#   ## distribution.  These aggregate values are recorded in in addition to raw\n#   ## bucket counts; if they are enabled.\n#   ##\n#   ## For a list of aligner strings see:\n#   ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n#   # distribution_aggregation_aligners = [\n#   # \t\"ALIGN_PERCENTILE_99\",\n#   # \t\"ALIGN_PERCENTILE_95\",\n#   # \t\"ALIGN_PERCENTILE_50\",\n#   # ]\n#\n#   ## Filters can be added to reduce the number of time series matched.  All\n#   ## functions are supported: starts_with, ends_with, has_substring, and\n#   ## one_of.  Only the '=' operator is supported.\n#   ##\n#   ## The logical operators when combining filters are defined statically using\n#   ## the following values:\n#   ##   filter ::= \u003cresource_labels\u003e {AND \u003cmetric_labels\u003e}\n#   ##   resource_labels ::= \u003cresource_labels\u003e {OR \u003cresource_label\u003e}\n#   ##   metric_labels ::= \u003cmetric_labels\u003e {OR \u003cmetric_label\u003e}\n#   ##\n#   ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n#   #\n#   ## Resource labels refine the time series selection with the following expression:\n#   ##   resource.labels.\u003ckey\u003e = \u003cvalue\u003e\n#   # [[inputs.stackdriver.filter.resource_labels]]\n#   #   key = \"instance_name\"\n#   #   value = 'starts_with(\"localhost\")'\n#   #\n#   ## Metric labels refine the time series selection with the following expression:\n#   ##   metric.labels.\u003ckey\u003e = \u003cvalue\u003e\n#   #  [[inputs.stackdriver.filter.metric_labels]]\n#   #  \t key = \"device_name\"\n#   #  \t value = 'one_of(\"sda\", \"sdb\")'\n\n\n# # Get synproxy counter statistics from procfs\n# [[inputs.synproxy]]\n#   # no configuration\n\n\n# # Sysstat metrics collector\n# [[inputs.sysstat]]\n#   ## Path to the sadc command.\n#   #\n#   ## Common Defaults:\n#   ##   Debian/Ubuntu: /usr/lib/sysstat/sadc\n#   ##   Arch:          /usr/lib/sa/sadc\n#   ##   RHEL/CentOS:   /usr/lib64/sa/sadc\n#   sadc_path = \"/usr/lib/sa/sadc\" # required\n#\n#   ## Path to the sadf command, if it is not in PATH\n#   # sadf_path = \"/usr/bin/sadf\"\n#\n#   ## Activities is a list of activities, that are passed as argument to the\n#   ## sadc collector utility (e.g: DISK, SNMP etc...)\n#   ## The more activities that are added, the more data is collected.\n#   # activities = [\"DISK\"]\n#\n#   ## Group metrics to measurements.\n#   ##\n#   ## If group is false each metric will be prefixed with a description\n#   ## and represents itself a measurement.\n#   ##\n#   ## If Group is true, corresponding metrics are grouped to a single measurement.\n#   # group = true\n#\n#   ## Options for the sadf command. The values on the left represent the sadf\n#   ## options and the values on the right their description (which are used for\n#   ## grouping and prefixing metrics).\n#   ##\n#   ## Run 'sar -h' or 'man sar' to find out the supported options for your\n#   ## sysstat version.\n#   [inputs.sysstat.options]\n#     -C = \"cpu\"\n#     -B = \"paging\"\n#     -b = \"io\"\n#     -d = \"disk\"             # requires DISK activity\n#     \"-n ALL\" = \"network\"\n#     \"-P ALL\" = \"per_cpu\"\n#     -q = \"queue\"\n#     -R = \"mem\"\n#     -r = \"mem_util\"\n#     -S = \"swap_util\"\n#     -u = \"cpu_util\"\n#     -v = \"inode\"\n#     -W = \"swap\"\n#     -w = \"task\"\n#   #  -H = \"hugepages\"        # only available for newer linux distributions\n#   #  \"-I ALL\" = \"interrupts\" # requires INT activity\n#\n#   ## Device tags can be used to add additional tags for devices.\n#   ## For example the configuration below adds a tag vg with value rootvg for\n#   ## all metrics with sda devices.\n#   # [[inputs.sysstat.device_tags.sda]]\n#   #  vg = \"rootvg\"\n\n\n# # Gather systemd units state\n# [[inputs.systemd_units]]\n#   ## Set timeout for systemctl execution\n#   # timeout = \"1s\"\n#   #\n#   ## Filter for a specific unit type, default is \"service\", other possible\n#   ## values are \"socket\", \"target\", \"device\", \"mount\", \"automount\", \"swap\",\n#   ## \"timer\", \"path\", \"slice\" and \"scope \":\n#   # unittype = \"service\"\n#   #\n#   ## Filter for a specific pattern, default is \"\" (i.e. all), other possible\n#   ## values are valid pattern for systemctl, e.g. \"a*\" for all units with\n#   ## names starting with \"a\"\n#   # pattern = \"\"\n#   ## pattern = \"telegraf* influxdb*\"\n#   ## pattern = \"a*\"\n\n\n# # Reads metrics from a Teamspeak 3 Server via ServerQuery\n# [[inputs.teamspeak]]\n#   ## Server address for Teamspeak 3 ServerQuery\n#   # server = \"127.0.0.1:10011\"\n#   ## Username for ServerQuery\n#   username = \"serverqueryuser\"\n#   ## Password for ServerQuery\n#   password = \"secret\"\n#   ## Array of virtual servers\n#   # virtual_servers = [1]\n\n\n# # Read metrics about temperature\n# [[inputs.temp]]\n#   # no configuration\n\n\n# # Read Tengine's basic status information (ngx_http_reqstat_module)\n# [[inputs.tengine]]\n#   # An array of Tengine reqstat module URI to gather stats.\n#   urls = [\"http://127.0.0.1/us\"]\n#\n#   # HTTP response timeout (default: 5s)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.cer\"\n#   # tls_key = \"/etc/telegraf/key.key\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Gather metrics from the Tomcat server status page.\n# [[inputs.tomcat]]\n#   ## URL of the Tomcat server status\n#   # url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n#\n#   ## HTTP Basic Auth Credentials\n#   # username = \"tomcat\"\n#   # password = \"s3cret\"\n#\n#   ## Request timeout\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Inserts sine and cosine waves for demonstration purposes\n# [[inputs.trig]]\n#   ## Set the amplitude\n#   amplitude = 10.0\n\n\n# # Read Twemproxy stats data\n# [[inputs.twemproxy]]\n#   ## Twemproxy stats address and port (no scheme)\n#   addr = \"localhost:22222\"\n#   ## Monitor pool name\n#   pools = [\"redis_pool\", \"mc_pool\"]\n\n\n# # A plugin to collect stats from the Unbound DNS resolver\n# [[inputs.unbound]]\n#   ## Address of server to connect to, read from unbound conf default, optionally ':port'\n#   ## Will lookup IP if given a hostname\n#   server = \"127.0.0.1:8953\"\n#\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   # use_sudo = false\n#\n#   ## The default location of the unbound-control binary can be overridden with:\n#   # binary = \"/usr/sbin/unbound-control\"\n#\n#   ## The default location of the unbound config file can be overridden with:\n#   # config_file = \"/etc/unbound/unbound.conf\"\n#\n#   ## The default timeout of 1s can be overridden with:\n#   # timeout = \"1s\"\n#\n#   ## When set to true, thread metrics are tagged with the thread id.\n#   ##\n#   ## The default is false for backwards compatibility, and will be changed to\n#   ## true in a future version.  It is recommended to set to true on new\n#   ## deployments.\n#   thread_as_tag = false\n\n\n# # Read uWSGI metrics.\n# [[inputs.uwsgi]]\n#   ## List with urls of uWSGI Stats servers. URL must match pattern:\n#   ## scheme://address[:port]\n#   ##\n#   ## For example:\n#   ## servers = [\"tcp://localhost:5050\", \"http://localhost:1717\", \"unix:///tmp/statsock\"]\n#   servers = [\"tcp://127.0.0.1:1717\"]\n#\n#   ## General connection timeout\n#   # timeout = \"5s\"\n\n\n# # A plugin to collect stats from Varnish HTTP Cache\n# [[inputs.varnish]]\n#   ## If running as a restricted user you can prepend sudo for additional access:\n#   #use_sudo = false\n#\n#   ## The default location of the varnishstat binary can be overridden with:\n#   binary = \"/usr/bin/varnishstat\"\n#\n#   ## Additional custom arguments for the varnishstat command\n#   # binary_args = [\"-f\", \"MAIN.*\"]\n#\n#   ## The default location of the varnishadm binary can be overriden with:\n#   adm_binary = \"/usr/bin/varnishadm\"\n#\n#   ## Custom arguments for the varnishadm command\n#   # adm_binary_args = [\"\"]\n#\n#   ## Metric version defaults to metric_version=1, use metric_version=2 for removal of nonactive vcls.\n#   metric_version = 1\n#\n#   ## Additional regexps to override builtin conversion of varnish metrics into telegraf metrics.\n#   ## Regexp group \"_vcl\" is used for extracting the VCL name. Metrics that contains nonactive VCL's are skipped.\n#   ## Regexp group \"_field\" overrides field name. Other named regexp groups are used as tags.\n#   # regexps = ['XCNT\\.(?P\u003c_vcl\u003e[\\w\\-]*)\\.(?P\u003cgroup\u003e[\\w\\-.+]*)\\.(?P\u003c_field\u003e[\\w\\-.+]*)\\.val']\n#\n#   ## By default, telegraf gather stats for 3 metric points.\n#   ## Setting stats will override the defaults shown below.\n#   ## Glob matching can be used, ie, stats = [\"MAIN.*\"]\n#   ## stats may also be set to [\"*\"], which will collect all stats\n#   stats = [\"MAIN.cache_hit\", \"MAIN.cache_miss\", \"MAIN.uptime\"]\n#\n#   ## Optional name for the varnish instance (or working directory) to query\n#   ## Usually append after -n in varnish cli\n#   # instance_name = instanceName\n#\n#   ## Timeout for varnishstat command\n#   # timeout = \"1s\"\n\n\n# # Read metrics from the Vault API\n# [[inputs.vault]]\n#   ## URL for the Vault agent\n#   # url = \"http://127.0.0.1:8200\"\n#\n#   ## Use Vault token for authorization.\n#   ## Vault token configuration is mandatory.\n#   ## If both are empty or both are set, an error is thrown.\n#   # token_file = \"/path/to/auth/token\"\n#   ## OR\n#   token = \"s.CDDrgg5zPv5ssI0Z2P4qxJj2\"\n#\n#   ## Set response_timeout (default 5 seconds)\n#   # response_timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n\n\n# # Collect Wireguard server interface and peer statistics\n# [[inputs.wireguard]]\n#   ## Optional list of Wireguard device/interface names to query.\n#   ## If omitted, all Wireguard interfaces are queried.\n#   # devices = [\"wg0\"]\n\n\n# # Monitor wifi signal strength and quality\n# [[inputs.wireless]]\n#   ## Sets 'proc' directory path\n#   ## If not specified, then default is /proc\n#   # host_proc = \"/proc\"\n\n\n# # Reads metrics from a SSL certificate\n# [[inputs.x509_cert]]\n#   ## List certificate sources\n#   ## Prefix your entry with 'file://' if you intend to use relative paths\n#   sources = [\"tcp://example.org:443\", \"https://influxdata.com:443\",\n#             \"udp://127.0.0.1:4433\", \"/etc/ssl/certs/ssl-cert-snakeoil.pem\",\n#             \"/etc/mycerts/*.mydomain.org.pem\", \"file:///path/to/*.pem\"]\n#\n#   ## Timeout for SSL connection\n#   # timeout = \"5s\"\n#\n#   ## Pass a different name into the TLS request (Server Name Indication)\n#   ##   example: server_name = \"myhost.example.org\"\n#   # server_name = \"\"\n#\n#   ## Don't include root or intermediate certificates in output\n#   # exclude_root_certs = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n\n\n# # Gathers Metrics From a Dell EMC XtremIO Storage Array's V3 API\n# [[inputs.xtremio]]\n#   ## XtremIO User Interface Endpoint\n#   url = \"https://xtremio.example.com/\" # required\n#\n#   ## Credentials\n#   username = \"user1\"\n#   password = \"pass123\"\n#\n#   ## Metrics to collect from the XtremIO\n#   # collectors = [\"bbus\",\"clusters\",\"ssds\",\"volumes\",\"xms\"]\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use SSL but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, pools and datasets\n# [[inputs.zfs]]\n#   ## ZFS kstat path. Ignored on FreeBSD\n#   ## If not specified, then default is:\n#   # kstatPath = \"/proc/spl/kstat/zfs\"\n#\n#   ## By default, telegraf gather all zfs stats\n#   ## If not specified, then default is:\n#   # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n#   ## For Linux, the default is:\n#   # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n#   #   \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n#   ## By default, don't gather zpool stats\n#   # poolMetrics = false\n#   ## By default, don't gather zdataset stats\n#   # datasetMetrics = false\n\n\n# # Reads 'mntr' stats from one or many zookeeper servers\n# [[inputs.zookeeper]]\n#   ## An array of address to gather stats about. Specify an ip or hostname\n#   ## with port. ie localhost:2181, 10.0.0.1:2181, etc.\n#\n#   ## If no servers are specified, then localhost is used as the host.\n#   ## If no port is specified, 2181 is used\n#   servers = [\":2181\"]\n#\n#   ## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n#   # timeout = \"5s\"\n#\n#   ## Optional TLS Config\n#   # enable_tls = true\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## If false, skip chain \u0026 host verification\n#   # insecure_skip_verify = true\n\n\n###############################################################################\n#                            SERVICE INPUT PLUGINS                            #\n###############################################################################\n\n\n# # Pull Metric Statistics from Aliyun CMS\n# [[inputs.aliyuncms]]\n#   ## Aliyun Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Ram RoleArn credential\n#   ## 2) AccessKey STS token credential\n#   ## 3) AccessKey credential\n#   ## 4) Ecs Ram Role credential\n#   ## 5) RSA keypair credential\n#   ## 6) Environment variables credential\n#   ## 7) Instance metadata credential\n#\n#   # access_key_id = \"\"\n#   # access_key_secret = \"\"\n#   # access_key_sts_token = \"\"\n#   # role_arn = \"\"\n#   # role_session_name = \"\"\n#   # private_key = \"\"\n#   # public_key_id = \"\"\n#   # role_name = \"\"\n#\n#   ## Specify the ali cloud region list to be queried for metrics and objects discovery\n#   ## If not set, all supported regions (see below) would be covered, it can provide a significant load on API, so the recommendation here\n#   ## is to limit the list as much as possible. Allowed values: https://www.alibabacloud.com/help/zh/doc-detail/40654.htm\n#   ## Default supported regions are:\n#   ## 21 items: cn-qingdao,cn-beijing,cn-zhangjiakou,cn-huhehaote,cn-hangzhou,cn-shanghai,cn-shenzhen,\n#   ##           cn-heyuan,cn-chengdu,cn-hongkong,ap-southeast-1,ap-southeast-2,ap-southeast-3,ap-southeast-5,\n#   ##           ap-south-1,ap-northeast-1,us-west-1,us-east-1,eu-central-1,eu-west-1,me-east-1\n#   ##\n#   ## From discovery perspective it set the scope for object discovery, the discovered info can be used to enrich\n#   ## the metrics with objects attributes/tags. Discovery is supported not for all projects (if not supported, then\n#   ## it will be reported on the start - for example for 'acs_cdn' project:\n#   ## 'E! [inputs.aliyuncms] Discovery tool is not activated: no discovery support for project \"acs_cdn\"' )\n#   ## Currently, discovery supported for the following projects:\n#   ## - acs_ecs_dashboard\n#   ## - acs_rds_dashboard\n#   ## - acs_slb_dashboard\n#   ## - acs_vpc_eip\n#   regions = [\"cn-hongkong\"]\n#\n#   # The minimum period for AliyunCMS metrics is 1 minute (60s). However not all\n#   # metrics are made available to the 1 minute period. Some are collected at\n#   # 3 minute, 5 minute, or larger intervals.\n#   # See: https://help.aliyun.com/document_detail/51936.html?spm=a2c4g.11186623.2.18.2bc1750eeOw1Pv\n#   # Note that if a period is configured that is smaller than the minimum for a\n#   # particular metric, that metric will not be returned by the Aliyun OpenAPI\n#   # and will not be collected by Telegraf.\n#   #\n#   ## Requested AliyunCMS aggregation Period (required - must be a multiple of 60s)\n#   period = \"5m\"\n#\n#   ## Collection Delay (required - must account for metrics availability via AliyunCMS API)\n#   delay = \"1m\"\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n#   ## gaps or overlap in pulled data\n#   interval = \"5m\"\n#\n#   ## Metric Statistic Project (required)\n#   project = \"acs_slb_dashboard\"\n#\n#   ## Maximum requests per second, default value is 200\n#   ratelimit = 200\n#\n#   ## How often the discovery API call executed (default 1m)\n#   #discovery_interval = \"1m\"\n#\n#   ## Metrics to Pull (Required)\n#   [[inputs.aliyuncms.metrics]]\n#   ## Metrics names to be requested,\n#   ## described here (per project): https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n#   names = [\"InstanceActiveConnection\", \"InstanceNewConnection\"]\n#\n#   ## Dimension filters for Metric (these are optional).\n#   ## This allows to get additional metric dimension. If dimension is not specified it can be returned or\n#   ## the data can be aggregated - it depends on particular metric, you can find details here: https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n#   ##\n#   ## Note, that by default dimension filter includes the list of discovered objects in scope (if discovery is enabled)\n#   ## Values specified here would be added into the list of discovered objects.\n#   ## You can specify either single dimension:\n#   #dimensions = '{\"instanceId\": \"p-example\"}'\n#\n#   ## Or you can specify several dimensions at once:\n#   #dimensions = '[{\"instanceId\": \"p-example\"},{\"instanceId\": \"q-example\"}]'\n#\n#   ## Enrichment tags, can be added from discovery (if supported)\n#   ## Notation is \u003cmeasurement_tag_name\u003e:\u003cJMES query path (https://jmespath.org/tutorial.html)\u003e\n#   ## To figure out which fields are available, consult the Describe\u003cObjectType\u003e API per project.\n#   ## For example, for SLB: https://api.aliyun.com/#/?product=Slb\u0026version=2014-05-15\u0026api=DescribeLoadBalancers\u0026params={}\u0026tab=MOCK\u0026lang=GO\n#   #tag_query_path = [\n#   #    \"address:Address\",\n#   #    \"name:LoadBalancerName\",\n#   #    \"cluster_owner:Tags.Tag[?TagKey=='cs.cluster.name'].TagValue | [0]\"\n#   #    ]\n#   ## The following tags added by default: regionId (if discovery enabled), userId, instanceId.\n#\n#   ## Allow metrics without discovery data, if discovery is enabled. If set to true, then metric without discovery\n#   ## data would be emitted, otherwise dropped. This cane be of help, in case debugging dimension filters, or partial coverage\n#   ## of discovery scope vs monitoring scope\n#   #allow_dps_without_discovery = false\n\n\n# # AMQP consumer plugin\n# [[inputs.amqp_consumer]]\n#   ## Brokers to consume from.  If multiple brokers are specified a random broker\n#   ## will be selected anytime a connection is established.  This can be\n#   ## helpful for load balancing when not using a dedicated load balancer.\n#   brokers = [\"amqp://localhost:5672/influxdb\"]\n#\n#   ## Authentication credentials for the PLAIN auth_method.\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Name of the exchange to declare.  If unset, no exchange will be declared.\n#   exchange = \"telegraf\"\n#\n#   ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n#   # exchange_type = \"topic\"\n#\n#   ## If true, exchange will be passively declared.\n#   # exchange_passive = false\n#\n#   ## Exchange durability can be either \"transient\" or \"durable\".\n#   # exchange_durability = \"durable\"\n#\n#   ## Additional exchange arguments.\n#   # exchange_arguments = { }\n#   # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n#\n#   ## AMQP queue name.\n#   queue = \"telegraf\"\n#\n#   ## AMQP queue durability can be \"transient\" or \"durable\".\n#   queue_durability = \"durable\"\n#\n#   ## If true, queue will be passively declared.\n#   # queue_passive = false\n#\n#   ## A binding between the exchange and queue using this binding key is\n#   ## created.  If unset, no binding is created.\n#   binding_key = \"#\"\n#\n#   ## Maximum number of messages server should give to the worker.\n#   # prefetch_count = 50\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Auth method. PLAIN and EXTERNAL are supported\n#   ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n#   ## described here: https://www.rabbitmq.com/plugins.html\n#   # auth_method = \"PLAIN\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Content encoding for message payloads, can be set to \"gzip\" to or\n#   ## \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read Cassandra metrics through Jolokia\n# [[inputs.cassandra]]\n#   ## DEPRECATED: The 'cassandra' plugin is deprecated in version 1.7.0, use 'inputs.jolokia2' with the 'cassandra.conf' example configuration instead.\n#   ## DEPRECATED: The cassandra plugin has been deprecated.  Please use the\n#   ## jolokia2 plugin instead.\n#   ##\n#   ## see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n#\n#   context = \"/jolokia/read\"\n#   ## List of cassandra servers exposing jolokia read service\n#   servers = [\"myuser:mypassword@10.10.10.1:8778\",\"10.10.10.2:8778\",\":8778\"]\n#   ## List of metrics collected on above servers\n#   ## Each metric consists of a jmx path.\n#   ## This will collect all heap memory usage metrics from the jvm and\n#   ## ReadLatency metrics for all keyspaces and tables.\n#   ## \"type=Table\" in the query works with Cassandra3.0. Older versions might\n#   ## need to use \"type=ColumnFamily\"\n#   metrics  = [\n#     \"/java.lang:type=Memory/HeapMemoryUsage\",\n#     \"/org.apache.cassandra.metrics:type=Table,keyspace=*,scope=*,name=ReadLatency\"\n#   ]\n\n\n# # Cisco model-driven telemetry (MDT) input plugin for IOS XR, IOS XE and NX-OS platforms\n# [[inputs.cisco_telemetry_mdt]]\n#  ## Telemetry transport can be \"tcp\" or \"grpc\".  TLS is only supported when\n#  ## using the grpc transport.\n#  transport = \"grpc\"\n#\n#  ## Address and port to host telemetry listener\n#  service_address = \":57000\"\n#\n#  ## Enable TLS; grpc transport only.\n#  # tls_cert = \"/etc/telegraf/cert.pem\"\n#  # tls_key = \"/etc/telegraf/key.pem\"\n#\n#  ## Enable TLS client authentication and define allowed CA certificates; grpc\n#  ##  transport only.\n#  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#  ## Define (for certain nested telemetry measurements with embedded tags) which fields are tags\n#  # embedded_tags = [\"Cisco-IOS-XR-qos-ma-oper:qos/interface-table/interface/input/service-policy-names/service-policy-instance/statistics/class-stats/class-name\"]\n#\n#  ## Define aliases to map telemetry encoding paths to simple measurement names\n#  [inputs.cisco_telemetry_mdt.aliases]\n#    ifstats = \"ietf-interfaces:interfaces-state/interface/statistics\"\n#  ##Define Property Xformation, please refer README and https://pubhub.devnetcloud.com/media/dme-docs-9-3-3/docs/appendix/ for Model details.\n#  [inputs.cisco_telemetry_mdt.dmes]\n#    ModTs = \"ignore\"\n#    CreateTs = \"ignore\"\n\n\n# # Read metrics from one or many ClickHouse servers\n# [[inputs.clickhouse]]\n#   ## Username for authorization on ClickHouse server\n#   ## example: username = \"default\"\n#   username = \"default\"\n#\n#   ## Password for authorization on ClickHouse server\n#   ## example: password = \"super_secret\"\n#\n#   ## HTTP(s) timeout while getting metrics values\n#   ## The timeout includes connection time, any redirects, and reading the response body.\n#   ##   example: timeout = 1s\n#   # timeout = 5s\n#\n#   ## List of servers for metrics scraping\n#   ## metrics scrape via HTTP(s) clickhouse interface\n#   ## https://clickhouse.tech/docs/en/interfaces/http/\n#   ##    example: servers = [\"http://127.0.0.1:8123\",\"https://custom-server.mdb.yandexcloud.net\"]\n#   servers         = [\"http://127.0.0.1:8123\"]\n#\n#   ## If \"auto_discovery\"\" is \"true\" plugin tries to connect to all servers available in the cluster\n#   ## with using same \"user:password\" described in \"user\" and \"password\" parameters\n#   ## and get this server hostname list from \"system.clusters\" table\n#   ## see\n#   ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters\n#   ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers\n#   ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n#   ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables\n#   ##    example: auto_discovery = false\n#   # auto_discovery = true\n#\n#   ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n#   ## when this filter present then \"WHERE cluster IN (...)\" filter will apply\n#   ## please use only full cluster names here, regexp and glob filters is not allowed\n#   ## for \"/etc/clickhouse-server/config.d/remote.xml\"\n#   ## \u003cyandex\u003e\n#   ##  \u003cremote_servers\u003e\n#   ##    \u003cmy-own-cluster\u003e\n#   ##        \u003cshard\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-ru-1.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-ru-2.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##        \u003c/shard\u003e\n#   ##        \u003cshard\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-eu-1.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##          \u003creplica\u003e\u003chost\u003eclickhouse-eu-2.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n#   ##        \u003c/shard\u003e\n#   ##    \u003c/my-onw-cluster\u003e\n#   ##  \u003c/remote_servers\u003e\n#   ##\n#   ## \u003c/yandex\u003e\n#   ##\n#   ## example: cluster_include = [\"my-own-cluster\"]\n#   # cluster_include = []\n#\n#   ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n#   ## when this filter present then \"WHERE cluster NOT IN (...)\" filter will apply\n#   ##    example: cluster_exclude = [\"my-internal-not-discovered-cluster\"]\n#   # cluster_exclude = []\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Read metrics from Google PubSub\n# [[inputs.cloud_pubsub]]\n#   ## Required. Name of Google Cloud Platform (GCP) Project that owns\n#   ## the given PubSub subscription.\n#   project = \"my-project\"\n#\n#   ## Required. Name of PubSub subscription to ingest metrics from.\n#   subscription = \"my-subscription\"\n#\n#   ## Required. Data format to consume.\n#   ## Each data format has its own unique set of configuration options.\n#   ## Read more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n#   ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n#   ## Application Default Credentials, which is preferred.\n#   # credentials_file = \"path/to/my/creds.json\"\n#\n#   ## Optional. Number of seconds to wait before attempting to restart the\n#   ## PubSub subscription receiver after an unexpected error.\n#   ## If the streaming pull for a PubSub Subscription fails (receiver),\n#   ## the agent attempts to restart receiving messages after this many seconds.\n#   # retry_delay_seconds = 5\n#\n#   ## Optional. Maximum byte length of a message to consume.\n#   ## Larger messages are dropped with an error. If less than 0 or unspecified,\n#   ## treated as no limit.\n#   # max_message_len = 1000000\n#\n#   ## Optional. Maximum messages to read from PubSub that have not been written\n#   ## to an output. Defaults to 1000.\n#   ## For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message contains 10 metrics and the output\n#   ## metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## The following are optional Subscription ReceiveSettings in PubSub.\n#   ## Read more about these values:\n#   ## https://godoc.org/cloud.google.com/go/pubsub#ReceiveSettings\n#\n#   ## Optional. Maximum number of seconds for which a PubSub subscription\n#   ## should auto-extend the PubSub ACK deadline for each message. If less than\n#   ## 0, auto-extension is disabled.\n#   # max_extension = 0\n#\n#   ## Optional. Maximum number of unprocessed messages in PubSub\n#   ## (unacknowledged but not yet expired in PubSub).\n#   ## A value of 0 is treated as the default PubSub value.\n#   ## Negative values will be treated as unlimited.\n#   # max_outstanding_messages = 0\n#\n#   ## Optional. Maximum size in bytes of unprocessed messages in PubSub\n#   ## (unacknowledged but not yet expired in PubSub).\n#   ## A value of 0 is treated as the default PubSub value.\n#   ## Negative values will be treated as unlimited.\n#   # max_outstanding_bytes = 0\n#\n#   ## Optional. Max number of goroutines a PubSub Subscription receiver can spawn\n#   ## to pull messages from PubSub concurrently. This limit applies to each\n#   ## subscription separately and is treated as the PubSub default if less than\n#   ## 1. Note this setting does not limit the number of messages that can be\n#   ## processed concurrently (use \"max_outstanding_messages\" instead).\n#   # max_receiver_go_routines = 0\n#\n#   ## Optional. If true, Telegraf will attempt to base64 decode the\n#   ## PubSub message data before parsing\n#   # base64_data = false\n\n\n# # Google Cloud Pub/Sub Push HTTP listener\n# [[inputs.cloud_pubsub_push]]\n#   ## Address and port to host HTTP listener on\n#   service_address = \":8080\"\n#\n#   ## Application secret to verify messages originate from Cloud Pub/Sub\n#   # token = \"\"\n#\n#   ## Path to listen to.\n#   # path = \"/\"\n#\n#   ## Maximum duration before timing out read of the request\n#   # read_timeout = \"10s\"\n#   ## Maximum duration before timing out write of the response. This should be set to a value\n#   ## large enough that you can send at least 'metric_batch_size' number of messages within the\n#   ## duration.\n#   # write_timeout = \"10s\"\n#\n#   ## Maximum allowed http request body size in bytes.\n#   ## 0 means to use the default of 524,288,00 bytes (500 mebibytes)\n#   # max_body_size = \"500MB\"\n#\n#   ## Whether to add the pubsub metadata, such as message attributes and subscription as a tag.\n#   # add_meta = false\n#\n#   ## Optional. Maximum messages to read from PubSub that have not been written\n#   ## to an output. Defaults to 1000.\n#   ## For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message contains 10 metrics and the output\n#   ## metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Ingests files in a directory and then moves them to a target directory.\n# [[inputs.directory_monitor]]\n#   ## The directory to monitor and read files from.\n#   directory = \"\"\n#   #\n#   ## The directory to move finished files to.\n#   finished_directory = \"\"\n#   #\n#   ## The directory to move files to upon file error.\n#   ## If not provided, erroring files will stay in the monitored directory.\n#   # error_directory = \"\"\n#   #\n#   ## The amount of time a file is allowed to sit in the directory before it is picked up.\n#   ## This time can generally be low but if you choose to have a very large file written to the directory and it's potentially slow,\n#   ## set this higher so that the plugin will wait until the file is fully copied to the directory.\n#   # directory_duration_threshold = \"50ms\"\n#   #\n#   ## A list of the only file names to monitor, if necessary. Supports regex. If left blank, all files are ingested.\n#   # files_to_monitor = [\"^.*\\.csv\"]\n#   #\n#   ## A list of files to ignore, if necessary. Supports regex.\n#   # files_to_ignore = [\".DS_Store\"]\n#   #\n#   ## Maximum lines of the file to process that have not yet be written by the\n#   ## output. For best throughput set to the size of the output's metric_buffer_limit.\n#   ## Warning: setting this number higher than the output's metric_buffer_limit can cause dropped metrics.\n#   # max_buffered_metrics = 10000\n#   #\n#   ## The maximum amount of file paths to queue up for processing at once, before waiting until files are processed to find more files.\n#   ## Lowering this value will result in *slightly* less memory use, with a potential sacrifice in speed efficiency, if absolutely necessary.\n#   #\tfile_queue_size = 100000\n#   #\n#   ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n#   ## to disable. Cautious when file name variation is high, this can increase the cardinality\n#   ## significantly. Read more about cardinality here:\n#   ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n#   # file_tag = \"\"\n#   #\n#   ## The dataformat to be read from the files.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   ## NOTE: We currently only support parsing newline-delimited JSON. See the format here: https://github.com/ndjson/ndjson-spec\n#   data_format = \"influx\"\n\n\n# # Read logging output from the Docker engine\n# [[inputs.docker_log]]\n#   ## Docker Endpoint\n#   ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n#   ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n#   # endpoint = \"unix:///var/run/docker.sock\"\n#\n#   ## When true, container logs are read from the beginning; otherwise\n#   ## reading begins at the end of the log.\n#   # from_beginning = false\n#\n#   ## Timeout for Docker API calls.\n#   # timeout = \"5s\"\n#\n#   ## Containers to include and exclude. Globs accepted.\n#   ## Note that an empty array for both will include all containers\n#   # container_name_include = []\n#   # container_name_exclude = []\n#\n#   ## Container states to include and exclude. Globs accepted.\n#   ## When empty only containers in the \"running\" state will be captured.\n#   # container_state_include = []\n#   # container_state_exclude = []\n#\n#   ## docker labels to include and exclude as tags.  Globs accepted.\n#   ## Note that an empty array for both will include all labels as tags\n#   # docker_label_include = []\n#   # docker_label_exclude = []\n#\n#   ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n#   source_tag = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # Azure Event Hubs service input plugin\n# [[inputs.eventhub_consumer]]\n#   ## The default behavior is to create a new Event Hub client from environment variables.\n#   ## This requires one of the following sets of environment variables to be set:\n#   ##\n#   ## 1) Expected Environment Variables:\n#   ##    - \"EVENTHUB_CONNECTION_STRING\"\n#   ##\n#   ## 2) Expected Environment Variables:\n#   ##    - \"EVENTHUB_NAMESPACE\"\n#   ##    - \"EVENTHUB_NAME\"\n#   ##    - \"EVENTHUB_KEY_NAME\"\n#   ##    - \"EVENTHUB_KEY_VALUE\"\n#\n#   ## 3) Expected Environment Variables:\n#   ##    - \"EVENTHUB_NAMESPACE\"\n#   ##    - \"EVENTHUB_NAME\"\n#   ##    - \"AZURE_TENANT_ID\"\n#   ##    - \"AZURE_CLIENT_ID\"\n#   ##    - \"AZURE_CLIENT_SECRET\"\n#\n#   ## Uncommenting the option below will create an Event Hub client based solely on the connection string.\n#   ## This can either be the associated environment variable or hard coded directly.\n#   ## If this option is uncommented, environment variables will be ignored.\n#   ## Connection string should contain EventHubName (EntityPath)\n#   # connection_string = \"\"\n#\n#   ## Set persistence directory to a valid folder to use a file persister instead of an in-memory persister\n#   # persistence_dir = \"\"\n#\n#   ## Change the default consumer group\n#   # consumer_group = \"\"\n#\n#   ## By default the event hub receives all messages present on the broker, alternative modes can be set below.\n#   ## The timestamp should be in https://github.com/toml-lang/toml#offset-date-time format (RFC 3339).\n#   ## The 3 options below only apply if no valid persister is read from memory or file (e.g. first run).\n#   # from_timestamp =\n#   # latest = true\n#\n#   ## Set a custom prefetch count for the receiver(s)\n#   # prefetch_count = 1000\n#\n#   ## Add an epoch to the receiver(s)\n#   # epoch = 0\n#\n#   ## Change to set a custom user agent, \"telegraf\" is used by default\n#   # user_agent = \"telegraf\"\n#\n#   ## To consume from a specific partition, set the partition_ids option.\n#   ## An empty array will result in receiving from all partitions.\n#   # partition_ids = [\"0\",\"1\"]\n#\n#   ## Max undelivered messages\n#   # max_undelivered_messages = 1000\n#\n#   ## Set either option below to true to use a system property as timestamp.\n#   ## You have the choice between EnqueuedTime and IoTHubEnqueuedTime.\n#   ## It is recommended to use this setting when the data itself has no timestamp.\n#   # enqueued_time_as_ts = true\n#   # iot_hub_enqueued_time_as_ts = true\n#\n#   ## Tags or fields to create from keys present in the application property bag.\n#   ## These could for example be set by message enrichments in Azure IoT Hub.\n#   # application_property_tags = []\n#   # application_property_fields = []\n#\n#   ## Tag or field name to use for metadata\n#   ## By default all metadata is disabled\n#   # sequence_number_field = \"SequenceNumber\"\n#   # enqueued_time_field = \"EnqueuedTime\"\n#   # offset_field = \"Offset\"\n#   # partition_id_tag = \"PartitionID\"\n#   # partition_key_tag = \"PartitionKey\"\n#   # iot_hub_device_connection_id_tag = \"IoTHubDeviceConnectionID\"\n#   # iot_hub_auth_generation_id_tag = \"IoTHubAuthGenerationID\"\n#   # iot_hub_connection_auth_method_tag = \"IoTHubConnectionAuthMethod\"\n#   # iot_hub_connection_module_id_tag = \"IoTHubConnectionModuleID\"\n#   # iot_hub_enqueued_time_field = \"IoTHubEnqueuedTime\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Run executable as long-running input plugin\n# [[inputs.execd]]\n#   ## Program to run as daemon\n#   command = [\"telegraf-smartctl\", \"-d\", \"/dev/sda\"]\n#\n#   ## Define how the process is signaled on each collection interval.\n#   ## Valid values are:\n#   ##   \"none\"   : Do not signal anything.\n#   ##              The process must output metrics by itself.\n#   ##   \"STDIN\"   : Send a newline on STDIN.\n#   ##   \"SIGHUP\"  : Send a HUP signal. Not available on Windows.\n#   ##   \"SIGUSR1\" : Send a USR1 signal. Not available on Windows.\n#   ##   \"SIGUSR2\" : Send a USR2 signal. Not available on Windows.\n#   signal = \"none\"\n#\n#   ## Delay before the process is restarted after an unexpected termination\n#   restart_delay = \"10s\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # gNMI telemetry input plugin\n# [[inputs.gnmi]]\n#  ## Address and port of the gNMI GRPC server\n#  addresses = [\"10.49.234.114:57777\"]\n#\n#  ## define credentials\n#  username = \"cisco\"\n#  password = \"cisco\"\n#\n#  ## gNMI encoding requested (one of: \"proto\", \"json\", \"json_ietf\", \"bytes\")\n#  # encoding = \"proto\"\n#\n#  ## redial in case of failures after\n#  redial = \"10s\"\n#\n#  ## enable client-side TLS and define CA to authenticate the device\n#  # enable_tls = true\n#  # tls_ca = \"/etc/telegraf/ca.pem\"\n#  # insecure_skip_verify = true\n#\n#  ## define client-side TLS certificate \u0026 key to authenticate to the device\n#  # tls_cert = \"/etc/telegraf/cert.pem\"\n#  # tls_key = \"/etc/telegraf/key.pem\"\n#\n#  ## gNMI subscription prefix (optional, can usually be left empty)\n#  ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n#  # origin = \"\"\n#  # prefix = \"\"\n#  # target = \"\"\n#\n#  ## Define additional aliases to map telemetry encoding paths to simple measurement names\n#  #[inputs.gnmi.aliases]\n#  #  ifcounters = \"openconfig:/interfaces/interface/state/counters\"\n#\n#  [[inputs.gnmi.subscription]]\n#   ## Name of the measurement that will be emitted\n#   name = \"ifcounters\"\n#\n#   ## Origin and path of the subscription\n#   ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n#   ##\n#   ## origin usually refers to a (YANG) data model implemented by the device\n#   ## and path to a specific substructure inside it that should be subscribed to (similar to an XPath)\n#   ## YANG models can be found e.g. here: https://github.com/YangModels/yang/tree/master/vendor/cisco/xr\n#   origin = \"openconfig-interfaces\"\n#   path = \"/interfaces/interface/state/counters\"\n#\n#   # Subscription mode (one of: \"target_defined\", \"sample\", \"on_change\") and interval\n#   subscription_mode = \"sample\"\n#   sample_interval = \"10s\"\n#\n#   ## Suppress redundant transmissions when measured values are unchanged\n#   # suppress_redundant = false\n#\n#   ## If suppression is enabled, send updates at least every X seconds anyway\n#   # heartbeat_interval = \"60s\"\n#\n#   #[[inputs.gnmi.subscription]]\n#     # name = \"descr\"\n#     # origin = \"openconfig-interfaces\"\n#     # path = \"/interfaces/interface/state/description\"\n#     # subscription_mode = \"on_change\"\n#\n#     ## If tag_only is set, the subscription in question will be utilized to maintain a map of\n#     ## tags to apply to other measurements emitted by the plugin, by matching path keys\n#     ## All fields from the tag-only subscription will be applied as tags to other readings,\n#     ## in the format \u003cname\u003e_\u003cfieldBase\u003e.\n#     # tag_only = true\n\n\n# # Accept metrics over InfluxDB 1.x HTTP API\n# [[inputs.http_listener]]\n#   ## DEPRECATED: The 'http_listener' plugin is deprecated in version 1.9.0, has been renamed to 'influxdb_listener', use 'inputs.influxdb_listener' or 'inputs.http_listener_v2' instead.\n#   ## Address and port to host InfluxDB listener on\n#   service_address = \":8186\"\n#\n#   ## maximum duration before timing out read of the request\n#   read_timeout = \"10s\"\n#   ## maximum duration before timing out write of the response\n#   write_timeout = \"10s\"\n#\n#   ## Maximum allowed HTTP request body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   max_body_size = \"32MiB\"\n#\n#   ## Optional tag name used to store the database.\n#   ## If the write has a database in the query string then it will be kept in this tag name.\n#   ## This tag can be used in downstream outputs.\n#   ## The default value of nothing means it will be off and the database will not be recorded.\n#   # database_tag = \"\"\n#\n#   ## If set the retention policy specified in the write query will be added as\n#   ## the value of this tag name.\n#   # retention_policy_tag = \"\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   tls_cert = \"/etc/telegraf/cert.pem\"\n#   tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional username and password to accept for HTTP basic authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # basic_username = \"foobar\"\n#   # basic_password = \"barfoo\"\n#\n#   ## Influx line protocol parser\n#   ## 'internal' is the default. 'upstream' is a newer parser that is faster\n#   ## and more memory efficient.\n#   # parser_type = \"internal\"\n\n\n# # Generic HTTP write listener\n# [[inputs.http_listener_v2]]\n#   ## Address and port to host HTTP listener on\n#   service_address = \":8080\"\n#\n#   ## Paths to listen to.\n#   # paths = [\"/telegraf\"]\n#\n#   ## Save path as http_listener_v2_path tag if set to true\n#   # path_tag = false\n#\n#   ## HTTP methods to accept.\n#   # methods = [\"POST\", \"PUT\"]\n#\n#   ## maximum duration before timing out read of the request\n#   # read_timeout = \"10s\"\n#   ## maximum duration before timing out write of the response\n#   # write_timeout = \"10s\"\n#\n#   ## Maximum allowed http request body size in bytes.\n#   ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n#   # max_body_size = \"500MB\"\n#\n#   ## Part of the request to consume.  Available options are \"body\" and\n#   ## \"query\".\n#   # data_source = \"body\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional username and password to accept for HTTP basic authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # basic_username = \"foobar\"\n#   # basic_password = \"barfoo\"\n#\n#   ## Optional setting to map http headers into tags\n#   ## If the http header is not present on the request, no corresponding tag will be added\n#   ## If multiple instances of the http header are present, only the first value will be used\n#   # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Accept metrics over InfluxDB 1.x HTTP API\n# [[inputs.influxdb_listener]]\n#   ## Address and port to host InfluxDB listener on\n#   service_address = \":8186\"\n#\n#   ## maximum duration before timing out read of the request\n#   read_timeout = \"10s\"\n#   ## maximum duration before timing out write of the response\n#   write_timeout = \"10s\"\n#\n#   ## Maximum allowed HTTP request body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   max_body_size = \"32MiB\"\n#\n#   ## Optional tag name used to store the database.\n#   ## If the write has a database in the query string then it will be kept in this tag name.\n#   ## This tag can be used in downstream outputs.\n#   ## The default value of nothing means it will be off and the database will not be recorded.\n#   # database_tag = \"\"\n#\n#   ## If set the retention policy specified in the write query will be added as\n#   ## the value of this tag name.\n#   # retention_policy_tag = \"\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   tls_cert = \"/etc/telegraf/cert.pem\"\n#   tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional username and password to accept for HTTP basic authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # basic_username = \"foobar\"\n#   # basic_password = \"barfoo\"\n#\n#   ## Influx line protocol parser\n#   ## 'internal' is the default. 'upstream' is a newer parser that is faster\n#   ## and more memory efficient.\n#   # parser_type = \"internal\"\n\n\n# # Accept metrics over InfluxDB 2.x HTTP API\n# [[inputs.influxdb_v2_listener]]\n#   ## Address and port to host InfluxDB listener on\n#   ## (Double check the port. Could be 9999 if using OSS Beta)\n#   service_address = \":8086\"\n#\n#   ## Maximum allowed HTTP request body size in bytes.\n#   ## 0 means to use the default of 32MiB.\n#   # max_body_size = \"32MiB\"\n#\n#   ## Optional tag to determine the bucket.\n#   ## If the write has a bucket in the query string then it will be kept in this tag name.\n#   ## This tag can be used in downstream outputs.\n#   ## The default value of nothing means it will be off and the database will not be recorded.\n#   # bucket_tag = \"\"\n#\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Add service certificate and key\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Optional token to accept for HTTP authentication.\n#   ## You probably want to make sure you have TLS configured above for this.\n#   # token = \"some-long-shared-secret-token\"\n#\n#   ## Influx line protocol parser\n#   ## 'internal' is the default. 'upstream' is a newer parser that is faster\n#   ## and more memory efficient.\n#   # parser_type = \"internal\"\n\n\n# # Intel Performance Monitoring Unit plugin exposes Intel PMU metrics available through Linux Perf subsystem\n# [[inputs.intel_pmu]]\n#   ## List of filesystem locations of JSON files that contain PMU event definitions.\n#   event_definitions = [\"/var/cache/pmu/GenuineIntel-6-55-4-core.json\", \"/var/cache/pmu/GenuineIntel-6-55-4-uncore.json\"]\n#\n#   ## List of core events measurement entities. There can be more than one core_events sections.\n#   [[inputs.intel_pmu.core_events]]\n#     ## List of events to be counted. Event names shall match names from event_definitions files.\n#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n#     ## If absent, all core events from provided event_definitions are counted skipping unresolvable ones.\n#     events = [\"INST_RETIRED.ANY\", \"CPU_CLK_UNHALTED.THREAD_ANY:config1=0x4043200000000k\"]\n#\n#     ## Limits the counting of events to core numbers specified.\n#     ## If absent, events are counted on all cores.\n#     ## Single \"0\", multiple \"0,1,2\" and range \"0-2\" notation is supported for each array element.\n#     ##   example: cores = [\"0,2\", \"4\", \"12-16\"]\n#     cores = [\"0\"]\n#\n#     ## Indicator that plugin shall attempt to run core_events.events as a single perf group.\n#     ## If absent or set to false, each event is counted individually. Defaults to false.\n#     ## This limits the number of events that can be measured to a maximum of available hardware counters per core.\n#     ## Could vary depending on type of event, use of fixed counters.\n#     # perf_group = false\n#\n#     ## Optionally set a custom tag value that will be added to every measurement within this events group.\n#     ## Can be applied to any group of events, unrelated to perf_group setting.\n#     # events_tag = \"\"\n#\n#   ## List of uncore event measurement entities. There can be more than one uncore_events sections.\n#   [[inputs.intel_pmu.uncore_events]]\n#     ## List of events to be counted. Event names shall match names from event_definitions files.\n#     ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n#     ## If absent, all uncore events from provided event_definitions are counted skipping unresolvable ones.\n#     events = [\"UNC_CHA_CLOCKTICKS\", \"UNC_CHA_TOR_OCCUPANCY.IA_MISS\"]\n#\n#     ## Limits the counting of events to specified sockets.\n#     ## If absent, events are counted on all sockets.\n#     ## Single \"0\", multiple \"0,1\" and range \"0-1\" notation is supported for each array element.\n#     ##   example: sockets = [\"0-2\"]\n#     sockets = [\"0\"]\n#\n#     ## Indicator that plugin shall provide an aggregated value for multiple units of same type distributed in an uncore.\n#     ## If absent or set to false, events for each unit are exposed as separate metric. Defaults to false.\n#     # aggregate_uncore_units = false\n#\n#     ## Optionally set a custom tag value that will be added to every measurement within this events group.\n#     # events_tag = \"\"\n\n\n# # Intel Resource Director Technology plugin\n# [[inputs.intel_rdt]]\n# \t## Optionally set sampling interval to Nx100ms.\n# \t## This value is propagated to pqos tool. Interval format is defined by pqos itself.\n# \t## If not provided or provided 0, will be set to 10 = 10x100ms = 1s.\n# \t# sampling_interval = \"10\"\n# \t\n# \t## Optionally specify the path to pqos executable.\n# \t## If not provided, auto discovery will be performed.\n# \t# pqos_path = \"/usr/local/bin/pqos\"\n#\n# \t## Optionally specify if IPC and LLC_Misses metrics shouldn't be propagated.\n# \t## If not provided, default value is false.\n# \t# shortened_metrics = false\n# \t\n# \t## Specify the list of groups of CPU core(s) to be provided as pqos input.\n# \t## Mandatory if processes aren't set and forbidden if processes are specified.\n# \t## e.g. [\"0-3\", \"4,5,6\"] or [\"1-3,4\"]\n# \t# cores = [\"0-3\"]\n# \t\n# \t## Specify the list of processes for which Metrics will be collected.\n# \t## Mandatory if cores aren't set and forbidden if cores are specified.\n# \t## e.g. [\"qemu\", \"pmd\"]\n# \t# processes = [\"process\"]\n#\n# \t## Specify if the pqos process should be called with sudo.\n# \t## Mandatory if the telegraf process does not run as root.\n# \t# use_sudo = false\n\n\n# # Read JTI OpenConfig Telemetry from listed sensors\n# [[inputs.jti_openconfig_telemetry]]\n#   ## List of device addresses to collect telemetry from\n#   servers = [\"localhost:1883\"]\n#\n#   ## Authentication details. Username and password are must if device expects\n#   ## authentication. Client ID must be unique when connecting from multiple instances\n#   ## of telegraf to the same device\n#   username = \"user\"\n#   password = \"pass\"\n#   client_id = \"telegraf\"\n#\n#   ## Frequency to get data\n#   sample_frequency = \"1000ms\"\n#\n#   ## Sensors to subscribe for\n#   ## A identifier for each sensor can be provided in path by separating with space\n#   ## Else sensor path will be used as identifier\n#   ## When identifier is used, we can provide a list of space separated sensors.\n#   ## A single subscription will be created with all these sensors and data will\n#   ## be saved to measurement with this identifier name\n#   sensors = [\n#    \"/interfaces/\",\n#    \"collection /components/ /lldp\",\n#   ]\n#\n#   ## We allow specifying sensor group level reporting rate. To do this, specify the\n#   ## reporting rate in Duration at the beginning of sensor paths / collection\n#   ## name. For entries without reporting rate, we use configured sample frequency\n#   sensors = [\n#    \"1000ms customReporting /interfaces /lldp\",\n#    \"2000ms collection /components\",\n#    \"/interfaces\",\n#   ]\n#\n#   ## Optional TLS Config\n#   # enable_tls = true\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Delay between retry attempts of failed RPC calls or streams. Defaults to 1000ms.\n#   ## Failed streams/calls will not be retried if 0 is provided\n#   retry_delay = \"1000ms\"\n#\n#   ## To treat all string values as tags, set this to true\n#   str_as_tags = false\n\n\n# # Read metrics from Kafka topics\n# [[inputs.kafka_consumer]]\n#   ## Kafka brokers.\n#   brokers = [\"localhost:9092\"]\n#\n#   ## Topics to consume.\n#   topics = [\"telegraf\"]\n#\n#   ## When set this tag will be added to all metrics with the topic as the value.\n#   # topic_tag = \"\"\n#\n#   ## Optional Client id\n#   # client_id = \"Telegraf\"\n#\n#   ## Set the minimal supported Kafka version.  Setting this enables the use of new\n#   ## Kafka features and APIs.  Must be 0.10.2.0 or greater.\n#   ##   ex: version = \"1.1.0\"\n#   # version = \"\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## SASL authentication credentials.  These settings should typically be used\n#   ## with TLS encryption enabled\n#   # sasl_username = \"kafka\"\n#   # sasl_password = \"secret\"\n#\n#   ## Optional SASL:\n#   ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n#   ## (defaults to PLAIN)\n#   # sasl_mechanism = \"\"\n#\n#   ## used if sasl_mechanism is GSSAPI (experimental)\n#   # sasl_gssapi_service_name = \"\"\n#   # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n#   # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n#   # sasl_gssapi_kerberos_config_path = \"/\"\n#   # sasl_gssapi_realm = \"realm\"\n#   # sasl_gssapi_key_tab_path = \"\"\n#   # sasl_gssapi_disable_pafxfast = false\n#\n#   ## used if sasl_mechanism is OAUTHBEARER (experimental)\n#   # sasl_access_token = \"\"\n#\n#   ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n#   # sasl_version = 1\n#\n#   # Disable Kafka metadata full fetch\n#   # metadata_full = false\n#\n#   ## Name of the consumer group.\n#   # consumer_group = \"telegraf_metrics_consumers\"\n#\n#   ## Compression codec represents the various compression codecs recognized by\n#   ## Kafka in messages.\n#   ##  0 : None\n#   ##  1 : Gzip\n#   ##  2 : Snappy\n#   ##  3 : LZ4\n#   ##  4 : ZSTD\n#    # compression_codec = 0\n#\n#   ## Initial offset position; one of \"oldest\" or \"newest\".\n#   # offset = \"oldest\"\n#\n#   ## Consumer group partition assignment strategy; one of \"range\", \"roundrobin\" or \"sticky\".\n#   # balance_strategy = \"range\"\n#\n#   ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n#   ## larger messages are dropped\n#   max_message_len = 1000000\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Maximum amount of time the consumer should take to process messages. If\n#   ## the debug log prints messages from sarama about 'abandoning subscription\n#   ## to [topic] because consuming was taking too long', increase this value to\n#   ## longer than the time taken by the output plugin(s).\n#   ##\n#   ## Note that the effective timeout could be between 'max_processing_time' and\n#   ## '2 * max_processing_time'.\n#   # max_processing_time = \"100ms\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read metrics from Kafka topic(s)\n# [[inputs.kafka_consumer_legacy]]\n#   ## DEPRECATED: The 'kafka_consumer_legacy' plugin is deprecated in version 1.4.0, use 'inputs.kafka_consumer' instead, NOTE: 'kafka_consumer' only supports Kafka v0.8+.\n#   ## topic(s) to consume\n#   topics = [\"telegraf\"]\n#\n#   ## an array of Zookeeper connection strings\n#   zookeeper_peers = [\"localhost:2181\"]\n#\n#   ## Zookeeper Chroot\n#   zookeeper_chroot = \"\"\n#\n#   ## the name of the consumer group\n#   consumer_group = \"telegraf_metrics_consumers\"\n#\n#   ## Offset (must be either \"oldest\" or \"newest\")\n#   offset = \"oldest\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n#   ## larger messages are dropped\n#   max_message_len = 65536\n\n\n# # Configuration for the AWS Kinesis input.\n# [[inputs.kinesis_consumer]]\n#   ## Amazon REGION of kinesis endpoint.\n#   region = \"ap-southeast-2\"\n#\n#   ## Amazon Credentials\n#   ## Credentials are loaded in the following order\n#   ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n#   ## 2) Assumed credentials via STS if role_arn is specified\n#   ## 3) explicit credentials from 'access_key' and 'secret_key'\n#   ## 4) shared profile from 'profile'\n#   ## 5) environment variables\n#   ## 6) shared credentials file\n#   ## 7) EC2 Instance Profile\n#   # access_key = \"\"\n#   # secret_key = \"\"\n#   # token = \"\"\n#   # role_arn = \"\"\n#   # web_identity_token_file = \"\"\n#   # role_session_name = \"\"\n#   # profile = \"\"\n#   # shared_credential_file = \"\"\n#\n#   ## Endpoint to make request against, the correct endpoint is automatically\n#   ## determined and this option should only be set if you wish to override the\n#   ## default.\n#   ##   ex: endpoint_url = \"http://localhost:8000\"\n#   # endpoint_url = \"\"\n#\n#   ## Kinesis StreamName must exist prior to starting telegraf.\n#   streamname = \"StreamName\"\n#\n#   ## Shard iterator type (only 'TRIM_HORIZON' and 'LATEST' currently supported)\n#   # shard_iterator_type = \"TRIM_HORIZON\"\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ##\n#   ## The content encoding of the data from kinesis\n#   ## If you are processing a cloudwatch logs kinesis stream then set this to \"gzip\"\n#   ## as AWS compresses cloudwatch log data before it is sent to kinesis (aws\n#   ## also base64 encodes the zip byte data before pushing to the stream.  The base64 decoding\n#   ## is done automatically by the golang sdk, as data is read from kinesis)\n#   ##\n#   # content_encoding = \"identity\"\n#\n#   ## Optional\n#   ## Configuration for a dynamodb checkpoint\n#   [inputs.kinesis_consumer.checkpoint_dynamodb]\n# \t## unique name for this consumer\n# \tapp_name = \"default\"\n# \ttable_name = \"default\"\n\n\n# # Listener capable of handling KNX bus messages provided through a KNX-IP Interface.\n# [[inputs.knx_listener]]\n#   ## Type of KNX-IP interface.\n#   ## Can be either \"tunnel\" or \"router\".\n#   # service_type = \"tunnel\"\n#\n#   ## Address of the KNX-IP interface.\n#   service_address = \"localhost:3671\"\n#\n#   ## Measurement definition(s)\n#   # [[inputs.knx_listener.measurement]]\n#   #   ## Name of the measurement\n#   #   name = \"temperature\"\n#   #   ## Datapoint-Type (DPT) of the KNX messages\n#   #   dpt = \"9.001\"\n#   #   ## List of Group-Addresses (GAs) assigned to the measurement\n#   #   addresses = [\"5/5/1\"]\n#\n#   # [[inputs.knx_listener.measurement]]\n#   #   name = \"illumination\"\n#   #   dpt = \"9.004\"\n#   #   addresses = [\"5/5/3\"]\n\n\n# # Read metrics off Arista LANZ, via socket\n# [[inputs.lanz]]\n#   ## URL to Arista LANZ endpoint\n#   servers = [\n#     \"tcp://127.0.0.1:50001\"\n#   ]\n\n\n\n# # Read metrics from MQTT topic(s)\n# [[inputs.mqtt_consumer]]\n#   ## Broker URLs for the MQTT server or cluster.  To connect to multiple\n#   ## clusters or standalone servers, use a separate plugin instance.\n#   ##   example: servers = [\"tcp://localhost:1883\"]\n#   ##            servers = [\"ssl://localhost:1883\"]\n#   ##            servers = [\"ws://localhost:1883\"]\n#   servers = [\"tcp://127.0.0.1:1883\"]\n#   ## Topics that will be subscribed to.\n#   topics = [\n#     \"telegraf/host01/cpu\",\n#     \"telegraf/+/mem\",\n#     \"sensors/#\",\n#   ]\n#   # topic_fields = \"_/_/_/temperature\"\n#   ## The message topic will be stored in a tag specified by this value.  If set\n#   ## to the empty string no topic tag will be created.\n#   # topic_tag = \"topic\"\n#   ## QoS policy for messages\n#   ##   0 = at most once\n#   ##   1 = at least once\n#   ##   2 = exactly once\n#   ##\n#   ## When using a QoS of 1 or 2, you should enable persistent_session to allow\n#   ## resuming unacknowledged messages.\n#   # qos = 0\n#   ## Connection timeout for initial connection in seconds\n#   # connection_timeout = \"30s\"\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#   ## Persistent session disables clearing of the client session on connection.\n#   ## In order for this option to work you must also set client_id to identify\n#   ## the client.  To receive messages that arrived while the client is offline,\n#   ## also set the qos option to 1 or 2 and don't forget to also set the QoS when\n#   ## publishing.\n#   # persistent_session = false\n#   ## If unset, a random client ID will be generated.\n#   # client_id = \"\"\n#   ## Username and password to connect MQTT server.\n#   # username = \"telegraf\"\n#   # password = \"metricsmetricsmetricsmetrics\"\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#   ## Enable extracting tag values from MQTT topics\n#   ## _ denotes an ignored entry in the topic path\n#   ## [[inputs.mqtt_consumer.topic_parsing]]\n#   ##  topic = \"\"\n#   ##  measurement = \"\"\n#   ##  tags = \"\"\n#   ##  fields = \"\"\n#   ## [inputs.mqtt_consumer.topic_parsing.types]\n#   ##\n\n\n# # Read metrics from NATS subject(s)\n# [[inputs.nats_consumer]]\n#   ## urls of NATS servers\n#   servers = [\"nats://localhost:4222\"]\n#\n#   ## subject(s) to consume\n#   subjects = [\"telegraf\"]\n#\n#   ## name a queue group\n#   queue_group = \"telegraf_consumers\"\n#\n#   ## Optional credentials\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Optional NATS 2.0 and NATS NGS compatible user credentials\n#   # credentials = \"/etc/telegraf/nats.creds\"\n#\n#   ## Use Transport Layer Security\n#   # secure = false\n#\n#   ## Optional TLS Config\n#   # tls_ca = \"/etc/telegraf/ca.pem\"\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## Sets the limits for pending msgs and bytes for each subscription\n#   ## These shouldn't need to be adjusted except in very high throughput scenarios\n#   # pending_message_limit = 65536\n#   # pending_bytes_limit = 67108864\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Read NSQ topic for metrics.\n# [[inputs.nsq_consumer]]\n#   ## An array representing the NSQD TCP HTTP Endpoints\n#   nsqd = [\"localhost:4150\"]\n#\n#   ## An array representing the NSQLookupd HTTP Endpoints\n#   nsqlookupd = [\"localhost:4161\"]\n#   topic = \"telegraf\"\n#   channel = \"consumer\"\n#   max_in_flight = 100\n#\n#   ## Maximum messages to read from the broker that have not been written by an\n#   ## output.  For best throughput set based on the number of metrics within\n#   ## each message and the size of the output's metric_batch_size.\n#   ##\n#   ## For example, if each message from the queue contains 10 metrics and the\n#   ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n#   ## full batch is collected and the write is triggered immediately without\n#   ## waiting until the next flush_interval.\n#   # max_undelivered_messages = 1000\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n\n\n# # Receive OpenTelemetry traces, metrics, and logs over gRPC\n# [[inputs.opentelemetry]]\n#   ## Override the default (0.0.0.0:4317) destination OpenTelemetry gRPC service\n#   ## address:port\n#   # service_address = \"0.0.0.0:4317\"\n#\n#   ## Override the default (5s) new connection timeout\n#   # timeout = \"5s\"\n#\n#   ## Override the default (prometheus-v1) metrics schema.\n#   ## Supports: \"prometheus-v1\", \"prometheus-v2\"\n#   ## For more information about the alternatives, read the Prometheus input\n#   ## plugin notes.\n#   # metrics_schema = \"prometheus-v1\"\n#\n#   ## Optional TLS Config.\n#   ## For advanced options: https://github.com/influxdata/telegraf/blob/v1.18.3/docs/TLS.md\n#   ##\n#   ## Set one or more allowed client CA certificate file names to\n#   ## enable mutually authenticated TLS connections.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#   ## Add service certificate and key.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n\n\n# # Read metrics from one or many pgbouncer servers\n# [[inputs.pgbouncer]]\n#   ## specify address via a url matching:\n#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n#   ##       ?sslmode=[disable|verify-ca|verify-full]\n#   ## or a simple string:\n#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n#   ##\n#   ## All connection parameters are optional.\n#   ##\n#   address = \"host=localhost user=pgbouncer sslmode=disable\"\n\n\n# # Read metrics from one or many postgresql servers\n# [[inputs.postgresql]]\n#   ## specify address via a url matching:\n#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n#   ##       ?sslmode=[disable|verify-ca|verify-full]\n#   ## or a simple string:\n#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n#   ##\n#   ## All connection parameters are optional.\n#   ##\n#   ## Without the dbname parameter, the driver will default to a database\n#   ## with the same name as the user. This dbname is just for instantiating a\n#   ## connection with the server and doesn't restrict the databases we are trying\n#   ## to grab metrics for.\n#   ##\n#   address = \"host=localhost user=postgres sslmode=disable\"\n#   ## A custom name for the database that will be used as the \"server\" tag in the\n#   ## measurement output. If not specified, a default one generated from\n#   ## the connection address is used.\n#   # outputaddress = \"db01\"\n#\n#   ## connection configuration.\n#   ## maxlifetime - specify the maximum lifetime of a connection.\n#   ## default is forever (0s)\n#   max_lifetime = \"0s\"\n#\n#   ## A  list of databases to explicitly ignore.  If not specified, metrics for all\n#   ## databases are gathered.  Do NOT use with the 'databases' option.\n#   # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n#\n#   ## A list of databases to pull metrics about. If not specified, metrics for all\n#   ## databases are gathered.  Do NOT use with the 'ignored_databases' option.\n#   # databases = [\"app_production\", \"testing\"]\n#\n#   ## Whether to use prepared statements when connecting to the database.\n#   ## This should be set to false when connecting through a PgBouncer instance\n#   ## with pool_mode set to transaction.\n#   # prepared_statements = true\n\n\n# # Read metrics from one or many postgresql servers\n# [[inputs.postgresql_extensible]]\n#   ## specify address via a url matching:\n#   ##   postgres://[pqgotest[:password]]@localhost[/dbname]\\\n#   ##       ?sslmode=[disable|verify-ca|verify-full]\n#   ## or a simple string:\n#   ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n#   #\n#   ## All connection parameters are optional.  #\n#   ## Without the dbname parameter, the driver will default to a database\n#   ## with the same name as the user. This dbname is just for instantiating a\n#   ## connection with the server and doesn't restrict the databases we are trying\n#   ## to grab metrics for.\n#   #\n#   address = \"host=localhost user=postgres sslmode=disable\"\n#\n#   ## connection configuration.\n#   ## maxlifetime - specify the maximum lifetime of a connection.\n#   ## default is forever (0s)\n#   max_lifetime = \"0s\"\n#\n#   ## Whether to use prepared statements when connecting to the database.\n#   ## This should be set to false when connecting through a PgBouncer instance\n#   ## with pool_mode set to transaction.\n#   # prepared_statements = true\n#\n#   ## A list of databases to pull metrics about. If not specified, metrics for all\n#   ## databases are gathered.\n#   ## databases = [\"app_production\", \"testing\"]\n#   #\n#   ## A custom name for the database that will be used as the \"server\" tag in the\n#   ## measurement output. If not specified, a default one generated from\n#   ## the connection address is used.\n#   # outputaddress = \"db01\"\n#   #\n#   ## Define the toml config where the sql queries are stored\n#   ## New queries can be added, if the withdbname is set to true and there is no\n#   ## databases defined in the 'databases field', the sql query is ended by a\n#   ## 'is not null' in order to make the query succeed.\n#   ## Example :\n#   ## The sqlquery : \"SELECT * FROM pg_stat_database where datname\" become\n#   ## \"SELECT * FROM pg_stat_database where datname IN ('postgres', 'pgbench')\"\n#   ## because the databases variable was set to ['postgres', 'pgbench' ] and the\n#   ## withdbname was true. Be careful that if the withdbname is set to false you\n#   ## don't have to define the where clause (aka with the dbname) the tagvalue\n#   ## field is used to define custom tags (separated by commas)\n#   ## The optional \"measurement\" value can be used to override the default\n#   ## output measurement name (\"postgresql\").\n#   ##\n#   ## The script option can be used to specify the .sql file path.\n#   ## If script and sqlquery options specified at same time, sqlquery will be used\n#   ##\n#   ## the tagvalue field is used to define custom tags (separated by comas).\n#   ## the query is expected to return columns which match the names of the\n#   ## defined tags. The values in these columns must be of a string-type,\n#   ## a number-type or a blob-type.\n#   ##\n#   ## The timestamp field is used to override the data points timestamp value. By\n#   ## default, all rows inserted with current time. By setting a timestamp column,\n#   ## the row will be inserted with that column's value.\n#   ##\n#   ## Structure :\n#   ## [[inputs.postgresql_extensible.query]]\n#   ##   sqlquery string\n#   ##   version string\n#   ##   withdbname boolean\n#   ##   tagvalue string (comma separated)\n#   ##   measurement string\n#   ##   timestamp string\n#   [[inputs.postgresql_extensible.query]]\n#     sqlquery=\"SELECT * FROM pg_stat_database\"\n#     version=901\n#     withdbname=false\n#     tagvalue=\"\"\n#     measurement=\"\"\n#   [[inputs.postgresql_extensible.query]]\n#     sqlquery=\"SELECT * FROM pg_stat_bgwriter\"\n#     version=901\n#     withdbname=false\n#     tagvalue=\"postgresql.stats\"\n\n\n# # Read metrics from one or many prometheus clients\n# [[inputs.prometheus]]\n#   ## An array of urls to scrape metrics from.\n#   urls = [\"http://localhost:9100/metrics\"]\n#\n#   ## Metric version controls the mapping from Prometheus metrics into\n#   ## Telegraf metrics.  When using the prometheus_client output, use the same\n#   ## value in both plugins to ensure metrics are round-tripped without\n#   ## modification.\n#   ##\n#   ##   example: metric_version = 1;\n#   ##            metric_version = 2; recommended version\n#   # metric_version = 1\n#\n#   ## Url tag name (tag containing scrapped url. optional, default is \"url\")\n#   # url_tag = \"url\"\n#\n#   ## Whether the timestamp of the scraped metrics will be ignored.\n#   ## If set to true, the gather time will be used.\n#   # ignore_timestamp = false\n#\n#   ## An array of Kubernetes services to scrape metrics from.\n#   # kubernetes_services = [\"http://my-service-dns.my-namespace:9100/metrics\"]\n#\n#   ## Kubernetes config file to create client from.\n#   # kube_config = \"/path/to/kubernetes.config\"\n#\n#   ## Scrape Kubernetes pods for the following prometheus annotations:\n#   ## - prometheus.io/scrape: Enable scraping for this pod\n#   ## - prometheus.io/scheme: If the metrics endpoint is secured then you will need to\n#   ##     set this to 'https' \u0026 most likely set the tls config.\n#   ## - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.\n#   ## - prometheus.io/port: If port is not 9102 use this annotation\n#   # monitor_kubernetes_pods = true\n#   ## Get the list of pods to scrape with either the scope of\n#   ## - cluster: the kubernetes watch api (default, no need to specify)\n#   ## - node: the local cadvisor api; for scalability. Note that the config node_ip or the environment variable NODE_IP must be set to the host IP.\n#   # pod_scrape_scope = \"cluster\"\n#   ## Only for node scrape scope: node IP of the node that telegraf is running on.\n#   ## Either this config or the environment variable NODE_IP must be set.\n#   # node_ip = \"10.180.1.1\"\n# \t## Only for node scrape scope: interval in seconds for how often to get updated pod list for scraping.\n# \t## Default is 60 seconds.\n# \t# pod_scrape_interval = 60\n#   ## Restricts Kubernetes monitoring to a single namespace\n#   ##   ex: monitor_kubernetes_pods_namespace = \"default\"\n#   # monitor_kubernetes_pods_namespace = \"\"\n#   # label selector to target pods which have the label\n#   # kubernetes_label_selector = \"env=dev,app=nginx\"\n#   # field selector to target pods\n#   # eg. To scrape pods on a specific node\n#   # kubernetes_field_selector = \"spec.nodeName=$HOSTNAME\"\n#\n#   ## Scrape Services available in Consul Catalog\n#   # [inputs.prometheus.consul]\n#   #   enabled = true\n#   #   agent = \"http://localhost:8500\"\n#   #   query_interval = \"5m\"\n#\n#   #   [[inputs.prometheus.consul.query]]\n#   #     name = \"a service name\"\n#   #     tag = \"a service tag\"\n#   #     url = 'http://{{if ne .ServiceAddress \"\"}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}'\n#   #     [inputs.prometheus.consul.query.tags]\n#   #       host = \"{{.Node}}\"\n#\n#   ## Use bearer token for authorization. ('bearer_token' takes priority)\n#   # bearer_token = \"/path/to/bearer/token\"\n#   ## OR\n#   # bearer_token_string = \"abc_123\"\n#\n#   ## HTTP Basic Authentication username and password. ('bearer_token' and\n#   ## 'bearer_token_string' take priority)\n#   # username = \"\"\n#   # password = \"\"\n#\n#   ## Specify timeout duration for slower prometheus clients (default is 3s)\n#   # response_timeout = \"3s\"\n#\n#   ## Optional TLS Config\n#   # tls_ca = /path/to/cafile\n#   # tls_cert = /path/to/certfile\n#   # tls_key = /path/to/keyfile\n#   ## Use TLS but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n\n\n# # RAS plugin exposes counter metrics for Machine Check Errors provided by RASDaemon (sqlite3 output is required).\n# [[inputs.ras]]\n#   ## Optional path to RASDaemon sqlite3 database.\n#   ## Default: /var/lib/rasdaemon/ras-mc_event.db\n#   # db_path = \"\"\n\n\n# # Riemann protobuff listener.\n# [[inputs.riemann_listener]]\n#   ## URL to listen on.\n#   ## Default is \"tcp://:5555\"\n#   # service_address = \"tcp://:8094\"\n#   # service_address = \"tcp://127.0.0.1:http\"\n#   # service_address = \"tcp4://:8094\"\n#   # service_address = \"tcp6://:8094\"\n#   # service_address = \"tcp6://[2001:db8::1]:8094\"\n#\n#   ## Maximum number of concurrent connections.\n#   ## 0 (default) is unlimited.\n#   # max_connections = 1024\n#   ## Read timeout.\n#   ## 0 (default) is unlimited.\n#   # read_timeout = \"30s\"\n#   ## Optional TLS configuration.\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key  = \"/etc/telegraf/key.pem\"\n#   ## Enables client authentication if set.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#   ## Maximum socket buffer size (in bytes when no unit specified).\n#   # read_buffer_size = \"64KiB\"\n#   ## Period between keep alive probes.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n\n\n# # SFlow V5 Protocol Listener\n# [[inputs.sflow]]\n#   ## Address to listen for sFlow packets.\n#   ##   example: service_address = \"udp://:6343\"\n#   ##            service_address = \"udp4://:6343\"\n#   ##            service_address = \"udp6://:6343\"\n#   service_address = \"udp://:6343\"\n#\n#   ## Set the size of the operating system's receive buffer.\n#   ##   example: read_buffer_size = \"64KiB\"\n#   # read_buffer_size = \"\"\n\n\n# # Receive SNMP traps\n# [[inputs.snmp_trap]]\n#   ## Transport, local address, and port to listen on.  Transport must\n#   ## be \"udp://\".  Omit local address to listen on all interfaces.\n#   ##   example: \"udp://127.0.0.1:1234\"\n#   ##\n#   ## Special permissions may be required to listen on a port less than\n#   ## 1024.  See README.md for details\n#   ##\n#   # service_address = \"udp://:162\"\n#   ##\n#   ## Path to mib files\n#   # path = [\"/usr/share/snmp/mibs\"]\n#   ##\n#   ## Snmp version, defaults to 2c\n#   # version = \"2c\"\n#   ## SNMPv3 authentication and encryption options.\n#   ##\n#   ## Security Name.\n#   # sec_name = \"myuser\"\n#   ## Authentication protocol; one of \"MD5\", \"SHA\" or \"\".\n#   # auth_protocol = \"MD5\"\n#   ## Authentication password.\n#   # auth_password = \"pass\"\n#   ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n#   # sec_level = \"authNoPriv\"\n#   ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\", \"AES192\", \"AES192C\", \"AES256\", \"AES256C\" or \"\".\n#   # priv_protocol = \"\"\n#   ## Privacy password used for encrypted messages.\n#   # priv_password = \"\"\n\n\n# # Generic socket listener capable of handling multiple socket types.\n# [[inputs.socket_listener]]\n#   ## URL to listen on\n#   # service_address = \"tcp://:8094\"\n#   # service_address = \"tcp://127.0.0.1:http\"\n#   # service_address = \"tcp4://:8094\"\n#   # service_address = \"tcp6://:8094\"\n#   # service_address = \"tcp6://[2001:db8::1]:8094\"\n#   # service_address = \"udp://:8094\"\n#   # service_address = \"udp4://:8094\"\n#   # service_address = \"udp6://:8094\"\n#   # service_address = \"unix:///tmp/telegraf.sock\"\n#   # service_address = \"unixgram:///tmp/telegraf.sock\"\n#\n#   ## Change the file mode bits on unix sockets.  These permissions may not be\n#   ## respected by some platforms, to safely restrict write permissions it is best\n#   ## to place the socket into a directory that has previously been created\n#   ## with the desired permissions.\n#   ##   ex: socket_mode = \"777\"\n#   # socket_mode = \"\"\n#\n#   ## Maximum number of concurrent connections.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   ## 0 (default) is unlimited.\n#   # max_connections = 1024\n#\n#   ## Read timeout.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   ## 0 (default) is unlimited.\n#   # read_timeout = \"30s\"\n#\n#   ## Optional TLS configuration.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key  = \"/etc/telegraf/key.pem\"\n#   ## Enables client authentication if set.\n#   # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n#\n#   ## Maximum socket buffer size (in bytes when no unit specified).\n#   ## For stream sockets, once the buffer fills up, the sender will start backing up.\n#   ## For datagram sockets, once the buffer fills up, metrics will start dropping.\n#   ## Defaults to the OS default.\n#   # read_buffer_size = \"64KiB\"\n#\n#   ## Period between keep alive probes.\n#   ## Only applies to TCP sockets.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   # keep_alive_period = \"5m\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   # data_format = \"influx\"\n#\n#   ## Content encoding for message payloads, can be set to \"gzip\" to or\n#   ## \"identity\" to apply no encoding.\n#   # content_encoding = \"identity\"\n\n\n# # Read metrics from SQL queries\n# [[inputs.sql]]\n#   ## Database Driver\n#   ## See https://github.com/influxdata/telegraf/blob/master/docs/SQL_DRIVERS_INPUT.md for\n#   ## a list of supported drivers.\n#   driver = \"mysql\"\n#\n#   ## Data source name for connecting\n#   ## The syntax and supported options depends on selected driver.\n#   dsn = \"username:password@mysqlserver:3307/dbname?param=value\"\n#\n#   ## Timeout for any operation\n#   ## Note that the timeout for queries is per query not per gather.\n#   # timeout = \"5s\"\n#\n#   ## Connection time limits\n#   ## By default the maximum idle time and maximum lifetime of a connection is unlimited, i.e. the connections\n#   ## will not be closed automatically. If you specify a positive time, the connections will be closed after\n#   ## idleing or existing for at least that amount of time, respectively.\n#   # connection_max_idle_time = \"0s\"\n#   # connection_max_life_time = \"0s\"\n#\n#   ## Connection count limits\n#   ## By default the number of open connections is not limited and the number of maximum idle connections\n#   ## will be inferred from the number of queries specified. If you specify a positive number for any of the\n#   ## two options, connections will be closed when reaching the specified limit. The number of idle connections\n#   ## will be clipped to the maximum number of connections limit if any.\n#   # connection_max_open = 0\n#   # connection_max_idle = auto\n#\n#   [[inputs.sql.query]]\n#     ## Query to perform on the server\n#     query=\"SELECT user,state,latency,score FROM Scoreboard WHERE application \u003e 0\"\n#     ## Alternatively to specifying the query directly you can select a file here containing the SQL query.\n#     ## Only one of 'query' and 'query_script' can be specified!\n#     # query_script = \"/path/to/sql/script.sql\"\n#\n#     ## Name of the measurement\n#     ## In case both measurement and 'measurement_col' are given, the latter takes precedence.\n#     # measurement = \"sql\"\n#\n#     ## Column name containing the name of the measurement\n#     ## If given, this will take precedence over the 'measurement' setting. In case a query result\n#     ## does not contain the specified column, we fall-back to the 'measurement' setting.\n#     # measurement_column = \"\"\n#\n#     ## Column name containing the time of the measurement\n#     ## If ommited, the time of the query will be used.\n#     # time_column = \"\"\n#\n#     ## Format of the time contained in 'time_col'\n#     ## The time must be 'unix', 'unix_ms', 'unix_us', 'unix_ns', or a golang time format.\n#     ## See https://golang.org/pkg/time/#Time.Format for details.\n#     # time_format = \"unix\"\n#\n#     ## Column names containing tags\n#     ## An empty include list will reject all columns and an empty exclude list will not exclude any column.\n#     ## I.e. by default no columns will be returned as tag and the tags are empty.\n#     # tag_columns_include = []\n#     # tag_columns_exclude = []\n#\n# \t\t## Column names containing fields (explicit types)\n#     ## Convert the given columns to the corresponding type. Explicit type conversions take precedence over\n# \t\t## the automatic (driver-based) conversion below.\n# \t\t## NOTE: Columns should not be specified for multiple types or the resulting type is undefined.\n#     # field_columns_float = []\n#     # field_columns_int = []\n# \t\t# field_columns_uint = []\n# \t\t# field_columns_bool = []\n# \t\t# field_columns_string = []\n#\n#     ## Column names containing fields (automatic types)\n#     ## An empty include list is equivalent to '[*]' and all returned columns will be accepted. An empty\n#     ## exclude list will not exclude any column. I.e. by default all columns will be returned as fields.\n#     ## NOTE: We rely on the database driver to perform automatic datatype conversion.\n#     # field_columns_include = []\n#     # field_columns_exclude = []\n\n\n# # Read metrics from Microsoft SQL Server\n# [[inputs.sqlserver]]\n# ## Specify instances to monitor with a list of connection strings.\n# ## All connection parameters are optional.\n# ## By default, the host is localhost, listening on default port, TCP 1433.\n# ##   for Windows, the user is the currently running AD user (SSO).\n# ##   See https://github.com/denisenkom/go-mssqldb for detailed connection\n# ##   parameters, in particular, tls connections can be created like so:\n# ##   \"encrypt=true;certificate=\u003ccert\u003e;hostNameInCertificate=\u003cSqlServer host fqdn\u003e\"\n# servers = [\n#   \"Server=192.168.1.10;Port=1433;User Id=\u003cuser\u003e;Password=\u003cpw\u003e;app name=telegraf;log=1;\",\n# ]\n#\n# ## Authentication method\n# ## valid methods: \"connection_string\", \"AAD\"\n# # auth_method = \"connection_string\"\n#\n# ## \"database_type\" enables a specific set of queries depending on the database type.\n# ## In the config file, the sql server plugin section should be repeated each with a set of servers for a specific database_type.\n# ## Possible values for database_type are - \"SQLServer\" or \"AzureSQLDB\" or \"AzureSQLManagedInstance\" or \"AzureSQLPool\"\n#\n# database_type = \"SQLServer\"\n#\n# ## A list of queries to include. If not specified, all the below listed queries are used.\n# include_query = []\n#\n# ## A list of queries to explicitly ignore.\n# exclude_query = [\"SQLServerAvailabilityReplicaStates\", \"SQLServerDatabaseReplicaStates\"]\n#\n# ## Queries enabled by default for database_type = \"SQLServer\" are -\n# ## SQLServerPerformanceCounters, SQLServerWaitStatsCategorized, SQLServerDatabaseIO, SQLServerProperties, SQLServerMemoryClerks,\n# ## SQLServerSchedulers, SQLServerRequests, SQLServerVolumeSpace, SQLServerCpu, SQLServerAvailabilityReplicaStates, SQLServerDatabaseReplicaStates\n#\n# ## Queries enabled by default for database_type = \"AzureSQLDB\" are -\n# ## AzureSQLDBResourceStats, AzureSQLDBResourceGovernance, AzureSQLDBWaitStats, AzureSQLDBDatabaseIO, AzureSQLDBServerProperties,\n# ## AzureSQLDBOsWaitstats, AzureSQLDBMemoryClerks, AzureSQLDBPerformanceCounters, AzureSQLDBRequests, AzureSQLDBSchedulers\n#\n# ## Queries enabled by default for database_type = \"AzureSQLManagedInstance\" are -\n# ## AzureSQLMIResourceStats, AzureSQLMIResourceGovernance, AzureSQLMIDatabaseIO, AzureSQLMIServerProperties, AzureSQLMIOsWaitstats,\n# ## AzureSQLMIMemoryClerks, AzureSQLMIPerformanceCounters, AzureSQLMIRequests, AzureSQLMISchedulers\n#\n# ## Queries enabled by default for database_type = \"AzureSQLPool\" are -\n# ## AzureSQLPoolResourceStats, AzureSQLPoolResourceGovernance, AzureSQLPoolDatabaseIO, AzureSQLPoolWaitStats,\n# ## AzureSQLPoolMemoryClerks, AzureSQLPoolPerformanceCounters, AzureSQLPoolSchedulers\n\n\n# # Statsd UDP/TCP Server\n# [[inputs.statsd]]\n#   ## Protocol, must be \"tcp\", \"udp\", \"udp4\" or \"udp6\" (default=udp)\n#   protocol = \"udp\"\n#\n#   ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)\n#   max_tcp_connections = 250\n#\n#   ## Enable TCP keep alive probes (default=false)\n#   tcp_keep_alive = false\n#\n#   ## Specifies the keep-alive period for an active network connection.\n#   ## Only applies to TCP sockets and will be ignored if tcp_keep_alive is false.\n#   ## Defaults to the OS configuration.\n#   # tcp_keep_alive_period = \"2h\"\n#\n#   ## Address and port to host UDP listener on\n#   service_address = \":8125\"\n#\n#   ## The following configuration options control when telegraf clears it's cache\n#   ## of previous values. If set to false, then telegraf will only clear it's\n#   ## cache when the daemon is restarted.\n#   ## Reset gauges every interval (default=true)\n#   delete_gauges = true\n#   ## Reset counters every interval (default=true)\n#   delete_counters = true\n#   ## Reset sets every interval (default=true)\n#   delete_sets = true\n#   ## Reset timings \u0026 histograms every interval (default=true)\n#   delete_timings = true\n#\n#   ## Percentiles to calculate for timing \u0026 histogram stats\n#   percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0]\n#\n#   ## separator to use between elements of a statsd metric\n#   metric_separator = \"_\"\n#\n#   ## Parses tags in the datadog statsd format\n#   ## http://docs.datadoghq.com/guides/dogstatsd/\n#   parse_data_dog_tags = false\n#\n#   ## Parses datadog extensions to the statsd format\n#   datadog_extensions = false\n#\n#   ## Parses distributions metric as specified in the datadog statsd format\n#   ## https://docs.datadoghq.com/developers/metrics/types/?tab=distribution#definition\n#   datadog_distributions = false\n#\n#   ## Statsd data translation templates, more info can be read here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md\n#   # templates = [\n#   #     \"cpu.* measurement*\"\n#   # ]\n#\n#   ## Number of UDP messages allowed to queue up, once filled,\n#   ## the statsd server will start dropping packets\n#   allowed_pending_messages = 10000\n#\n#   ## Number of timing/histogram values to track per-measurement in the\n#   ## calculation of percentiles. Raising this limit increases the accuracy\n#   ## of percentiles but also increases the memory usage and cpu time.\n#   percentile_limit = 1000\n#\n#   ## Max duration (TTL) for each metric to stay cached/reported without being updated.\n#   #max_ttl = \"1000h\"\n#\n#   ## Sanitize name method\n#   ## By default, telegraf will pass names directly as they are received.\n#   ## However, upstream statsd now does sanitization of names which can be\n#   ## enabled by using the \"upstream\" method option. This option will a) replace\n#   ## white space with '_', replace '/' with '-', and remove charachters not\n#   ## matching 'a-zA-Z_\\-0-9\\.;='.\n#   #sanitize_name_method = \"\"\n\n\n# # Suricata stats and alerts plugin\n# [[inputs.suricata]]\n#   ## Data sink for Suricata stats and alerts logs\n#   # This is expected to be a filename of a\n#   # unix socket to be created for listening.\n#   source = \"/var/run/suricata-stats.sock\"\n#\n#   # Delimiter for flattening field keys, e.g. subitem \"alert\" of \"detect\"\n#   # becomes \"detect_alert\" when delimiter is \"_\".\n#   delimiter = \"_\"\n#\n#   ## Detect alert logs\n#   # alerts = false\n\n\n# # Accepts syslog messages following RFC5424 format with transports as per RFC5426, RFC5425, or RFC6587\n# [[inputs.syslog]]\n#   ## Specify an ip or hostname with port - eg., tcp://localhost:6514, tcp://10.0.0.1:6514\n#   ## Protocol, address and port to host the syslog receiver.\n#   ## If no host is specified, then localhost is used.\n#   ## If no port is specified, 6514 is used (RFC5425#section-4.1).\n#   server = \"tcp://:6514\"\n#\n#   ## TLS Config\n#   # tls_allowed_cacerts = [\"/etc/telegraf/ca.pem\"]\n#   # tls_cert = \"/etc/telegraf/cert.pem\"\n#   # tls_key = \"/etc/telegraf/key.pem\"\n#\n#   ## Period between keep alive probes.\n#   ## 0 disables keep alive probes.\n#   ## Defaults to the OS configuration.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   # keep_alive_period = \"5m\"\n#\n#   ## Maximum number of concurrent connections (default = 0).\n#   ## 0 means unlimited.\n#   ## Only applies to stream sockets (e.g. TCP).\n#   # max_connections = 1024\n#\n#   ## Read timeout is the maximum time allowed for reading a single message (default = 5s).\n#   ## 0 means unlimited.\n#   # read_timeout = \"5s\"\n#\n#   ## The framing technique with which it is expected that messages are transported (default = \"octet-counting\").\n#   ## Whether the messages come using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n#   ## or the non-transparent framing technique (RFC6587#section-3.4.2).\n#   ## Must be one of \"octet-counting\", \"non-transparent\".\n#   # framing = \"octet-counting\"\n#\n#   ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n#   ## Must be one of \"LF\", or \"NUL\".\n#   # trailer = \"LF\"\n#\n#   ## Whether to parse in best effort mode or not (default = false).\n#   ## By default best effort parsing is off.\n#   # best_effort = false\n#\n#   ## The RFC standard to use for message parsing\n#   ## By default RFC5424 is used. RFC3164 only supports UDP transport (no streaming support)\n#   ## Must be one of \"RFC5424\", or \"RFC3164\".\n#   # syslog_standard = \"RFC5424\"\n#\n#   ## Character to prepend to SD-PARAMs (default = \"_\").\n#   ## A syslog message can contain multiple parameters and multiple identifiers within structured data section.\n#   ## Eg., [id1 name1=\"val1\" name2=\"val2\"][id2 name1=\"val1\" nameA=\"valA\"]\n#   ## For each combination a field is created.\n#   ## Its name is created concatenating identifier, sdparam_separator, and parameter name.\n#   # sdparam_separator = \"_\"\n\n\n# # Parse the new lines appended to a file\n# [[inputs.tail]]\n#   ## File names or a pattern to tail.\n#   ## These accept standard unix glob matching rules, but with the addition of\n#   ## ** as a \"super asterisk\". ie:\n#   ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n#   ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n#   ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n#   ##   \"/var/log/log[!1-2]*  -\u003e tail files without 1-2\n#   ##   \"/var/log/log[^1-2]*  -\u003e identical behavior as above\n#   ## See https://github.com/gobwas/glob for more examples\n#   ##\n#   files = [\"/var/mymetrics.out\"]\n#\n#   ## Read file from beginning.\n#   # from_beginning = false\n#\n#   ## Whether file is a named pipe\n#   # pipe = false\n#\n#   ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n#   # watch_method = \"inotify\"\n#\n#   ## Maximum lines of the file to process that have not yet be written by the\n#   ## output.  For best throughput set based on the number of metrics on each\n#   ## line and the size of the output's metric_batch_size.\n#   # max_undelivered_lines = 1000\n#\n#   ## Character encoding to use when interpreting the file contents.  Invalid\n#   ## characters are replaced using the unicode replacement character.  When set\n#   ## to the empty string the data is not decoded to text.\n#   ##   ex: character_encoding = \"utf-8\"\n#   ##       character_encoding = \"utf-16le\"\n#   ##       character_encoding = \"utf-16be\"\n#   ##       character_encoding = \"\"\n#   # character_encoding = \"\"\n#\n#   ## Data format to consume.\n#   ## Each data format has its own unique set of configuration options, read\n#   ## more about them here:\n#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n#   data_format = \"influx\"\n#\n#   ## Set the tag that will contain the path of the tailed file. If you don't want this tag, set it to an empty string.\n#   # path_tag = \"path\"\n#\n#   ## multiline parser/codec\n#   ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html\n#   #[inputs.tail.multiline]\n#     ## The pattern should be a regexp which matches what you believe to be an\n# \t## indicator that the field is part of an event consisting of multiple lines of log data.\n#     #pattern = \"^\\s\"\n#\n#     ## This field must be either \"previous\" or \"next\".\n# \t## If a line matches the pattern, \"previous\" indicates that it belongs to the previous line,\n# \t## whereas \"next\" indicates that the line belongs to the next one.\n#     #match_which_line = \"previous\"\n#\n#     ## The invert_match field can be true or false (defaults to false).\n#     ## If true, a message not matching the pattern will constitute a match of the multiline\n# \t## filter and the what will be applied. (vice-versa is also true)\n#     #invert_match = false\n#\n#     ## After the specified timeout, this plugin sends a multiline event even if no new pattern\n# \t## is found to start a new event. The default timeout is 5s.\n#     #timeout = 5s\n\n\n# # Generic TCP listener\n# [[inputs.tcp_listener]]\n#   ## DEPRECATED: The 'tcp_listener' plugin is deprecated in version 1.3.0, use 'inputs.socket_listener' instead.\n#   # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n#   # socket_listener plugin\n#   # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n\n# # Generic UDP listener\n# [[inputs.udp_listener]]\n#   ## DEPRECATED: The 'udp_listener' plugin is deprecated in version 1.3.0, use 'inputs.socket_listener' instead.\n#   # DEPRECATED: the TCP listener plugin has been deprecated in favor of the\n#   # socket_listener plugin\n#   # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n\n\n# # Read metrics from VMware vCenter\n# [[inputs.vsphere]]\n#   ## List of vCenter URLs to be monitored. These three lines must be uncommented\n#   ## and edited for the plugin to work.\n#   vcenters = [ \"https://vcenter.local/sdk\" ]\n#   username = \"user@corp.local\"\n#   password = \"secret\"\n#\n#   ## VMs\n#   ## Typical VM metrics (if omitted or empty, all metrics are collected)\n#   # vm_include = [ \"/*/vm/**\"] # Inventory path to VMs to collect (by default all are collected)\n#   # vm_exclude = [] # Inventory paths to exclude\n#   vm_metric_include = [\n#     \"cpu.demand.average\",\n#     \"cpu.idle.summation\",\n#     \"cpu.latency.average\",\n#     \"cpu.readiness.average\",\n#     \"cpu.ready.summation\",\n#     \"cpu.run.summation\",\n#     \"cpu.usagemhz.average\",\n#     \"cpu.used.summation\",\n#     \"cpu.wait.summation\",\n#     \"mem.active.average\",\n#     \"mem.granted.average\",\n#     \"mem.latency.average\",\n#     \"mem.swapin.average\",\n#     \"mem.swapinRate.average\",\n#     \"mem.swapout.average\",\n#     \"mem.swapoutRate.average\",\n#     \"mem.usage.average\",\n#     \"mem.vmmemctl.average\",\n#     \"net.bytesRx.average\",\n#     \"net.bytesTx.average\",\n#     \"net.droppedRx.summation\",\n#     \"net.droppedTx.summation\",\n#     \"net.usage.average\",\n#     \"power.power.average\",\n#     \"virtualDisk.numberReadAveraged.average\",\n#     \"virtualDisk.numberWriteAveraged.average\",\n#     \"virtualDisk.read.average\",\n#     \"virtualDisk.readOIO.latest\",\n#     \"virtualDisk.throughput.usage.average\",\n#     \"virtualDisk.totalReadLatency.average\",\n#     \"virtualDisk.totalWriteLatency.average\",\n#     \"virtualDisk.write.average\",\n#     \"virtualDisk.writeOIO.latest\",\n#     \"sys.uptime.latest\",\n#   ]\n#   # vm_metric_exclude = [] ## Nothing is excluded by default\n#   # vm_instances = true ## true by default\n#\n#   ## Hosts\n#   ## Typical host metrics (if omitted or empty, all metrics are collected)\n#   # host_include = [ \"/*/host/**\"] # Inventory path to hosts to collect (by default all are collected)\n#   # host_exclude [] # Inventory paths to exclude\n#   host_metric_include = [\n#     \"cpu.coreUtilization.average\",\n#     \"cpu.costop.summation\",\n#     \"cpu.demand.average\",\n#     \"cpu.idle.summation\",\n#     \"cpu.latency.average\",\n#     \"cpu.readiness.average\",\n#     \"cpu.ready.summation\",\n#     \"cpu.swapwait.summation\",\n#     \"cpu.usage.average\",\n#     \"cpu.usagemhz.average\",\n#     \"cpu.used.summation\",\n#     \"cpu.utilization.average\",\n#     \"cpu.wait.summation\",\n#     \"disk.deviceReadLatency.average\",\n#     \"disk.deviceWriteLatency.average\",\n#     \"disk.kernelReadLatency.average\",\n#     \"disk.kernelWriteLatency.average\",\n#     \"disk.numberReadAveraged.average\",\n#     \"disk.numberWriteAveraged.average\",\n#     \"disk.read.average\",\n#     \"disk.totalReadLatency.average\",\n#     \"disk.totalWriteLatency.average\",\n#     \"disk.write.average\",\n#     \"mem.active.average\",\n#     \"mem.latency.average\",\n#     \"mem.state.latest\",\n#     \"mem.swapin.average\",\n#     \"mem.swapinRate.average\",\n#     \"mem.swapout.average\",\n#     \"mem.swapoutRate.average\",\n#     \"mem.totalCapacity.average\",\n#     \"mem.usage.average\",\n#     \"mem.vmmemctl.average\",\n#     \"net.bytesRx.average\",\n#     \"net.bytesTx.average\",\n#     \"net.droppedRx.summation\",\n#     \"net.droppedTx.summation\",\n#     \"net.errorsRx.summation\",\n#     \"net.errorsTx.summation\",\n#     \"net.usage.average\",\n#     \"power.power.average\",\n#     \"storageAdapter.numberReadAveraged.average\",\n#     \"storageAdapter.numberWriteAveraged.average\",\n#     \"storageAdapter.read.average\",\n#     \"storageAdapter.write.average\",\n#     \"sys.uptime.latest\",\n#   ]\n#     ## Collect IP addresses? Valid values are \"ipv4\" and \"ipv6\"\n#   # ip_addresses = [\"ipv6\", \"ipv4\" ]\n#\n#   # host_metric_exclude = [] ## Nothing excluded by default\n#   # host_instances = true ## true by default\n#\n#\n#   ## Clusters\n#   # cluster_include = [ \"/*/host/**\"] # Inventory path to clusters to collect (by default all are collected)\n#   # cluster_exclude = [] # Inventory paths to exclude\n#   # cluster_metric_include = [] ## if omitted or empty, all metrics are collected\n#   # cluster_metric_exclude = [] ## Nothing excluded by default\n#   # cluster_instances = false ## false by default\n#\n#   ## Datastores\n#   # datastore_include = [ \"/*/datastore/**\"] # Inventory path to datastores to collect (by default all are collected)\n#   # datastore_exclude = [] # Inventory paths to exclude\n#   # datastore_metric_include = [] ## if omitted or empty, all metrics are collected\n#   # datastore_metric_exclude = [] ## Nothing excluded by default\n#   # datastore_instances = false ## false by default\n#\n#   ## Datacenters\n#   # datacenter_include = [ \"/*/host/**\"] # Inventory path to clusters to collect (by default all are collected)\n#   # datacenter_exclude = [] # Inventory paths to exclude\n#   datacenter_metric_include = [] ## if omitted or empty, all metrics are collected\n#   datacenter_metric_exclude = [ \"*\" ] ## Datacenters are not collected by default.\n#   # datacenter_instances = false ## false by default\n#\n#   ## Plugin Settings\n#   ## separator character to use for measurement and field names (default: \"_\")\n#   # separator = \"_\"\n#\n#   ## number of objects to retrieve per query for realtime resources (vms and hosts)\n#   ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n#   # max_query_objects = 256\n#\n#   ## number of metrics to retrieve per query for non-realtime resources (clusters and datastores)\n#   ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n#   # max_query_metrics = 256\n#\n#   ## number of go routines to use for collection and discovery of objects and metrics\n#   # collect_concurrency = 1\n#   # discover_concurrency = 1\n#\n#   ## the interval before (re)discovering objects subject to metrics collection (default: 300s)\n#   # object_discovery_interval = \"300s\"\n#\n#   ## timeout applies to any of the api request made to vcenter\n#   # timeout = \"60s\"\n#\n#   ## When set to true, all samples are sent as integers. This makes the output\n#   ## data types backwards compatible with Telegraf 1.9 or lower. Normally all\n#   ## samples from vCenter, with the exception of percentages, are integer\n#   ## values, but under some conditions, some averaging takes place internally in\n#   ## the plugin. Setting this flag to \"false\" will send values as floats to\n#   ## preserve the full precision when averaging takes place.\n#   # use_int_samples = true\n#\n#   ## Custom attributes from vCenter can be very useful for queries in order to slice the\n#   ## metrics along different dimension and for forming ad-hoc relationships. They are disabled\n#   ## by default, since they can add a considerable amount of tags to the resulting metrics. To\n#   ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include\n#   ## to select the attributes you want to include.\n#   ## By default, since they can add a considerable amount of tags to the resulting metrics. To\n#   ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include\n#   ## to select the attributes you want to include.\n#   # custom_attribute_include = []\n#   # custom_attribute_exclude = [\"*\"]\n#\n#   ## The number of vSphere 5 minute metric collection cycles to look back for non-realtime metrics. In\n#   ## some versions (6.7, 7.0 and possible more), certain metrics, such as cluster metrics, may be reported\n#   ## with a significant delay (\u003e30min). If this happens, try increasing this number. Please note that increasing\n#   ## it too much may cause performance issues.\n#   # metric_lookback = 3\n#\n#   ## Optional SSL Config\n#   # ssl_ca = \"/path/to/cafile\"\n#   # ssl_cert = \"/path/to/certfile\"\n#   # ssl_key = \"/path/to/keyfile\"\n#   ## Use SSL but skip chain \u0026 host verification\n#   # insecure_skip_verify = false\n#\n#   ## The Historical Interval value must match EXACTLY the interval in the daily\n#   # \"Interval Duration\" found on the VCenter server under Configure \u003e General \u003e Statistics \u003e Statistic intervals\n#   # historical_interval = \"5m\"\n\n\n# # A Webhooks Event collector\n# [[inputs.webhooks]]\n#   ## Address and port to host Webhook listener on\n#   service_address = \":1619\"\n#\n#   [inputs.webhooks.filestack]\n#     path = \"/filestack\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.github]\n#     path = \"/github\"\n#     # secret = \"\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.mandrill]\n#     path = \"/mandrill\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.rollbar]\n#     path = \"/rollbar\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.papertrail]\n#     path = \"/papertrail\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n#\n#   [inputs.webhooks.particle]\n#     path = \"/particle\"\n#\n# \t## HTTP basic auth\n# \t#username = \"\"\n# \t#password = \"\"\n\n\n# # This plugin implements the Zipkin http server to gather trace and timing data needed to troubleshoot latency problems in microservice architectures.\n# [[inputs.zipkin]]\n#   # path = \"/api/v1/spans\" # URL path for span data\n#   # port = 9411            # Port on which Telegraf listens\n",
            "content_base64": null,
            "directory_permission": "0777",
            "file_permission": "0777",
            "filename": "./install_tig/telegraf/telegraf.conf",
            "id": "fdb6826485f8648b5d6885d721a5ef197c02ceec",
            "sensitive_content": null,
            "source": null
          },
          "sensitive_attributes": [],
          "dependencies": [
            "module.docker_composer.data.template_file.init_telegraf_config"
          ]
        }
      ]
    },
    {
      "module": "module.docker_composer",
      "mode": "managed",
      "type": "null_resource",
      "name": "cp_docker_composer_source",
      "provider": "provider[\"registry.terraform.io/hashicorp/null\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "8424916210238817372",
            "triggers": null
          },
          "sensitive_attributes": [],
          "dependencies": [
            "module.docker_composer.null_resource.create_tmp_directory_for_install"
          ]
        }
      ]
    },
    {
      "module": "module.docker_composer",
      "mode": "managed",
      "type": "null_resource",
      "name": "create_tmp_directory_for_install",
      "provider": "provider[\"registry.terraform.io/hashicorp/null\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "9105064957806412885",
            "triggers": null
          },
          "sensitive_attributes": []
        }
      ]
    }
  ],
  "check_results": null
}
